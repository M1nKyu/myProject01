#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
SEO êµ¬í˜„ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
ê° í˜ì´ì§€ì˜ ë©”íƒ€ íƒœê·¸ì™€ êµ¬ì¡°í™” ë°ì´í„°ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
"""

import requests
from bs4 import BeautifulSoup
import json
from typing import Dict, List

# ê²€ì¦í•  í˜ì´ì§€ ëª©ë¡
PAGES_TO_CHECK = [
    {'url': '/', 'name': 'í™ˆí˜ì´ì§€'},
    {'url': '/about', 'name': 'ì†Œê°œ í˜ì´ì§€'},
    {'url': '/guidelines', 'name': 'ê°€ì´ë“œë¼ì¸'},
    {'url': '/membership/plans', 'name': 'íšŒì›ê¶Œ'},
    {'url': '/badge', 'name': 'ë±ƒì§€'},
]

BASE_URL = 'http://localhost:5000'

def check_meta_tags(soup: BeautifulSoup) -> Dict:
    """ë©”íƒ€ íƒœê·¸ í™•ì¸"""
    results = {
        'canonical': None,
        'description': None,
        'og_title': None,
        'og_description': None,
        'og_url': None,
        'og_image': None,
        'title': None,
    }

    # Title
    title_tag = soup.find('title')
    if title_tag:
        results['title'] = title_tag.string

    # Canonical
    canonical = soup.find('link', {'rel': 'canonical'})
    if canonical:
        results['canonical'] = canonical.get('href')

    # Description
    description = soup.find('meta', {'name': 'description'})
    if description:
        results['description'] = description.get('content')

    # Open Graph
    og_title = soup.find('meta', {'property': 'og:title'})
    if og_title:
        results['og_title'] = og_title.get('content')

    og_desc = soup.find('meta', {'property': 'og:description'})
    if og_desc:
        results['og_description'] = og_desc.get('content')

    og_url = soup.find('meta', {'property': 'og:url'})
    if og_url:
        results['og_url'] = og_url.get('content')

    og_image = soup.find('meta', {'property': 'og:image'})
    if og_image:
        results['og_image'] = og_image.get('content')

    return results

def check_structured_data(soup: BeautifulSoup) -> List[Dict]:
    """êµ¬ì¡°í™” ë°ì´í„° (JSON-LD) í™•ì¸"""
    scripts = soup.find_all('script', {'type': 'application/ld+json'})
    structured_data = []

    for script in scripts:
        try:
            data = json.loads(script.string)
            structured_data.append(data)
        except json.JSONDecodeError:
            pass

    return structured_data

def verify_page(url: str, name: str):
    """ê°œë³„ í˜ì´ì§€ ê²€ì¦"""
    print(f"\n{'='*60}")
    print(f"ğŸ“„ {name} ({url})")
    print(f"{'='*60}")

    try:
        response = requests.get(BASE_URL + url, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')

        # ë©”íƒ€ íƒœê·¸ í™•ì¸
        print("\nâœ… ë©”íƒ€ íƒœê·¸:")
        meta_tags = check_meta_tags(soup)

        for key, value in meta_tags.items():
            if value:
                display_value = value if len(str(value)) < 80 else str(value)[:77] + "..."
                print(f"  âœ“ {key}: {display_value}")
            else:
                print(f"  âœ— {key}: ì—†ìŒ")

        # êµ¬ì¡°í™” ë°ì´í„° í™•ì¸
        print("\nâœ… êµ¬ì¡°í™” ë°ì´í„° (JSON-LD):")
        structured_data = check_structured_data(soup)

        if structured_data:
            for idx, data in enumerate(structured_data, 1):
                schema_type = data.get('@type', 'ì•Œ ìˆ˜ ì—†ìŒ')
                print(f"  âœ“ Schema {idx}: {schema_type}")
        else:
            print(f"  âœ— êµ¬ì¡°í™” ë°ì´í„° ì—†ìŒ")

        # ì¢…í•© í‰ê°€
        print("\nğŸ“Š ì¢…í•© í‰ê°€:")
        score = 0
        total = 0

        # í•„ìˆ˜ í•­ëª© ì²´í¬
        required_items = [
            ('Title', meta_tags['title']),
            ('Canonical URL', meta_tags['canonical']),
            ('Description', meta_tags['description']),
            ('OG Title', meta_tags['og_title']),
            ('êµ¬ì¡°í™” ë°ì´í„°', len(structured_data) > 0),
        ]

        for item_name, item_value in required_items:
            total += 1
            if item_value:
                score += 1
                print(f"  âœ“ {item_name}")
            else:
                print(f"  âœ— {item_name} ëˆ„ë½")

        percentage = (score / total) * 100
        print(f"\n  ì ìˆ˜: {score}/{total} ({percentage:.0f}%)")

        if percentage == 100:
            print(f"  ğŸ‰ ì™„ë²½í•©ë‹ˆë‹¤!")
        elif percentage >= 80:
            print(f"  âœ… ì–‘í˜¸í•©ë‹ˆë‹¤.")
        else:
            print(f"  âš ï¸ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")

        return True

    except requests.exceptions.ConnectionError:
        print(f"  âŒ ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. localhost:5000ì´ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return False
    except requests.exceptions.Timeout:
        print(f"  âŒ ìš”ì²­ ì‹œê°„ ì´ˆê³¼")
        return False
    except Exception as e:
        print(f"  âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return False

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("=" * 60)
    print("ğŸ” eCarbon SEO êµ¬í˜„ ê²€ì¦ ì‹œì‘")
    print("=" * 60)
    print(f"\nì„œë²„ URL: {BASE_URL}")
    print(f"ê²€ì¦ í˜ì´ì§€ ìˆ˜: {len(PAGES_TO_CHECK)}ê°œ")

    # ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸
    print("\nì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...")
    try:
        response = requests.get(BASE_URL, timeout=5)
        print("âœ… ì„œë²„ ì—°ê²° ì„±ê³µ")
    except:
        print("âŒ ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„œë²„ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:")
        print("  python run.py")
        print("  ë˜ëŠ”")
        print("  docker-compose -f docker-compose.dev.yml up")
        return

    # ê° í˜ì´ì§€ ê²€ì¦
    success_count = 0
    for page in PAGES_TO_CHECK:
        if verify_page(page['url'], page['name']):
            success_count += 1

    # ìµœì¢… ìš”ì•½
    print(f"\n{'='*60}")
    print(f"ğŸ“Š ìµœì¢… ê²°ê³¼")
    print(f"{'='*60}")
    print(f"ê²€ì¦ ì™„ë£Œ: {success_count}/{len(PAGES_TO_CHECK)} í˜ì´ì§€")

    if success_count == len(PAGES_TO_CHECK):
        print("ğŸ‰ ëª¨ë“  í˜ì´ì§€ ê²€ì¦ ì„±ê³µ!")
    else:
        print(f"âš ï¸ {len(PAGES_TO_CHECK) - success_count}ê°œ í˜ì´ì§€ì—ì„œ ë¬¸ì œ ë°œê²¬")

    print("\nğŸ’¡ ì°¸ê³ :")
    print("  - ë™ì  í˜ì´ì§€(ë¶„ì„ ê²°ê³¼ ë“±)ëŠ” ì‹¤ì œ ë¶„ì„ í›„ task_idë¡œ í™•ì¸í•˜ì„¸ìš”")
    print("  - Google Search Console ë“±ë¡ í›„ ì‹¤ì œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”")
    print("  - Lighthouse SEO ì ìˆ˜ ì¸¡ì •: Chrome DevTools â†’ Lighthouse â†’ SEO")

if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nê²€ì¦ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
