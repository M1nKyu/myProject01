"""
Sitemap.xml ë° Robots.txt ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” /sitemap.xmlê³¼ /robots.txt ì—”ë“œí¬ì¸íŠ¸ê°€
ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python verify_sitemap_robots.py

ìš”êµ¬ì‚¬í•­:
    - Flask ì„œë²„ê°€ localhost:5000ì—ì„œ ì‹¤í–‰ ì¤‘ì´ì–´ì•¼ í•©ë‹ˆë‹¤
    - requests ë¼ì´ë¸ŒëŸ¬ë¦¬ í•„ìš”

ì‘ì„±ì¼: 2025-10-22
"""

import requests
from urllib.parse import urlparse
import sys


def test_sitemap():
    """Sitemap.xml í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("ğŸ—ºï¸  Sitemap.xml ê²€ì¦")
    print("="*60)

    url = "http://localhost:5000/sitemap.xml"

    try:
        response = requests.get(url, timeout=10)

        print(f"\nâœ… ì‘ë‹µ ì½”ë“œ: {response.status_code}")

        if response.status_code == 200:
            print(f"âœ… Content-Type: {response.headers.get('Content-Type')}")

            # XML í˜•ì‹ í™•ì¸
            if 'xml' in response.headers.get('Content-Type', ''):
                print("âœ… Content-Typeì´ XMLì…ë‹ˆë‹¤")
            else:
                print(f"âš ï¸  Content-Typeì´ XMLì´ ì•„ë‹™ë‹ˆë‹¤: {response.headers.get('Content-Type')}")

            # ë‚´ìš© ê²€ì¦
            content = response.text

            # XML ì„ ì–¸ í™•ì¸
            if '<?xml version=' in content:
                print("âœ… XML ì„ ì–¸ì´ ìˆìŠµë‹ˆë‹¤")

            # urlset í™•ì¸
            if '<urlset' in content and 'sitemaps.org' in content:
                print("âœ… Sitemap í˜•ì‹ì´ ì˜¬ë°”ë¦…ë‹ˆë‹¤")

            # URL ê°œìˆ˜ í™•ì¸
            url_count = content.count('<loc>')
            print(f"âœ… í¬í•¨ëœ URL ê°œìˆ˜: {url_count}ê°œ")

            # í•„ìˆ˜ í˜ì´ì§€ í™•ì¸
            required_pages = [
                ('/', 'í™ˆí˜ì´ì§€'),
                ('/about', 'ì†Œê°œ'),
                ('/guidelines', 'ê°€ì´ë“œë¼ì¸'),
                ('/membership/plans', 'íšŒì›ê¶Œ'),
                ('/badge', 'ë±ƒì§€')
            ]

            print("\nğŸ“‹ í¬í•¨ëœ í˜ì´ì§€:")
            for path, name in required_pages:
                if path in content:
                    print(f"  âœ… {name} ({path})")
                else:
                    print(f"  âŒ {name} ({path}) - ëˆ„ë½!")

            # changefreq í™•ì¸
            if '<changefreq>' in content:
                print("\nâœ… changefreq íƒœê·¸ê°€ ìˆìŠµë‹ˆë‹¤")

            # priority í™•ì¸
            if '<priority>' in content:
                print("âœ… priority íƒœê·¸ê°€ ìˆìŠµë‹ˆë‹¤")

            print("\nâœ… Sitemap.xml ê²€ì¦ ì™„ë£Œ!")
            return True

        else:
            print(f"âŒ ì‹¤íŒ¨: HTTP {response.status_code}")
            print(f"ì‘ë‹µ ë‚´ìš©: {response.text[:500]}")
            return False

    except requests.exceptions.ConnectionError:
        print("âŒ ì—°ê²° ì‹¤íŒ¨: ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”")
        print("   docker ps ë˜ëŠ” python run.py í™•ì¸")
        return False
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False


def test_robots():
    """Robots.txt í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("ğŸ¤– Robots.txt ê²€ì¦")
    print("="*60)

    url = "http://localhost:5000/robots.txt"

    try:
        response = requests.get(url, timeout=10)

        print(f"\nâœ… ì‘ë‹µ ì½”ë“œ: {response.status_code}")

        if response.status_code == 200:
            print(f"âœ… Content-Type: {response.headers.get('Content-Type')}")

            # text/plain í™•ì¸
            if 'text/plain' in response.headers.get('Content-Type', ''):
                print("âœ… Content-Typeì´ text/plainì…ë‹ˆë‹¤")

            content = response.text

            # User-agent í™•ì¸
            if 'User-agent:' in content:
                print("âœ… User-agent ì§€ì‹œë¬¸ì´ ìˆìŠµë‹ˆë‹¤")

            # Allow í™•ì¸
            if 'Allow:' in content:
                print("âœ… Allow ì§€ì‹œë¬¸ì´ ìˆìŠµë‹ˆë‹¤")

            # Disallow í™•ì¸
            if 'Disallow:' in content:
                print("âœ… Disallow ì§€ì‹œë¬¸ì´ ìˆìŠµë‹ˆë‹¤")

            # Sitemap ì°¸ì¡° í™•ì¸
            if 'Sitemap:' in content:
                print("âœ… Sitemap ìœ„ì¹˜ê°€ ì§€ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
                # Sitemap URL ì¶”ì¶œ
                for line in content.split('\n'):
                    if line.startswith('Sitemap:'):
                        sitemap_url = line.split(':', 1)[1].strip()
                        print(f"   ğŸ“ Sitemap URL: {sitemap_url}")

            # ì£¼ìš” Disallow ê·œì¹™ í™•ì¸
            print("\nğŸ“‹ ì£¼ìš” Disallow ê·œì¹™:")
            disallow_rules = [
                ('/carbon_calculate_emission/', 'ë¶„ì„ ê²°ê³¼ (ë™ì )'),
                ('/code_analysis/', 'ì½”ë“œ ë¶„ì„ (ë™ì )'),
                ('/img_optimization/', 'ì´ë¯¸ì§€ ìµœì í™” (ë™ì )'),
                ('/dev/', 'ê°œë°œ ë„êµ¬'),
                ('/api/', 'API'),
                ('/auth/', 'ì¸ì¦')
            ]

            for path, name in disallow_rules:
                if f'Disallow: {path}' in content:
                    print(f"  âœ… {name} ({path})")
                else:
                    print(f"  âš ï¸  {name} ({path}) - ëˆ„ë½")

            # Crawl-delay í™•ì¸
            if 'Crawl-delay:' in content:
                print("\nâœ… Crawl-delayê°€ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤")

            print("\nâœ… Robots.txt ê²€ì¦ ì™„ë£Œ!")
            return True

        else:
            print(f"âŒ ì‹¤íŒ¨: HTTP {response.status_code}")
            print(f"ì‘ë‹µ ë‚´ìš©: {response.text[:500]}")
            return False

    except requests.exceptions.ConnectionError:
        print("âŒ ì—°ê²° ì‹¤íŒ¨: ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”")
        print("   docker ps ë˜ëŠ” python run.py í™•ì¸")
        return False
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("\n" + "="*60)
    print("ğŸ” ECO-WEB Sitemap & Robots ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸")
    print("="*60)

    sitemap_ok = test_sitemap()
    robots_ok = test_robots()

    print("\n" + "="*60)
    print("ğŸ“Š ìµœì¢… ê²°ê³¼")
    print("="*60)

    if sitemap_ok and robots_ok:
        print("\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
        print("\në‹¤ìŒ ë‹¨ê³„:")
        print("1. Google Search Consoleì— Sitemap ì œì¶œ")
        print("   â†’ https://search.google.com/search-console")
        print("2. Robots.txt ë¬¸ë²• ê²€ì¦")
        print("   â†’ https://www.google.com/webmasters/tools/robots-testing-tool")
        print("3. ì‹¤ì œ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸")
        print("   â†’ curl -A 'Googlebot' http://localhost:5000/sitemap.xml")
        return 0
    else:
        print("\nâŒ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        print("\ní•´ê²° ë°©ë²•:")
        print("1. Flask ì„œë²„ ì¬ì‹œì‘: docker-compose restart web")
        print("2. ë¡œê·¸ í™•ì¸: docker logs ecoweb-web-1")
        print("3. Blueprint ë“±ë¡ í™•ì¸: logsì— 'SEO blueprint registered' ë©”ì‹œì§€")
        return 1


if __name__ == '__main__':
    sys.exit(main())
