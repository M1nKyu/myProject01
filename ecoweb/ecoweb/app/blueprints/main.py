# í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import json
import logging
import os
import random
import threading
import time
import uuid
from datetime import datetime, timedelta, timezone
from urllib.parse import urlsplit, urlunsplit

# ì„œë“œíŒŒí‹° ë¼ì´ë¸ŒëŸ¬ë¦¬
import requests
import urllib3
from bson import Int64, ObjectId

# SSL ê²½ê³  ë©”ì‹œì§€ ë¹„í™œì„±í™” (ì‚¬ì´íŠ¸ ì ‘ê·¼ì„± ì²´í¬ ì‹œ verify=False ì‚¬ìš©)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
from celery.result import AsyncResult
from flask import (
    Blueprint, current_app, flash, jsonify, make_response, redirect,
    render_template, request, send_from_directory, session, url_for
)
from werkzeug.utils import secure_filename

# ë¡œì»¬ ì• í”Œë¦¬ì¼€ì´ì…˜
from ecoweb.app import celery, db
from ecoweb.app.services.analysis.analysis_service import perform_detailed_analysis, process_content_emission_data
from ecoweb.app.services.analysis.emissions import estimate_emission_per_page, estimate_emission_from_kb
# from ecoweb.app.services.report.pdf import CarbonReportGenerator  # WeasyPrint ë¹„í™œì„±í™”ë¡œ ì£¼ì„ ì²˜ë¦¬
from ecoweb.app.tasks import analyze_url_task
from ecoweb.app.utils.grade import grade_point, grade_point_by_emission
from ecoweb.app.utils.emission_calculator import EmissionCalculator
from ecoweb.app.utils.seo_helpers import MetaDataGenerator
from ecoweb.app.utils.structured_data import StructuredDataGenerator
from ecoweb.app.utils.validators import validate_and_normalize_url
from ecoweb.app.services.capture.accessibility import check_site_accessibility_sync  # Phase 2: ë¹„ë™ê¸° ì ‘ê·¼ì„± ì²´í¬
from ecoweb.app.utils.event_logger import log_analysis_start, log_analysis_cancel, log_user_event, is_logging_enabled, log_page_view

from ..database import get_db
from .utils import get_active_celery_tasks
from ecoweb.app.utils.task_cancellation import log_task_cancellation, is_task_cancelled

main_bp = Blueprint('main', __name__)

# ===================================================================
# ğŸ” ì‚¬ì´íŠ¸ ì ‘ê·¼ì„± ì²´í¬ í•¨ìˆ˜
# ===================================================================
def check_site_accessibility(url, timeout=5):
    """
    URLì´ ì ‘ê·¼ ê°€ëŠ¥í•œì§€ í™•ì¸í•©ë‹ˆë‹¤.

    Args:
        url (str): í™•ì¸í•  URL
        timeout (int): íƒ€ì„ì•„ì›ƒ ì‹œê°„ (ì´ˆ). Phase 1: 10ì´ˆ â†’ 5ì´ˆë¡œ ë‹¨ì¶•

    Returns:
        bool: ì ‘ê·¼ ê°€ëŠ¥í•˜ë©´ True, ë¶ˆê°€ëŠ¥í•˜ë©´ False
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    try:
        # ë¨¼ì € HEAD ìš”ì²­ ì‹œë„
        response = requests.head(url, headers=headers, timeout=timeout, allow_redirects=True, verify=False)
        if 200 <= response.status_code < 400:
            return True

        # HEAD ìš”ì²­ì´ ì‹¤íŒ¨í•˜ë©´ GET ìš”ì²­ ì‹œë„ (ì¼ë¶€ ì‚¬ì´íŠ¸ëŠ” HEADë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŒ)
        current_app.logger.info(f"HEAD ìš”ì²­ ì‹¤íŒ¨ (ìƒíƒœì½”ë“œ: {response.status_code}), GET ìš”ì²­ìœ¼ë¡œ ì¬ì‹œë„: {url}")

    except Exception as head_error:
        current_app.logger.info(f"HEAD ìš”ì²­ ì˜ˆì™¸ ë°œìƒ, GET ìš”ì²­ìœ¼ë¡œ ì¬ì‹œë„: {url}, ì˜¤ë¥˜: {head_error}")

    try:
        # GET ìš”ì²­ìœ¼ë¡œ ì¬ì‹œë„ (ì²« ëª‡ ë°”ì´íŠ¸ë§Œ ë°›ìŒ)
        response = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True,
                              verify=False, stream=True)

        # ì‘ë‹µì´ ì‹œì‘ë˜ë©´ ì¦‰ì‹œ ì—°ê²° ì¢…ë£Œ (ì „ì²´ ë‹¤ìš´ë¡œë“œí•˜ì§€ ì•ŠìŒ)
        response.close()

        if 200 <= response.status_code < 400:
            return True
        else:
            current_app.logger.warning(f"ì‚¬ì´íŠ¸ ì ‘ê·¼ ì‹¤íŒ¨ - ìƒíƒœì½”ë“œ: {response.status_code}, URL: {url}")
            return False

    except requests.exceptions.Timeout:
        current_app.logger.warning(f"ì‚¬ì´íŠ¸ ì ‘ê·¼ ì‹¤íŒ¨ - íƒ€ì„ì•„ì›ƒ({timeout}ì´ˆ), URL: {url}")
        return False
    except requests.exceptions.ConnectionError as e:
        current_app.logger.warning(f"ì‚¬ì´íŠ¸ ì ‘ê·¼ ì‹¤íŒ¨ - ì—°ê²° ì˜¤ë¥˜: {e}, URL: {url}")
        return False
    except requests.exceptions.SSLError as e:
        current_app.logger.warning(f"ì‚¬ì´íŠ¸ ì ‘ê·¼ ì‹¤íŒ¨ - SSL ì˜¤ë¥˜: {e}, URL: {url}")
        return False
    except Exception as e:
        current_app.logger.warning(f"ì‚¬ì´íŠ¸ ì ‘ê·¼ ì‹¤íŒ¨ - ì˜ˆì™¸: {e}, URL: {url}")
        return False

# ===================================================================
# ğŸ  ë©”ì¸ í˜ì´ì§€ ë¼ìš°íŠ¸
# ===================================================================
@main_bp.route('/', methods=['GET', 'POST'])
def home():
    # [1] ìµœê·¼ ì…ë ¥í•œ URL ëª©ë¡ì„ ì¿ í‚¤ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸° (ìµœëŒ€ 5ê°œ ê´€ë¦¬)
    recent_urls = json.loads(request.cookies.get('recent_urls', '[]'))
    
    # ë©”ì¸ í˜ì´ì§€ ì ‘ì† ë¡œê¹… (GET ìš”ì²­ë§Œ)
    if request.method == 'GET':
        log_page_view('home')

    # [2] í¼ ì œì¶œ(POST) ì‹œ ë¶„ì„ ì‘ì—…ì„ ìƒì„±í•˜ê³  ë¡œë”© í˜ì´ì§€ë¡œ ì´ë™
    if request.method == 'POST':
        # [3] ì‚¬ìš©ì ì—ì´ì „íŠ¸ë¥¼ í†µí•´ ëª¨ë°”ì¼ ì—¬ë¶€ íŒë³„ (UI/ë¶„ì„ êµ¬ë¶„ìš©)
        user_agent = request.headers.get('User-Agent', '').lower()
        is_mobile = 'iphone' in user_agent or 'android' in user_agent or 'mobile' in user_agent
        
        # [4] í¼ì—ì„œ URL ì…ë ¥ê°’ì„ ê°€ì ¸ì™€ ê³µë°± ì œê±°
        url = request.form.get('wgd-cc-url', '').strip()

        # [5] URLì´ ë¹„ì–´ìˆìœ¼ë©´ ì—ëŸ¬ ë©”ì‹œì§€ í›„ ë©”ì¸ìœ¼ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
        if not url:
            flash('ìœ íš¨í•œ URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.', 'error')
            return redirect(url_for('main.home'))

        # [6] URL ê²€ì¦ ë° ì •ê·œí™” (ê°œì„ ëœ ë³´ì•ˆ ê²€ì¦)
        is_valid, normalized_url, error_msg = validate_and_normalize_url(url)
        if not is_valid:
            current_app.logger.warning(f'URL ê²€ì¦ ì‹¤íŒ¨: {url} - {error_msg}')
            flash(f'URL í˜•ì‹ ì˜¤ë¥˜: {error_msg}', 'error')
            return redirect(url_for('main.home'))

        # ê²€ì¦ëœ URL ì‚¬ìš©
        url = normalized_url
        current_app.logger.info(f'URL ê²€ì¦ ì„±ê³µ: {url}')
        
        # ì´ë²¤íŠ¸ ë¡œê¹…: ë¶„ì„ ì‹œì‘
        user_id = session.get('user_id')
        log_analysis_start(url, user_id=str(user_id) if user_id else None, is_mobile=is_mobile)

        # [6-1] ì‚¬ì´íŠ¸ ì ‘ê·¼ì„± ì‚¬ì „ ì²´í¬ (Phase 2: ë¹„ë™ê¸° ì²´í¬ ì‚¬ìš©)
        if not check_site_accessibility_sync(url, timeout=5):
            current_app.logger.error(f'ì‚¬ì´íŠ¸ ì ‘ê·¼ ì‹¤íŒ¨: {url}')

            # MongoDB í•¸ë“¤ íšë“
            db = get_db()
            task_id = str(uuid.uuid4())

            # ì‚¬ìš©ì ì¹œí™”ì  ì—ëŸ¬ ì •ë³´ ìƒì„±
            error_info = {
                'title': 'ì‚¬ì´íŠ¸ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤',
                'message': 'ì…ë ¥í•˜ì‹  ì›¹ì‚¬ì´íŠ¸ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
                'suggestion': 'URLì„ ë‹¤ì‹œ í™•ì¸í•˜ê±°ë‚˜ í•´ë‹¹ ì‚¬ì´íŠ¸ê°€ ì •ìƒ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•´ ì£¼ì„¸ìš”.'
            }

            # ì ‘ê·¼ì„± ì‹¤íŒ¨í•œ task ê²°ê³¼ë¥¼ DBì— ì €ì¥ (ë¡œë”© í˜ì´ì§€ í‘œì‹œìš©)
            db.task_results.insert_one({
                '_id': task_id,
                'status': 'FAILURE',
                'failure_type': 'ACCESSIBILITY_CHECK',
                'url': url,
                'user_id': session.get('user_id', 'anonymous'),
                'is_mobile': is_mobile,
                'error': 'ì‚¬ì´íŠ¸ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
                'error_info': error_info,
                'created_at': datetime.now(timezone.utc)
            })

            # ì„¸ì…˜ì— task_idë§Œ ì €ì¥ (Phase 4: DB-centered architecture)
            session['task_id'] = task_id

            # ìµœê·¼ URL ëª©ë¡ ê°±ì‹ 
            if url in recent_urls:
                recent_urls.remove(url)
            recent_urls.insert(0, url)
            recent_urls = recent_urls[:5]

            # ë¡œë”© í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ (ì˜¤ë¥˜ ë©”ì‹œì§€ê°€ í‘œì‹œë  ê²ƒì„)
            response = make_response(redirect(url_for('main.loading', task_id=task_id, url=url)))
            response.set_cookie('recent_urls', json.dumps(recent_urls), max_age=30*24*60*60)
            return response

        # [7] ì‚¬ìš©ì ì‹ë³„ì íšë“ ë° ìš”ì²­ ë¡œê¹…
        user_id = session.get('user_id', 'anonymous')
        # current_app.logger.info(f'URL ë¶„ì„ ìš”ì²­ - ì‚¬ìš©ì: {user_id}, URL: {url}')

        # [8] MongoDB í•¸ë“¤ íšë“
        db = get_db()

        # [9] ìµœê·¼ ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (ì¼ì£¼ì¼ ì´ë‚´)
        recent_threshold = datetime.now(timezone.utc) - timedelta(days=7)

        # lighthouse_traffic_02ì™€ lighthouse_resources_02 ì»¬ë ‰ì…˜ì—ì„œ ìµœê·¼ ë°ì´í„° í™•ì¸
        # MongoDB Projection: ëª¨ë“  í•„ë“œ í•„ìš” (process_existing_dataì—ì„œ ì‚¬ìš©)
        traffic_data = db.lighthouse_traffic_02.find_one({
            'url': url,
            'timestamp': {'$gte': recent_threshold}
        }, sort=[('timestamp', -1)])

        resource_data = db.lighthouse_resources_02.find_one({
            'url': url,
            'timestamp': {'$gte': recent_threshold}
        }, sort=[('timestamp', -1)])

        # [10] ë‘ ì»¬ë ‰ì…˜ ëª¨ë‘ì— ìµœê·¼ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ê¸°ì¡´ Lighthouse ë°ì´í„° ì¬ì‚¬ìš©í•˜ì—¬ Celery ì‘ì—… ì‹¤í–‰
        if traffic_data and resource_data:
            print(f'[DB ì¡°íšŒ ì„±ê³µ] ìµœê·¼ Lighthouse ë°ì´í„° ë°œê²¬: {url}')
            print(f'[DB ì¡°íšŒ ì„±ê³µ] Traffic ë°ì´í„°: {traffic_data.get("timestamp")}')
            print(f'[DB ì¡°íšŒ ì„±ê³µ] Resource ë°ì´í„°: {resource_data.get("timestamp")}')
            current_app.logger.info(f'ìµœê·¼ Lighthouse ë°ì´í„° ë°œê²¬: {url} - í•˜ìœ„í˜ì´ì§€ ë¶„ì„ ë° ì´ë¯¸ì§€ ìµœì í™”ë§Œ ì‹¤í–‰')

            # ê¸°ì¡´ Lighthouse ë°ì´í„°ë¡œë¶€í„° view_data ìƒì„±
            try:
                from ecoweb.app.services.lighthouse import process_existing_data
                view_data = process_existing_data(traffic_data, resource_data, url, is_mobile)
                print(f'[DB ì¡°íšŒ ì„±ê³µ] view_data ìƒì„± ì™„ë£Œ: total_byte_weight={view_data.get("total_byte_weight", 0)} bytes')

                # ë™ì‹œ ì‹¤í–‰ ì œí•œ(ì“°ë¡œí‹€ë§) í™•ì¸
                CELERY_QUEUE_THRESHOLD = 5
                active_tasks = get_active_celery_tasks()

                original_task_id = str(uuid.uuid4())
                print(f'[DB ì¡°íšŒ ì„±ê³µ] í•˜ìœ„í˜ì´ì§€+ì´ë¯¸ì§€ìµœì í™” task_id ìƒì„±: {original_task_id}')

                if active_tasks >= CELERY_QUEUE_THRESHOLD:
                    # íì— ëŒ€ê¸°
                    db.task_results.insert_one({
                        '_id': original_task_id,
                        'status': 'QUEUED',
                        'url': url,
                        'user_id': user_id,
                        'is_mobile': is_mobile,
                        'created_at': datetime.now(timezone.utc)
                    })
                    current_app.logger.info(f'Task {original_task_id} (ê¸°ì¡´ ë°ì´í„° í™œìš©) queued. Active tasks: {active_tasks}')
                else:
                    # ì¦‰ì‹œ ì‹¤í–‰: í•˜ìœ„í˜ì´ì§€ + ì´ë¯¸ì§€ ìµœì í™”ë§Œ ìˆ˜í–‰í•˜ëŠ” Celery ì‘ì—…
                    task = analyze_url_task.delay(url, user_id, is_mobile, original_task_id,
                                                perform_subpage_crawling=True, existing_view_data=view_data)
                    celery_task_id = task.id

                    db.task_results.insert_one({
                        '_id': original_task_id,
                        'celery_task_id': celery_task_id,
                        'status': 'PENDING',
                        'url': url,
                        'user_id': user_id,
                        'is_mobile': is_mobile,
                        'created_at': datetime.now(timezone.utc)
                    })
                    current_app.logger.info(f'Task {original_task_id} (ê¸°ì¡´ ë°ì´í„° í™œìš©) started with Celery ID {celery_task_id}')

                session['task_id'] = original_task_id

                # ìµœê·¼ URL ëª©ë¡ ê°±ì‹ 
                if url in recent_urls:
                    recent_urls.remove(url)
                recent_urls.insert(0, url)
                recent_urls = recent_urls[:5]

                # ë¡œë”© í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ (í•˜ìœ„í˜ì´ì§€ ë¶„ì„ + ì´ë¯¸ì§€ ìµœì í™” ì§„í–‰)
                print(f'[DB ì¡°íšŒ ì„±ê³µ] ë¡œë”© í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸: task_id={original_task_id}')
                response = make_response(redirect(url_for('main.loading', task_id=original_task_id, url=url)))
                response.set_cookie('recent_urls', json.dumps(recent_urls), max_age=30*24*60*60)
                return response

            except Exception as e:
                current_app.logger.warning(f'ê¸°ì¡´ ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨: {e}. ìƒˆë¡œìš´ ì¸¡ì •ì„ ì§„í–‰í•©ë‹ˆë‹¤.')
                # ê¸°ì¡´ ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ìƒˆë¡œìš´ ì¸¡ì • ì§„í–‰

        # [11] ë™ì‹œ ì‹¤í–‰ ì œí•œ(ì“°ë¡œí‹€ë§) ë° í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ì‘ì—… ìˆ˜ ì¡°íšŒ
        CELERY_QUEUE_THRESHOLD = 5  # ë™ì‹œì— ì‹¤í–‰í•  ìµœëŒ€ ì‘ì—… ìˆ˜
        active_tasks = get_active_celery_tasks()
        
        # [12] ì¦‰ì‹œ ì‹¤í–‰ ë¶ˆê°€: í ìƒíƒœë¡œ task_resultsì— ì´ˆê¸° ë¬¸ì„œ ì‚½ì…
        if active_tasks >= CELERY_QUEUE_THRESHOLD:
            task_id = str(uuid.uuid4())
            db.task_results.insert_one({
                '_id': task_id,
                'status': 'QUEUED',
                'url': url,
                'user_id': user_id,
                'is_mobile': is_mobile,
                'result': None,
                'created_at': datetime.now(timezone.utc)
            })
            current_app.logger.info(f'Task {task_id} for {url} is queued. Active tasks: {active_tasks}')

        # [13] ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥: Celery ì‘ì—… ìƒì„± í›„ task_resultsì— PENDINGìœ¼ë¡œ ê¸°ë¡
        else:
            original_task_id = str(uuid.uuid4())
            task = analyze_url_task.delay(url, user_id, is_mobile, original_task_id, perform_subpage_crawling=False)
            celery_task_id = task.id

            db.task_results.insert_one({
                '_id': original_task_id,
                'celery_task_id': celery_task_id,
                'status': 'PENDING',
                'url': url,
                'user_id': user_id,
                'is_mobile': is_mobile,
                'created_at': datetime.now(timezone.utc)
            })
            task_id = original_task_id
            current_app.logger.info(f'Task {task_id} for {url} started immediately with Celery ID {celery_task_id}. Active tasks: {active_tasks}')
        
        # [14] ì„¸ì…˜ì— í˜„ì¬ ì‘ì—… ì‹ë³„ìë§Œ ì €ì¥ (Phase 4: DB-centered architecture)
        session['task_id'] = task_id

        # [15] ìµœê·¼ URL ëª©ë¡ ê°±ì‹ (ì¤‘ë³µ ì œê±° í›„ ë§¨ ì•ì— ì¶”ê°€, ìµœëŒ€ 5ê°œ ìœ ì§€)
        if url in recent_urls:
            recent_urls.remove(url)
        recent_urls.insert(0, url)
        recent_urls = recent_urls[:5]

        # [16] ë¡œë”© í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ + ìµœê·¼ URL ì¿ í‚¤ ì €ì¥(30ì¼)
        response = make_response(redirect(url_for('main.loading', task_id=task_id, url=url)))
        response.set_cookie('recent_urls', json.dumps(recent_urls), max_age=30*24*60*60)
        return response
    
    # [17] GET ìš”ì²­: ë©”ì¸ í˜ì´ì§€ ë Œë”ë§
    # SEO: ë©”íƒ€ ë°ì´í„° ë° Structured Data ìƒì„±
    meta = MetaDataGenerator.generate_home_meta()
    structured_data = [
        StructuredDataGenerator.generate_organization_schema(),
        StructuredDataGenerator.generate_website_schema(),
        StructuredDataGenerator.generate_web_application_schema()
    ]

    return render_template(
        'pages/main/main.html',
        recent_urls=recent_urls,
        meta=meta,
        structured_data=structured_data
    )

# ==========================================================================
# ğŸ“‘ í†µí•© ë¶„ì„ í˜ì´ì§€ ë¼ìš°íŠ¸ (ì‚¬ì´ë“œë°”ì—ì„œ ì ‘ê·¼)
# ==========================================================================
@main_bp.route('/carbon_calculate_emission')
def carbon_calculate_emission_router():
    """í†µí•© ë¶„ì„ í˜ì´ì§€ - ìµœê·¼ ë¶„ì„ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ê²°ê³¼ í‘œì‹œ, ì—†ìœ¼ë©´ ìƒˆ ë¶„ì„ ì‹œì‘"""
    # ì„¸ì…˜ì—ì„œ ìµœê·¼ task_id í™•ì¸
    last_task_id = session.get('last_completed_task_id')

    if last_task_id:
        # task_idê°€ ìˆìœ¼ë©´ í•´ë‹¹ ê²°ê³¼ê°€ ìœ íš¨í•œì§€ í™•ì¸
        try:
            mongo_db = db.get_db()
            task_results_collection = mongo_db.task_results
            # MongoDB Projection: ìƒíƒœ í™•ì¸ì— í•„ìš”í•œ í•„ë“œë§Œ ì¡°íšŒ
            task_result = task_results_collection.find_one(
                {'_id': last_task_id},
                {'status': 1, '_id': 0}
            )

            if task_result and task_result.get('status') in ['SUCCESS', 'MEASUREMENT_COMPLETE']:
                # ìœ íš¨í•œ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ê²°ê³¼ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
                current_app.logger.info(f'ë§ˆì§€ë§‰ ì™„ë£Œëœ ë¶„ì„ ê²°ê³¼ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸: {last_task_id}')
                return redirect(url_for('main.carbon_calculate_emission', task_id=last_task_id))
        except Exception as e:
            current_app.logger.warning(f'ë§ˆì§€ë§‰ task_id í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}')

    # task_idê°€ ì—†ê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´ ìƒˆ ë¶„ì„ ì‹œì‘ í˜ì´ì§€ í‘œì‹œ
    recent_urls = json.loads(request.cookies.get('recent_urls', '[]'))
    return render_template('pages/main/main.html', recent_urls=recent_urls)

# ==========================================================================
# ğŸ“‘ URL ë¶„ì„ ê²°ê³¼ í˜ì´ì§€ ë¼ìš°íŠ¸
# ==========================================================================
@main_bp.route('/carbon_calculate_emission/<task_id>')
def carbon_calculate_emission(task_id):
    """
    URL ë¶„ì„ ê²°ê³¼ í˜ì´ì§€ (Phase 2: Session-to-DB Refactoring)

    ë³€ê²½ ì‚¬í•­:
    - MongoDBì˜ calculated ì„¹ì…˜ì—ì„œ ì´ë¯¸ ê³„ì‚°ëœ ë°ì´í„° ì§ì ‘ ì‚¬ìš©
    - ëª¨ë“  ê³„ì‚° ë¡œì§ ì œê±° (tasks.pyì˜ _enrich_view_data()ì—ì„œ ì´ë¯¸ ê³„ì‚°ë¨)
    - ì„¸ì…˜ ì €ì¥ ìµœì†Œí™” (task_idì™€ í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ê¸°ë³¸ ì •ë³´ë§Œ)
    """
    current_app.logger.info(f'ê²°ê³¼ í˜ì´ì§€ ìš”ì²­ ì‹œì‘: Task ID = {task_id}')
    
    # í˜ì´ì§€ ì¡°íšŒ ë¡œê¹…
    log_page_view('carbon_calculate_emission', task_id=task_id)
    
    try:
        mongo_db = db.get_db()
        task_results_collection = mongo_db.task_results

        # [1] MongoDBì—ì„œ enriched_result ì½ê¸° (Phase 1+2: ì¬ì‹œë„ ìµœì í™”)
        max_retries = 5  # 10 â†’ 5ë¡œ ê°ì†Œ
        retry_delay = 0.8  # Phase 2 ìˆ˜ì •: 0.5ì´ˆ â†’ 0.8ì´ˆ (DB ì“°ê¸° ì™„ë£Œ ëŒ€ê¸°)
        task_result = None
        for attempt in range(max_retries):
            task_result = task_results_collection.find_one({'_id': task_id})
            if task_result:
                task_status = task_result.get('status')
                # Phase 2: SUCCESS ìƒíƒœë„ ì™„ë£Œë¡œ ì²˜ë¦¬ (Celery ì™„ë£Œ ìƒíƒœ)
                # ì„±ê³µ ìƒíƒœì¸ ê²½ìš°ì—ë§Œ ë£¨í”„ ì¢…ë£Œ
                if task_status in ['SUCCESS', 'MEASUREMENT_COMPLETE']:
                    break
                # ì‘ì—…ì´ ì•„ì§ ì§„í–‰ ì¤‘ì¸ ê²½ìš° ì¬ì‹œë„
                elif task_status in ['PENDING', 'PROCESSING', 'STARTED', 'PROGRESS']:
                    current_app.logger.debug(f'ì‘ì—… ì§„í–‰ ì¤‘ (ì¬ì‹œë„ {attempt+1}/{max_retries}): Task ID {task_id}, ìƒíƒœ={task_status}')
                    time.sleep(retry_delay)
                    continue
                # ì‹¤íŒ¨/ì·¨ì†Œ ìƒíƒœì¸ ê²½ìš° ì¦‰ì‹œ ì¢…ë£Œ
                else:
                    break
            else:
                current_app.logger.debug(f'ì‘ì—… ê²°ê³¼ ì—†ìŒ (ì¬ì‹œë„ {attempt+1}/{max_retries}): Task ID {task_id}')
                time.sleep(retry_delay)

        if not task_result:
            current_app.logger.error(f'ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: Task ID {task_id}ê°€ ë°ì´í„°ë² ì´ìŠ¤ì— ì—†ìŠµë‹ˆë‹¤.')
            flash('ë¶„ì„ ê²°ê³¼ë¥¼ ì°¾ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.', 'error')
            return redirect(url_for('main.home'))

        task_status = task_result.get('status')
        if task_status not in ['SUCCESS', 'MEASUREMENT_COMPLETE']:
            error_info = task_result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')
            current_app.logger.error(f'ì‘ì—… ì‹¤íŒ¨: Task ID {task_id}ì˜ ìƒíƒœê°€ {task_status}. ì˜¤ë¥˜: {error_info}')

            # PENDING ìƒíƒœì¸ ê²½ìš° ë” ì¹œì ˆí•œ ë©”ì‹œì§€ ì œê³µ
            if task_status in ['PENDING', 'PROCESSING', 'STARTED', 'PROGRESS']:
                flash('ë¶„ì„ì´ ì•„ì§ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.', 'warning')
                return redirect(url_for('main.loading', task_id=task_id, url=task_result.get('url', '')))
            else:
                flash(f'URL ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_info}', 'error')
                return redirect(url_for('main.home'))

        view_data = task_result.get('result')
        if not view_data or not isinstance(view_data, dict):
            current_app.logger.error(f'ë°ì´í„° í˜•ì‹ ì˜¤ë¥˜: Task ID {task_id}ì˜ ê²°ê³¼ ë°ì´í„°ê°€ ë¹„ì–´ ìˆê±°ë‚˜ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤.')
            flash('ë¶„ì„ ë°ì´í„°ê°€ ë¹„ì–´ìˆê±°ë‚˜ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ë¥¸ URLë¡œ ì‹œë„í•´ ì£¼ì„¸ìš”.', 'error')
            return redirect(url_for('main.home'))

        # [2] calculated ì„¹ì…˜ ì¶”ì¶œ (ëª¨ë“  ê³„ì‚°ëœ ë°ì´í„°)
        calculated = view_data.get('calculated', {})

        # [3] í•˜ìœ„ í˜¸í™˜ì„±: calculated ì„¹ì…˜ì´ ì—†ëŠ” ê²½ìš° (ê¸°ì¡´ ë°ì´í„°) - ì¦‰ì„ ê³„ì‚°
        if not calculated:
            current_app.logger.warning(f'Task ID {task_id}: calculated ì„¹ì…˜ ì—†ìŒ. ì¦‰ì„ ê³„ì‚° ìˆ˜í–‰ (í•˜ìœ„ í˜¸í™˜ì„±)')
            total_byte_weight = view_data.get('total_byte_weight', 0)
            kb_weight = total_byte_weight / 1024
            carbon_emission = round(estimate_emission_from_kb(kb_weight), 2)
            korea_avg_carbon = round(estimate_emission_per_page(0.00456), 2)
            global_avg_carbon = round(estimate_emission_per_page(0.002344), 2)
            korea_diff = round(korea_avg_carbon - carbon_emission, 2)
            global_diff = round(global_avg_carbon - carbon_emission, 2)
            emission_percentile = EmissionCalculator.predict_percentile(carbon_emission)
            emission_grade = EmissionCalculator.get_emission_grade(carbon_emission)
            korea_carbon_percentage_diff = round(abs((carbon_emission - korea_avg_carbon) / korea_avg_carbon) * 100) if korea_avg_carbon > 0 else 0
            korea_comparison_status = "ë‚®ìŠµë‹ˆë‹¤" if korea_diff > 0 else "ë†’ìŠµë‹ˆë‹¤"  # DEPRECATED
            korea_emission_status = "below_avg" if korea_diff > 0 else "above_avg"  # for i18n

            calculated = {
                'carbon_emission': carbon_emission,
                'kb_weight': kb_weight,
                'emission_grade': emission_grade,
                'emission_percentile': emission_percentile,
                'korea_avg_carbon': korea_avg_carbon,
                'global_avg_carbon': global_avg_carbon,
                'korea_diff': korea_diff,
                'global_diff': global_diff,
                'korea_diff_abs': round(abs(korea_diff), 2),
                'global_diff_abs': round(abs(global_diff), 2),
                'korea_carbon_percentage_diff': korea_carbon_percentage_diff,
                'korea_comparison_status': korea_comparison_status,  # DEPRECATED
                'korea_emission_status': korea_emission_status  # for i18n
            }

        # [4] URL ì¶”ì¶œ
        url = view_data.get('url')
        if not url:
            current_app.logger.error(f'URL ì—†ìŒ: Task ID {task_id}ì˜ ê²°ê³¼ ë°ì´í„°ì— URLì´ ì—†ìŠµë‹ˆë‹¤.')
            flash('ë¶„ì„ ë°ì´í„°ì—ì„œ URLì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', 'error')
            return redirect(url_for('main.home'))

        # [5] ì„¸ì…˜ ì €ì¥ ìµœì†Œí™” (Phase 4: DB-centered architecture)
        # task_id ì¶”ì ìš©ìœ¼ë¡œë§Œ ì„¸ì…˜ ì‚¬ìš© (ëŒ€ìš©ëŸ‰ ë°ì´í„°ëŠ” MongoDBì—ì„œ ì§ì ‘ ì½ê¸°)
        session['current_task_id'] = task_id
        session['last_completed_task_id'] = task_id

        # [6] í…œí”Œë¦¿ ë Œë”ë§ìš© ë³€ìˆ˜ ì¤€ë¹„ (calculated ì„¹ì…˜ì—ì„œ ì§ì ‘ ì¶”ì¶œ)
        subpages_data = view_data.get('subpages', [])
        emission_trend_data = []  # í–¥í›„ êµ¬í˜„ ê°€ëŠ¥

        current_app.logger.info(f'Task ID {task_id}ì˜ ê²°ê³¼ í˜ì´ì§€ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë Œë”ë§í•©ë‹ˆë‹¤.')

        # [7] SEO: ë©”íƒ€ ë°ì´í„° ë° Structured Data ìƒì„±
        meta = MetaDataGenerator.generate_analysis_meta(task_result, task_id)
        structured_data = [
            StructuredDataGenerator.generate_organization_schema(),
            StructuredDataGenerator.generate_analysis_article_schema(task_result, task_id),
            StructuredDataGenerator.generate_breadcrumb_schema([
                {'name': 'í™ˆ', 'url': '/'},
                {'name': 'ë¶„ì„ ê²°ê³¼', 'url': f'/carbon_calculate_emission/{task_id}'}
            ])
        ]

        return render_template(
            'pages/analysis/carbon_calculate_emission.html',
            task_id=task_id,
            view_data=view_data,
            url=url,
            kb_weight=f"{calculated.get('kb_weight', 0):,.0f}",
            grade=None,  # ê¸°ì¡´ grade_point ì‚¬ìš© ì•ˆ í•¨
            carbon_emission=calculated.get('carbon_emission'),
            global_avg_carbon=calculated.get('global_avg_carbon'),
            korea_avg_carbon=calculated.get('korea_avg_carbon'),
            korea_diff=calculated.get('korea_diff'),
            global_diff=calculated.get('global_diff'),
            korea_diff_abs=calculated.get('korea_diff_abs'),
            global_diff_abs=calculated.get('global_diff_abs'),
            institution_type=session.get('institution_type'),
            analysis_date=datetime.now(),
            emission_percentile=calculated.get('emission_percentile'),
            korea_carbon_emission_grade=calculated.get('emission_grade'),
            world_carbon_emission_grade=calculated.get('emission_grade'),
            subpages_data=subpages_data,
            emission_trend_data=json.dumps(emission_trend_data),
            korea_carbon_percentage_diff=calculated.get('korea_carbon_percentage_diff'),
            korea_comparison_status=calculated.get('korea_comparison_status'),  # DEPRECATED
            korea_emission_status=calculated.get('korea_emission_status', 'below_avg'),  # for i18n
            meta=meta,  # SEO meta data
            structured_data=structured_data  # Schema.org JSON-LD
        )

    except Exception as e:
        current_app.logger.error(f"ê²°ê³¼ í˜ì´ì§€ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        flash('ê²°ê³¼ë¥¼ í‘œì‹œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.', 'error')
        return redirect(url_for('main.home'))

# ==========================================================================
# ğŸ§ª ì •ë°€ ë¶„ì„ í˜ì´ì§€ ë¼ìš°íŠ¸ 
# ==========================================================================
@main_bp.route('/detailed-analysis', methods=['GET', 'POST'])
def detailed_analysis():
    """ì •ë°€ ë¶„ì„ í˜ì´ì§€ ë¼ìš°íŠ¸ - ì›¹ì‚¬ì´íŠ¸ì˜ ìƒì„¸ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤."""
    # í˜ì´ì§€ ì¡°íšŒ ë¡œê¹…
    task_id_for_logging = session.get('last_completed_task_id')
    log_page_view('detailed_analysis', task_id=task_id_for_logging)
    
    # ë¡œì»¬ ì„í¬íŠ¸ë¡œ ìˆœí™˜ ì°¸ì¡° ë°©ì§€
    from ecoweb.app.services.analysis.emissions import emissions_breakdown_from_bytes

    def normalize_url(u: str) -> str:
        """ìŠ¤í‚´/ì¿¼ë¦¬/í”„ë˜ê·¸ë¨¼íŠ¸ ë¬´ì‹œ, www ì œê±°, ë§ë¯¸ ìŠ¬ë˜ì‹œ ì œê±°, ì†Œë¬¸ìí™”.
        ë¹„êµëŠ” schemeì„ ì œì™¸í•˜ê³  netloc+path ê¸°ì¤€ìœ¼ë¡œ ìˆ˜í–‰í•œë‹¤."""
        if not u:
            return ''
        try:
            u = u.strip()
            parts = urlsplit(u)
            netloc = parts.netloc.lower()
            if netloc.startswith('www.'):
                netloc = netloc[4:]
            path = parts.path.rstrip('/')
            # scheme, query, fragment ì œê±°í•˜ê³  netloc+pathë§Œ ë°˜í™˜
            return f"{netloc}{path}"
        except Exception:
            return (u or '').lower().rstrip('/').replace('http://', '').replace('https://', '').lstrip('www.')

    # Phase 4: DB-centered architecture - ì„¸ì…˜ì—ì„œ task_id ê°€ì ¸ì™€ì„œ DB ì¡°íšŒ
    task_id = session.get('last_completed_task_id')
    url = None
    subpages = []
    total_byte_weight = None
    emissions_breakdown = None  # ì´ˆê¸°í™”
    content_emission_data = []  # ì´ˆê¸°í™”
    korea_carbon_percentage_diff = None  # ì´ˆê¸°í™”
    korea_comparison_status = None  # ì´ˆê¸°í™” (DEPRECATED)
    korea_emission_status = 'below_avg'  # ì´ˆê¸°í™” (for i18n)

    if task_id:
        # DBì—ì„œ task_idë¡œ ìµœì‹  ë¶„ì„ ê²°ê³¼ ì¡°íšŒ
        mongo_db = db.get_db()
        task_results_collection = mongo_db.task_results
        # MongoDB Projection: detailed_analysisì— í•„ìš”í•œ í•„ë“œ ì¡°íšŒ
        result_doc = task_results_collection.find_one(
            {'_id': task_id},
            {
                'status': 1,
                'result': 1  # ì „ì²´ result ê°ì²´ ì¡°íšŒ (partial object ë¬¸ì œ ë°©ì§€)
            }
        )

        if result_doc and result_doc.get('status') in ['SUCCESS', 'MEASUREMENT_COMPLETE']:
            result = result_doc.get('result', {})
            url = result.get('url')
            subpages = result.get('subpages', [])
            total_byte_weight = result.get('total_byte_weight')

            # calculated ì„¹ì…˜ì—ì„œ ì‚¬ì „ ê³„ì‚°ëœ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (Phase 3: DB-centered)
            calculated = result.get('calculated', {})
            emissions_breakdown = calculated.get('emissions_breakdown')
            content_emission_data = calculated.get('content_emission_data', [])
            content_count_data = calculated.get('content_count_data', [])  # tasks.pyì—ì„œ ê³„ì‚°ëœ ë°ì´í„° ì‚¬ìš©
            korea_carbon_percentage_diff = calculated.get('korea_carbon_percentage_diff')
            korea_comparison_status = calculated.get('korea_comparison_status')  # DEPRECATED
            korea_emission_status = calculated.get('korea_emission_status', 'below_avg')

    # í•˜ìœ„ í˜¸í™˜ì„±: task_idê°€ ì—†ê±°ë‚˜ DB ì¡°íšŒ ì‹¤íŒ¨ ì‹œ ì„¸ì…˜ì—ì„œ ì½ê¸° (ê¸°ì¡´ ë™ì‘)
    if not url:
        url = session.get('url')
        subpages = session.get('subpages', [])
        view_data_json = session.get('view_data')
        if view_data_json:
            try:
                vd = json.loads(view_data_json)
                total_byte_weight = vd.get('total_byte_weight')
            except Exception:
                pass

    # 3) ë°°ì¶œëŸ‰ ìƒì„¸ breakdown: DBì—ì„œ ê°€ì ¸ì˜¨ ê°’ì´ ì—†ìœ¼ë©´ ê³„ì‚° (í•˜ìœ„ í˜¸í™˜ì„±)
    if not emissions_breakdown:
        try:
            emissions_breakdown = emissions_breakdown_from_bytes(total_byte_weight or 0, region='korea', round_digits=4)
        except Exception:
            emissions_breakdown = {}

    # 3-1) ë„¤íŠ¸ì›Œí¬ ìƒì„¸ê°’(í•´ì €/ì™¸ë¶€/í™ˆ) íŒŒìƒ ìƒì„±
    # ìš”êµ¬ì‚¬í•­: ë„¤íŠ¸ì›Œí¬ ì´ëŸ‰ì€ í•´ì € ì¼€ì´ë¸”ë¡œ ë‘ê³ , ë””ë°”ì´ìŠ¤ì™€ ë„¤íŠ¸ì›Œí¬ ì‚¬ì´ êµ¬ê°„ì—
    # ë‘ ê°œì˜ ì¤‘ê°„ê°’ì„ ìƒì„±í•´ ê°ê° ì™¸ë¶€ ë„¤íŠ¸ì›Œí¬, í™ˆ ë„¤íŠ¸ì›Œí¬ë¡œ í• ë‹¹
    try:
        device_g = float(((emissions_breakdown or {}).get('device') or {}).get('total_g') or 0)
        network_g = float(((emissions_breakdown or {}).get('network') or {}).get('total_g') or 0)
        # sea = network
        sea_g = network_g
        # ì™¸ë¶€/í™ˆì€ net~device êµ¬ê°„ì—ì„œ ë“±ë¶„ê°’ (ì˜ˆ: 1/3, 2/3)
        diff = device_g - network_g
        external_g = network_g + diff * (1.0/3.0)
        home_g = network_g + diff * (2.0/3.0)
        # ì†Œìˆ˜ ì •ë¦¬(í…œí”Œë¦¿ì—ì„œ %.2f í¬ë§· ì‚¬ìš©í•˜ë¯€ë¡œ ì›ê°’ì€ float ìœ ì§€)
        emissions_breakdown['network_detail'] = {
            'sea_g': sea_g,
            'external_g': external_g,
            'home_g': home_g,
        }
    except Exception:
        if isinstance(emissions_breakdown, dict):
            emissions_breakdown.setdefault('network_detail', {
                'sea_g': 0.0,
                'external_g': 0.0,
                'home_g': 0.0,
            })

    # 4) ì„œë¸Œí˜ì´ì§€ë³„ íƒ„ì†Œë°°ì¶œëŸ‰(g) ê³„ì‚° ë° í…œí”Œë¦¿ìš© ë°ì´í„° êµ¬ì„±
    enriched_subpages = []
    total_emission_g = 0.0
    # í˜„ì¬ ìš”ì²­ URL ì •ê·œí™”
    current_url_norm = normalize_url(url or '')
    try:
        for sp in (subpages or []):
            # spëŠ” dict ë˜ëŠ” ë¬¸ìì—´ì¼ ìˆ˜ ìˆìŒ
            # ë¨¼ì € URL ë™ì¼ì„± ê²€ì‚¬(ìš”ì²­ URLê³¼ ê°™ì€ í•­ëª© ì œì™¸)
            try:
                sp_url_raw = ''
                if isinstance(sp, dict):
                    sp_url_raw = str(sp.get('url') or '')
                else:
                    sp_url_raw = str(sp)
                sp_url_norm = normalize_url(sp_url_raw)
                if current_url_norm and sp_url_norm and (sp_url_norm == current_url_norm):
                    continue  # ë™ì¼ URLì€ ì œì™¸
            except Exception:
                pass

            if isinstance(sp, dict):
                kb = None
                if 'total_kb' in sp:
                    kb = sp.get('total_kb')
                elif 'total_bytes' in sp:
                    try:
                        kb = (float(sp.get('total_bytes') or 0) / 1024.0)
                    except Exception:
                        kb = 0.0
                emission_g = 0.0
                if kb is not None:
                    try:
                        emission_g = float(estimate_emission_from_kb(kb))
                    except Exception:
                        emission_g = 0.0
                sp_en = dict(sp)
                sp_en['emission_g'] = round(emission_g, 2)
                enriched_subpages.append(sp_en)
                total_emission_g += emission_g
            else:
                # ë¬¸ìì—´ URLë§Œ ìˆëŠ” ê²½ìš°
                enriched_subpages.append({'url': str(sp), 'emission_g': 0.0})
    except Exception:
        # ì‹¤íŒ¨í•˜ë”ë¼ë„ ê¸°ì¡´ subpagesë¡œ í´ë°±
        enriched_subpages = subpages or []
        total_emission_g = 0.0

    avg_emission_g = 0.0
    if enriched_subpages:
        try:
            cnt = max(1, len(enriched_subpages))
            avg_emission_g = round(total_emission_g / cnt, 2)
        except Exception:
            avg_emission_g = 0.0
    total_emission_g = round(total_emission_g, 2)

    # ìƒëŒ€ ë§‰ëŒ€ ê¸¸ì´ ê³„ì‚°ì„ ìœ„í•œ ìµœëŒ€ê°’
    max_emission_g = 0.0
    try:
        max_emission_g = max((sp.get('emission_g') or 0.0) for sp in enriched_subpages) if enriched_subpages else 0.0
    except Exception:
        max_emission_g = 0.0

    if max_emission_g > 0:
        for sp in enriched_subpages:
            try:
                pct = (float(sp.get('emission_g') or 0.0) / max_emission_g) * 100.0
            except Exception:
                pct = 0.0
            # ìµœì†Œ ê°€ì‹œì„± í™•ë³´ë¥¼ ìœ„í•´ 4% í•˜í•œ ì ìš© (0ì€ 0 ìœ ì§€)
            if pct > 0 and pct < 4:
                pct = 4.0
            sp['emission_pct'] = round(pct, 2)
    else:
        for sp in enriched_subpages:
            sp['emission_pct'] = 0.0

    # user-bar ë°ì´í„°ëŠ” ìœ„ì—ì„œ ì´ë¯¸ calculated ì„¹ì…˜ì—ì„œ ê°€ì ¸ì™”ìŒ (ì¤‘ë³µ ì¡°íšŒ ì œê±°)

    # SEO: ë©”íƒ€ ë°ì´í„° ë° Structured Data ìƒì„±
    meta = MetaDataGenerator.generate_detailed_analysis_meta(url or 'N/A')
    structured_data = [
        StructuredDataGenerator.generate_organization_schema(),
        StructuredDataGenerator.generate_breadcrumb_schema([
            {'name': 'í™ˆ', 'url': '/'},
            {'name': 'ë¶„ì„ ê²°ê³¼', 'url': f'/carbon_calculate_emission/{task_id}' if task_id else '/'},
            {'name': 'ìƒì„¸ ë¶„ì„', 'url': '/detailed-analysis'}
        ])
    ]

    # ì½˜í…ì¸  ì¹´ìš´íŠ¸ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”
    if 'content_count_data' not in locals():
        content_count_data = []
    
    return render_template(
        'pages/analysis/detailed_analysis.html',
        task_id=task_id,  # task_id ì¶”ê°€
        url=url or 'N/A',
        subpages=enriched_subpages,
        emissions_breakdown=emissions_breakdown,
        content_emission_data=content_emission_data,  # ì½˜í…ì¸  ìœ í˜•ë³„ ë°°ì¶œëŸ‰ ë°ì´í„°
        content_count_data=content_count_data,  # ì½˜í…ì¸  íƒ€ì…ë³„ ì¹´ìš´íŠ¸ ë°ì´í„° (íŒŒì´ì°¨íŠ¸ìš©)
        avg_emission_g=avg_emission_g,
        total_emission_g=total_emission_g,
        # user-bar ë°ì´í„°
        korea_carbon_percentage_diff=korea_carbon_percentage_diff,
        korea_comparison_status=korea_comparison_status,  # DEPRECATED
        korea_emission_status=korea_emission_status,  # for i18n
        meta=meta,  # SEO meta data
        structured_data=structured_data  # Schema.org JSON-LD
    )

# ==========================================================================
# ğŸŒ± ì§€ì† ê°€ëŠ¥ì„± ê°€ì´ë“œë¼ì¸ í˜ì´ì§€ ë¼ìš°íŠ¸
# ==========================================================================
@main_bp.route('/guidelines')
def guidelines_page():
    # í˜ì´ì§€ ì¡°íšŒ ë¡œê¹…
    task_id_for_logging = session.get('last_completed_task_id')
    log_page_view('sustainability_analysis', task_id=task_id_for_logging)
    
    # Use module-level DATA_FILE_PATH (points to data/urls/wsg_guideline.json)
    json_paths_to_try = [
        DATA_FILE_PATH,
        os.path.join(current_app.root_path, 'data', 'urls', 'wsg_guideline.json'),
        os.path.join(current_app.root_path, '..', 'data', 'guidelines.json'),
    ]

    full_json_data = {}
    last_error = None
    for path in json_paths_to_try:
        try:
            with open(path, 'r', encoding='utf-8') as f:
                full_json_data = json.load(f)
                current_app.logger.info(f"Loaded guidelines JSON from: {path}")
                break
        except FileNotFoundError as e:
            last_error = e
            continue
        except json.JSONDecodeError as e:
            last_error = e
            break

    if not full_json_data:
        current_app.logger.error(f"Failed to load guidelines JSON. Last error: {last_error}")
        flash('ì§€ì† ê°€ëŠ¥ì„± ê°€ì´ë“œë¼ì¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ í˜•ì‹ ì˜¤ë¥˜ì…ë‹ˆë‹¤.', 'error')

    processed_guidelines = []
    if isinstance(full_json_data, dict):
        categories = full_json_data.get('category', [])
        if isinstance(categories, list):
            for category_item in categories:
                if isinstance(category_item, dict):
                    guidelines_in_category = category_item.get('guidelines', [])
                    # Use shortName for prefix (e.g., 'UX'), default to 'CAT' if not found
                    category_prefix = category_item.get('shortName', category_item.get('name', 'CAT'))
                    # Simplify prefix if it's like 'UX Design' to just 'UX'
                    if isinstance(category_prefix, str) and ' ' in category_prefix:
                        category_prefix = category_prefix.split(' ')[0]

                    if isinstance(guidelines_in_category, list):
                        for guideline_data in guidelines_in_category:
                            if isinstance(guideline_data, dict):
                                display_id = f"{category_prefix}-{guideline_data.get('id', 'N/A')}"
                                title = guideline_data.get('guideline', 'No title provided')

                                # Extract description from the first item in criteria list
                                criteria_list = guideline_data.get('criteria', [])
                                description_text = 'N/A'
                                if criteria_list and isinstance(criteria_list, list) and len(criteria_list) > 0 and isinstance(criteria_list[0], dict):
                                    description_text = criteria_list[0].get('description', 'N/A')

                                # Explicit benefits processing to ensure a dictionary is passed to the template
                                current_benefits_value_for_template = {} # Default to an empty dict
                                raw_benefits_from_json = guideline_data.get('benefits')

                                if isinstance(raw_benefits_from_json, list):
                                    if len(raw_benefits_from_json) > 0:
                                        first_item_in_benefits_list = raw_benefits_from_json[0]
                                        if isinstance(first_item_in_benefits_list, dict):
                                            current_benefits_value_for_template = first_item_in_benefits_list

                                effort_str = guideline_data.get('effort')
                                impact_str = guideline_data.get('impact')
                                effort_display_val = get_level_display(effort_str)
                                impact_display_val = get_level_display(impact_str)

                                # Mock compliance status
                                compliance_status = random.choice([True, False])

                                processed_guidelines.append({
                                    'id': display_id,
                                    'title': title,
                                    'area': category_item.get('name', 'N/A'),
                                    'effort': guideline_data.get('effort'), # Value from JSON
                                    'impact': guideline_data.get('impact'), # Value from JSON
                                    'effort_display': effort_display_val,
                                    'impact_display': impact_display_val,
                                    'description': description_text,
                                    'intent': guideline_data.get('intent', 'N/A'),
                                    'benefits': current_benefits_value_for_template, # Guaranteed to be a dict
                                    'compliance_status': compliance_status  # Added compliance status
                                })
    
    # Prepare top_urgent_items
    non_compliant_guidelines = [g for g in processed_guidelines if not g['compliance_status']]

    level_to_numeric = {
        "High": 3, "ë†’ìŒ": 3,
        "Medium": 2, "ì¤‘ê°„": 2,
        "Low": 1, "ë‚®ìŒ": 1
    }

    def calculate_priority_score(guideline):
        impact_str = guideline.get('impact')
        effort_str = guideline.get('effort')
        
        numeric_impact = level_to_numeric.get(impact_str, 0) # Default to 0 if unknown
        numeric_effort = level_to_numeric.get(effort_str, 1) # Default to 1 if unknown to avoid division by zero
        
        if numeric_effort == 0: # Should not happen with default 1
            return 0 
        return numeric_impact / numeric_effort

    # Sort non-compliant guidelines by the calculated priority score, descending
    non_compliant_guidelines.sort(key=calculate_priority_score, reverse=True)
    
    top_urgent_items_for_stats = []
    for g in non_compliant_guidelines[:3]: # Take top 3
        top_urgent_items_for_stats.append({
            'id': g['id'],
            'title': g['title'],
            'effort_display': g['effort_display'],
            'impact_display': g['impact_display'],
            # Add original effort/impact if needed by other parts of template, though not for display here
            'effort': g['effort'], 
            'impact': g['impact'] 
        })

    stats_data = {
        'overall_score': '56/92', # Placeholder
        'categories': [
            {'name': 'UX', 'score': '12/29', 'priority': 10},
            {'name': 'Hosting', 'score': '10/12', 'priority': 7},
            {'name': 'Web Design', 'score': '15/22', 'priority': 8},
            {'name': 'BM', 'score': '19/29', 'priority': 6}
        ],
        'radar_chart_labels': ['UX', 'Hosting', 'Web Design', 'BM'], # Updated labels
        'radar_chart_data': [12, 10, 15, 19], # Example data for radar chart
        'score_trend_labels': ['Jan', 'Feb', 'Mar', 'Apr'], # Example labels for line chart
        'score_trend_data': [50, 55, 52, 60], # Example data for line chart
        'top_urgent_items': top_urgent_items_for_stats
    }

    # Calculate category scores and priorities for 'ì¹´í…Œê³ ë¦¬ë³„ ìƒì„¸'
    # Phase 4: ì„¸ì…˜ì—ì„œ task_idì™€ url ê°€ì ¸ì˜¤ê¸° (sidebar í‘œì‹œìš©)
    task_id = session.get('last_completed_task_id')
    url = None
    korea_carbon_percentage_diff = None
    korea_comparison_status = None  # DEPRECATED
    korea_emission_status = 'below_avg'  # for i18n

    if task_id:
        try:
            mongo_db = db.get_db()
            task_doc = mongo_db.task_results.find_one(
                {'_id': task_id},
                {'result.url': 1, 'result.calculated': 1}
            )
            if task_doc and 'result' in task_doc:
                result = task_doc['result']
                url = result.get('url')
                # user-bar ë°ì´í„°
                calculated = result.get('calculated', {})
                korea_carbon_percentage_diff = calculated.get('korea_carbon_percentage_diff')
                korea_comparison_status = calculated.get('korea_comparison_status')  # DEPRECATED
                korea_emission_status = calculated.get('korea_emission_status', 'below_avg')
        except Exception as e:
            current_app.logger.warning(f'guidelines_pageì—ì„œ URL ì¡°íšŒ ì‹¤íŒ¨: {e}')

    # SEO: ë©”íƒ€ ë°ì´í„° ë° Structured Data ìƒì„±
    meta = MetaDataGenerator.generate_guidelines_meta()
    structured_data = [
        StructuredDataGenerator.generate_organization_schema(),
        StructuredDataGenerator.generate_breadcrumb_schema([
            {'name': 'í™ˆ', 'url': '/'},
            {'name': 'ì§€ì†ê°€ëŠ¥ì„± ê°€ì´ë“œë¼ì¸', 'url': '/guidelines'}
        ])
    ]

    return render_template('pages/analysis/sustainability_analysis.html',
                           task_id=task_id,
                           url=url,
                           guidelines=processed_guidelines,
                           stats=stats_data,
                           # user-bar ë°ì´í„°
                           korea_carbon_percentage_diff=korea_carbon_percentage_diff,
                           korea_comparison_status=korea_comparison_status,  # DEPRECATED
                           korea_emission_status=korea_emission_status,  # for i18n
                           meta=meta,  # SEO meta data
                           structured_data=structured_data)  # Schema.org JSON-LD
# ==========================================================================
# âœ… í”„ë¡œì„¸ìŠ¤ ê´€ë¦¬ í•¨ìˆ˜
# ==========================================================================
def process_queued_tasks():
    db = get_db()
    active_tasks = get_active_celery_tasks()
    CELERY_QUEUE_THRESHOLD = 5

    if active_tasks < CELERY_QUEUE_THRESHOLD:
        # Find the oldest queued task and update its status to prevent race conditions
        queued_task_doc = db.task_results.find_one_and_update(
            {'status': 'QUEUED'},
            {'$set': {'status': 'PROCESSING'}},
            sort=[('created_at', 1)]
        )

        if queued_task_doc:
            url = queued_task_doc['url']
            user_id = queued_task_doc.get('user_id', 'anonymous')
            is_mobile = queued_task_doc.get('is_mobile', False)
            original_task_id = queued_task_doc['_id'] # This is the original, unique ID from the queue
            existing_lighthouse_data = queued_task_doc.get('existing_lighthouse_data')

            # Submit the task to Celery, ê¸°ì¡´ ë°ì´í„°ê°€ ìˆìœ¼ë©´ í•¨ê»˜ ì „ë‹¬
            if existing_lighthouse_data:
                task = analyze_url_task.delay(url, user_id, is_mobile, original_task_id,
                                            perform_subpage_crawling=True, existing_view_data=existing_lighthouse_data)
                print(f'[í ì²˜ë¦¬] ê¸°ì¡´ Lighthouse ë°ì´í„° í™œìš©í•˜ì—¬ ì‘ì—… ì‹œì‘: {original_task_id}')
            else:
                task = analyze_url_task.delay(url, user_id, is_mobile, original_task_id)
            new_celery_id = task.id

            # Update the document with the new Celery task ID and a proper initial state
            db.task_results.update_one(
                {'_id': original_task_id}, # Find by the original, unique ID
                {'$set': {'celery_task_id': new_celery_id, 'status': 'PENDING'}}
            )

            current_app.logger.info(f"Queued task {original_task_id} for {url} started with new Celery ID {new_celery_id}.")
        else:
            current_app.logger.debug("No queued tasks to process.")
    else:
        current_app.logger.debug(f"Queue processing skipped. Active tasks: {active_tasks}")


# ==========================================================================
# ğŸ“ new-ui íŒŒì¼ ì œê³µ ë¼ìš°íŠ¸ 
# ==========================================================================
@main_bp.route('/pages/main/<path:filename>')
def serve_ui_files(filename):
    return send_from_directory(os.path.join(current_app.root_path, 'templates', 'pages', 'main'), filename)

# ==========================================================================
# ğŸ“¸ ìº¡ì²˜ ì´ë¯¸ì§€ ì„œë¹™ ë¼ìš°íŠ¸
# ==========================================================================
@main_bp.route('/var/captures/<path:filename>')
def serve_capture_image(filename):
    """var/captures ë””ë ‰í† ë¦¬ì˜ ìº¡ì²˜ ì´ë¯¸ì§€ íŒŒì¼ì„ ì„œë¹™"""
    from ecoweb.config import Config
    import os
    
    # var/captures ë””ë ‰í† ë¦¬ ê²½ë¡œ
    captures_dir = Config.CAPTURE_FOLDER
    file_path = os.path.join(captures_dir, filename)
    
    # ë³´ì•ˆ ì²´í¬: captures_dir ë°–ìœ¼ë¡œ ë‚˜ê°€ëŠ” ê²½ë¡œ ì°¨ë‹¨
    captures_dir_abs = os.path.abspath(captures_dir)
    file_path_abs = os.path.abspath(file_path)
    
    if not file_path_abs.startswith(captures_dir_abs):
        current_app.logger.warning(f"ë³´ì•ˆ ìœ„í˜‘: ìº¡ì²˜ ë””ë ‰í† ë¦¬ ë°–ìœ¼ë¡œ ì ‘ê·¼ ì‹œë„: {file_path}")
        return jsonify({'error': 'Invalid path'}), 403
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(file_path):
        current_app.logger.warning(f"ìº¡ì²˜ ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        return jsonify({'error': 'File not found'}), 404
    
    # ì´ë¯¸ì§€ íŒŒì¼ ì„œë¹™
    return send_from_directory(captures_dir, filename, mimetype='image/png')

# ==========================================================================
# ğŸ–¼ï¸ ì´ë¯¸ì§€ íŒŒì¼ ì„œë¹™ ë¼ìš°íŠ¸ (var/optimization_images ë””ë ‰í† ë¦¬)
# ==========================================================================
@main_bp.route('/var/optimization_images/<path:filename>')
def serve_image_file(filename):
    """var/optimization_images ë””ë ‰í† ë¦¬ì˜ ì´ë¯¸ì§€ íŒŒì¼ì„ ì„œë¹™"""
    from ecoweb.config import Config
    import os
    from mimetypes import guess_type
    
    # var/optimization_images ë””ë ‰í† ë¦¬ ê²½ë¡œ
    images_dir = Config.OPTIMIZATION_IMAGES_FOLDER
    file_path = os.path.join(images_dir, filename)
    
    # ë³´ì•ˆ ì²´í¬: images_dir ë°–ìœ¼ë¡œ ë‚˜ê°€ëŠ” ê²½ë¡œ ì°¨ë‹¨
    images_dir_abs = os.path.abspath(images_dir)
    file_path_abs = os.path.abspath(file_path)
    
    if not file_path_abs.startswith(images_dir_abs):
        current_app.logger.warning(f"ë³´ì•ˆ ìœ„í˜‘: ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ë°–ìœ¼ë¡œ ì ‘ê·¼ ì‹œë„: {file_path}")
        return jsonify({'error': 'Invalid path'}), 403
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(file_path):
        current_app.logger.warning(f"ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        return jsonify({'error': 'File not found'}), 404
    
    # MIME íƒ€ì… ìë™ ê°ì§€
    mime_type, _ = guess_type(file_path)
    if not mime_type:
        # íŒŒì¼ í™•ì¥ì ê¸°ë°˜ MIME íƒ€ì… ì„¤ì •
        if filename.lower().endswith('.webp'):
            mime_type = 'image/webp'
        elif filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.svg')):
            mime_type = 'image/png' if filename.lower().endswith('.png') else \
                       'image/jpeg' if filename.lower().endswith(('.jpg', '.jpeg')) else \
                       'image/gif' if filename.lower().endswith('.gif') else 'image/svg+xml'
        else:
            mime_type = 'application/octet-stream'
    
    # ì´ë¯¸ì§€ íŒŒì¼ ì„œë¹™
    return send_from_directory(images_dir, filename, mimetype=mime_type)

# Path for sustainability guidelines JSON
DATA_FILE_PATH = os.path.join(os.path.dirname(__file__), '..', 'static', 'data', 'urls', 'wsg_guideline.json')

def get_level_display(level_str):
    """Converts 'ë‚®ìŒ', 'ì¤‘ê°„', 'ë†’ìŒ' to a star rating string."""
    if level_str == "ë‚®ìŒ" or level_str == "Low":
        return "â˜…â˜†â˜†"
    elif level_str == "ì¤‘ê°„" or level_str == "Medium":
        return "â˜…â˜…â˜†"
    elif level_str == "ë†’ìŒ" or level_str == "High":
        return "â˜…â˜…â˜…"
    return "--- " # Default for unknown, added a space to ensure it's not empty visually

# ==========================================================================
# â³ URL ë¶„ì„ ë¡œë”© í˜ì´ì§€ 
# ==========================================================================
@main_bp.route('/loading/<task_id>')
def loading(task_id):
    # [1] Phase 4: URLì€ DBì—ì„œ ì¡°íšŒ (ì„¸ì…˜ ì‚¬ìš© ì•ˆ í•¨)
    # MongoDB Projection: URL í•„ë“œë§Œ ì¡°íšŒí•˜ì—¬ ì„±ëŠ¥ ìµœì í™”
    url = ''
    try:
        mongo_db = db.get_db()
        task_doc = mongo_db.task_results.find_one(
            {'_id': task_id},
            {'url': 1, '_id': 0}  # Projection: URLë§Œ ê°€ì ¸ì˜¤ê¸°
        )
        if task_doc:
            # URLì€ task_results ë¬¸ì„œì— ì§ì ‘ ì €ì¥ë˜ì–´ ìˆìŒ
            url = task_doc.get('url', '')
    except Exception as e:
        current_app.logger.warning(f'ë¡œë”© í˜ì´ì§€ì—ì„œ URL ì¡°íšŒ ì‹¤íŒ¨: {e}')

    return render_template('pages/common/loading.html', task_id=task_id, url=url)

# ==========================================================================
# âœ”ï¸ URL ë¶„ì„ ìƒíƒœ í™•ì¸ ì—”ë“œí¬ì¸íŠ¸ 
# ==========================================================================
@main_bp.route('/check_status/<task_id>')
def check_status(task_id):
    try:
        db = get_db()
    except Exception as e:
        # MongoDB ì—°ê²° ì‹¤íŒ¨ ì‹œ ì ì ˆí•œ ì—ëŸ¬ ì‘ë‹µ ë°˜í™˜
        current_app.logger.error(f"MongoDB ì—°ê²° ì‹¤íŒ¨ (check_status): {str(e)}")
        return jsonify({
            'status': 'ERROR',
            'error': 'ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.',
            'error_type': 'DATABASE_CONNECTION_ERROR'
        }), 503  # Service Unavailable
    
    try:
        # The task_id from the URL is our original_task_id (UUID)
        # MongoDB Projection: ìƒíƒœ í™•ì¸ì— í•„ìš”í•œ í•„ë“œë§Œ ì¡°íšŒ
        task_doc = db.task_results.find_one(
            {'_id': task_id},
            {
                'status': 1,
                'progress': 1,
                'celery_task_id': 1,
                'created_at': 1,
                'cancellation_reason': 1,
                'cancelled_at': 1
            }
        )

        if not task_doc:
            return jsonify({'status': 'NOT_FOUND'}), 404

        # If the task status is final in our DB, we can trust it.
        if task_doc.get('status') in ['SUCCESS', 'FAILURE', 'MEASUREMENT_COMPLETE', 'CANCELLED']:
            # Phase 4: ë” ì´ìƒ ì„¸ì…˜ì— subpagesë¥¼ ì €ì¥í•˜ì§€ ì•ŠìŒ (DBì—ì„œ ì§ì ‘ ì½ê¸°)
            response_data = {
                'status': task_doc['status'],
                'progress': task_doc.get('progress')
            }

            # ì·¨ì†Œëœ ì‘ì—…ì˜ ê²½ìš° ì¶”ê°€ ì •ë³´ í¬í•¨
            if task_doc.get('status') == 'CANCELLED':
                response_data['cancellation_reason'] = task_doc.get('cancellation_reason', 'unknown')
                response_data['cancelled_at'] = task_doc.get('cancelled_at')

            return jsonify(response_data)

        # If the task is queued, calculate its position.
        if task_doc.get('status') == 'QUEUED':
            queued_tasks_before = db.task_results.count_documents({
                'status': 'QUEUED',
                'created_at': {'$lt': task_doc.get('created_at', datetime.now(timezone.utc))}
            })
            queue_position = queued_tasks_before + 1
            return jsonify({'status': 'QUEUED', 'queue_position': queue_position, 'progress': task_doc.get('progress')})

        # If the task is PENDING, PROCESSING, or STARTED, check Celery for a more current state.
        celery_task_id = task_doc.get('celery_task_id')
        if not celery_task_id:
            # This can happen if the task is queued but not yet processed by process_queued_tasks
            return jsonify({'status': 'QUEUED', 'queue_position': 'N/A', 'progress': task_doc.get('progress')})

        task_result = AsyncResult(celery_task_id, app=celery)

        # Phase 2 ìˆ˜ì •: Celery ìƒíƒœê°€ SUCCESSì´ê³  DBê°€ ì•„ì§ ì—…ë°ì´íŠ¸ ì•ˆ ëœ ê²½ìš° ì²˜ë¦¬
        celery_state = task_result.state

        # Celeryê°€ SUCCESSì´ë©´ DB ìƒíƒœ í•œ ë²ˆ ë” í™•ì¸ (ì¬ì¡°íšŒ)
        if celery_state == 'SUCCESS':
            # DBì—ì„œ ìµœì‹  ìƒíƒœ ì¬ì¡°íšŒ
            task_doc_refresh = db.task_results.find_one(
                {'_id': task_doc['_id']},
                {'status': 1, 'progress': 1}
            )
            if task_doc_refresh:
                db_status = task_doc_refresh.get('status')
                # DBì— MEASUREMENT_COMPLETE ë˜ëŠ” SUCCESSê°€ ì“°ì˜€ìœ¼ë©´ ê·¸ê²ƒ ë°˜í™˜
                if db_status in ['SUCCESS', 'MEASUREMENT_COMPLETE']:
                    return jsonify({
                        'status': db_status,
                        'progress': task_doc_refresh.get('progress')
                    })

            # DBì— ì•„ì§ ì•ˆ ì“°ì˜€ìœ¼ë©´ Celery SUCCESSë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜
            # (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ SUCCESSë„ ì™„ë£Œ ìƒíƒœë¡œ ì²˜ë¦¬)
            return jsonify({
                'status': 'SUCCESS',
                'progress': task_doc.get('progress')
            })

        # Return the current state from Celery. The final state will be written to DB by the task itself.
        meta = None
        try:
            meta = task_result.info if hasattr(task_result, 'info') else None
        except Exception:
            meta = None
        return jsonify({'status': celery_state, 'progress': task_doc.get('progress'), 'meta': meta})
    
    except Exception as e:
        # MongoDB ì¿¼ë¦¬ ì¤‘ ë°œìƒí•œ ì˜ˆì™¸ ì²˜ë¦¬
        current_app.logger.error(f"MongoDB ì¿¼ë¦¬ ì˜¤ë¥˜ (check_status): {str(e)}")
        return jsonify({
            'status': 'ERROR',
            'error': 'ì‘ì—… ìƒíƒœë¥¼ í™•ì¸í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.',
            'error_type': 'DATABASE_QUERY_ERROR'
        }), 500

# ==========================================================================
# ğŸš« ì‘ì—… ì·¨ì†Œ ë¼ìš°íŠ¸
# ==========================================================================
@main_bp.route('/cancel_task/<task_id>', methods=['POST'])
def cancel_task(task_id):
    """
    ëª¨ë“  ìœ í˜•ì˜ ì‘ì—…ì„ ì·¨ì†Œí•©ë‹ˆë‹¤.

    ì§€ì›í•˜ëŠ” ì‘ì—… ìœ í˜•:
    - analyze_url_task (Lighthouse ë¶„ì„)
    - í–¥í›„ ì¶”ê°€ë  ë‹¤ë¥¸ Celery ì‘ì—…ë“¤
    """
    current_app.logger.info(f"[CANCEL_ENDPOINT] Received cancellation request for task: {task_id}")
    current_app.logger.info(f"[CANCEL_ENDPOINT] Request method: {request.method}, Content-Type: {request.content_type}")
    current_app.logger.info(f"[CANCEL_ENDPOINT] Request data: {request.get_data()}")

    try:
        db_handle = get_db()
        # MongoDB Projection: ì·¨ì†Œ ì²˜ë¦¬ì— í•„ìš”í•œ í•„ë“œë§Œ ì¡°íšŒ
        task_doc = db_handle.task_results.find_one(
            {'_id': task_id},
            {
                'status': 1,
                'celery_task_id': 1,
                'progress': 1
            }
        )

        if not task_doc:
            return jsonify({'status': 'error', 'message': 'Task not found'}), 404

        # ì´ë¯¸ ì·¨ì†Œëœ ì‘ì—…ì€ ì¤‘ë³µ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ
        if task_doc.get('status') == 'CANCELLED':
            log_task_cancellation(task_id, "already_cancelled", current_app.logger)
            return jsonify({'status': 'already_cancelled', 'message': 'Task already cancelled'})

        # ìš”ì²­ì—ì„œ ì·¨ì†Œ ì‚¬ìœ  ì¶”ì¶œ
        cancellation_reason = 'user_cancelled'
        if request.json:
            cancellation_reason = request.json.get('reason', 'user_cancelled')
        
        # ì´ë²¤íŠ¸ ë¡œê¹…: ë¶„ì„ ì·¨ì†Œ
        user_id = session.get('user_id')
        log_analysis_cancel(task_id, user_id=str(user_id) if user_id else None)

        # ì™„ë£Œëœ ì‘ì—…ì´ë¼ë„ Celery ì›Œì»¤ê°€ ì•„ì§ ì‹¤í–‰ ì¤‘ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì •ë¦¬
        completed_statuses = ['SUCCESS', 'FAILURE', 'MEASUREMENT_COMPLETE']
        if task_doc.get('status') in completed_statuses:
            current_app.logger.info(f"Task {task_id} is {task_doc.get('status')}, but cleaning up any remaining processes")

            cleanup_success = _cleanup_celery_task(task_doc.get('celery_task_id'), task_id)
            log_task_cancellation(task_id, f"cleanup_after_completion:{cancellation_reason}", current_app.logger)

            return jsonify({
                'status': 'cleaned_up',
                'message': 'Task completed but background processes cleaned up',
                'cleanup_success': cleanup_success
            })

        # ì§„í–‰ ì¤‘ì¸ ì‘ì—… ì·¨ì†Œ
        revoke_success = _cleanup_celery_task(task_doc.get('celery_task_id'), task_id)

        # MongoDBì—ì„œ ì‘ì—… ìƒíƒœë¥¼ ì·¨ì†Œë¨ìœ¼ë¡œ ì—…ë°ì´íŠ¸
        update_data = {
            'status': 'CANCELLED',
            'cancelled_at': datetime.utcnow(),
            'cancellation_reason': cancellation_reason,
            'progress.updated_at': datetime.utcnow().isoformat(),
            'celery_revoke_success': revoke_success
        }

        # ì§„í–‰ ë‹¨ê³„ë³„ ì·¨ì†Œ ìƒíƒœ ì—…ë°ì´íŠ¸ (ì¼ë°˜í™”)
        _update_progress_steps_cancelled(task_doc, update_data)

        result = db_handle.task_results.update_one(
            {'_id': task_id},
            {'$set': update_data}
        )

        if result.modified_count > 0:
            log_task_cancellation(task_id, cancellation_reason, current_app.logger)
            current_app.logger.info(f"Task {task_id} successfully cancelled by user. Reason: {cancellation_reason}")
            return jsonify({
                'status': 'success',
                'message': 'Task cancelled successfully',
                'cancellation_reason': cancellation_reason,
                'celery_revoke_success': revoke_success
            })
        else:
            current_app.logger.warning(f"Failed to update task {task_id} status to cancelled")
            return jsonify({'status': 'error', 'message': 'Failed to cancel task'}), 500

    except Exception as e:
        current_app.logger.error(f"Error cancelling task {task_id}: {e}", exc_info=True)
        return jsonify({'status': 'error', 'message': 'Internal server error'}), 500


def _cleanup_celery_task(celery_task_id, task_id):
    """
    Celery ì‘ì—…ì„ ì •ë¦¬í•©ë‹ˆë‹¤.

    Returns:
        bool: ì •ë¦¬ ì„±ê³µ ì—¬ë¶€
    """
    if not celery_task_id:
        return True  # Celery IDê°€ ì—†ìœ¼ë©´ ì •ë¦¬í•  ê²ƒë„ ì—†ìŒ

    try:
        task_result = AsyncResult(celery_task_id, app=celery)

        # ì‹¤í–‰ ì¤‘ì¸ ì‘ì—…ë§Œ revoke
        if task_result.state in ['PENDING', 'STARTED', 'PROGRESS']:
            task_result.revoke(terminate=True)
            current_app.logger.info(f"Celery task {celery_task_id} revoked for task_id {task_id}")
            return True
        else:
            current_app.logger.info(f"Celery task {celery_task_id} was in state {task_result.state}, no revoke needed")
            return True

    except Exception as e:
        current_app.logger.warning(f"Failed to revoke Celery task {celery_task_id}: {e}")
        return False


def _update_progress_steps_cancelled(task_doc, update_data):
    """
    ì‘ì—…ì˜ ì§„í–‰ ë‹¨ê³„ë¥¼ ì·¨ì†Œ ìƒíƒœë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.

    ë‹¤ì–‘í•œ ì‘ì—… ìœ í˜•ì˜ ë‹¨ê³„ êµ¬ì¡°ë¥¼ ìë™ìœ¼ë¡œ ê°ì§€í•˜ì—¬ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    try:
        progress = task_doc.get('progress', {})
        current_step = progress.get('current_step')
        steps = progress.get('steps', {})

        # í˜„ì¬ ë‹¨ê³„ê°€ ìˆìœ¼ë©´ í•´ë‹¹ ë‹¨ê³„ë¥¼ ì·¨ì†Œë¡œ ë§ˆí¬
        if current_step and current_step in steps:
            update_data[f'progress.steps.{current_step}'] = {
                'status': 'cancelled',
                'message': 'ì‚¬ìš©ìì— ì˜í•´ ì·¨ì†Œë¨'
            }

        # ì•Œë ¤ì§„ ë‹¨ê³„ë“¤ë„ ì²˜ë¦¬ (í•˜ìœ„ í˜¸í™˜ì„±)
        known_steps = ['input', 'subpages', 'image_opt', 'processing', 'analysis', 'output']
        for step in known_steps:
            if step in steps and steps[step].get('status') == 'in_progress':
                update_data[f'progress.steps.{step}'] = {
                    'status': 'cancelled',
                    'message': 'ì‚¬ìš©ìì— ì˜í•´ ì·¨ì†Œë¨'
                }

    except Exception as e:
        current_app.logger.warning(f"Failed to update progress steps for cancelled task: {e}")

# ==========================================================================
# ğŸ–±ï¸ í´ë¦­ ì´ë²¤íŠ¸ ë¡œê¹… ë¼ìš°íŠ¸ (ê¸°ì¡´ - í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)
# ==========================================================================
@main_bp.route('/log-click', methods=['POST'])
def log_click_event():
    data = request.get_json()
    if not data or 'element_id' not in data or 'page_url' not in data:
        return jsonify({'status': 'error', 'message': 'Missing data'}), 400

    session_id = session.sid
    element_id = data['element_id']
    page_url = data['page_url']

    # ë°ì´í„°ë² ì´ìŠ¤ì— í´ë¦­ ì´ë²¤íŠ¸ ê¸°ë¡ (ì¤‘ë³µ ë°©ì§€)
    try:
        mongo_db = db.get_db()
        click_events = mongo_db.click_events
        
        # session_idì™€ element_idë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê³ ìœ í•œ í´ë¦­ì„ ë³´ì¥
        click_events.update_one(
            {'session_id': session_id, 'element_id': element_id},
            {
                '$setOnInsert': {
                    'session_id': session_id,
                    'element_id': element_id,
                    'page_url': page_url,
                    'timestamp': datetime.utcnow()
                }
            },
            upsert=True
        )
        return jsonify({'status': 'success', 'message': 'Click logged'})
    except Exception as e:
        current_app.logger.error(f"Error logging click event: {e}")
        return jsonify({'status': 'error', 'message': 'Internal server error'}), 500


# ==========================================================================
# ğŸ“Š ì‚¬ìš©ì ì´ë²¤íŠ¸ ë¡œê¹… API (ì„ íƒì  - ìˆ˜ë™ í˜¸ì¶œìš©)
# ==========================================================================
# ì°¸ê³ : í˜„ì¬ëŠ” ì„œë²„ ì‚¬ì´ë“œ ë¡œê¹…ë§Œ ì‚¬ìš©í•˜ë©°, í´ë¼ì´ì–¸íŠ¸ ì‚¬ì´ë“œ ìë™ ì¶”ì ì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
# ì´ ì—”ë“œí¬ì¸íŠ¸ëŠ” í•„ìš” ì‹œ ìˆ˜ë™ìœ¼ë¡œ ì´ë²¤íŠ¸ë¥¼ ê¸°ë¡í•˜ê¸° ìœ„í•´ ìœ ì§€ë©ë‹ˆë‹¤.
@main_bp.route('/api/log-event', methods=['POST'])
def log_event():
    """
    ìˆ˜ë™ìœ¼ë¡œ ì‚¬ìš©ì ì´ë²¤íŠ¸ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤.
    (í˜„ì¬ëŠ” ì„œë²„ ì‚¬ì´ë“œ ë¡œê¹…ë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)
    
    Request Body:
    {
        "event_type": "button_click",
        "event_category": "navigation",
        "element_id": "measureBtn",
        "metadata": {...}
    }
    """
    if not is_logging_enabled():
        return jsonify({'status': 'disabled', 'message': 'Event logging is disabled'}), 200
    
    try:
        data = request.get_json()
        if not data:
            return jsonify({'status': 'error', 'message': 'Missing JSON data'}), 400
        
        event_type = data.get('event_type')
        event_category = data.get('event_category', 'navigation')
        element_id = data.get('element_id')
        metadata = data.get('metadata')
        
        if not event_type:
            return jsonify({'status': 'error', 'message': 'event_type is required'}), 400
        
        # ì´ë²¤íŠ¸ ê¸°ë¡
        log_user_event(
            event_type=event_type,
            event_category=event_category,
            metadata=metadata,
            element_id=element_id
        )
        
        return jsonify({'status': 'success', 'message': 'Event logged'})
    except Exception as e:
        current_app.logger.error(f"Error logging event: {e}")
        return jsonify({'status': 'error', 'message': 'Internal server error'}), 500


@main_bp.route('/api/logging-status', methods=['GET'])
def logging_status():
    """
    ì´ë²¤íŠ¸ ë¡œê¹… í™œì„±í™” ìƒíƒœë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    (í˜„ì¬ëŠ” ì„œë²„ ì‚¬ì´ë“œ ë¡œê¹…ë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)
    """
    return jsonify({
        'enabled': is_logging_enabled()
    })



# ==========================================================================
# âœ¨ ì†Œê°œ í˜ì´ì§€
# ==========================================================================
@main_bp.route('/about')
def about():
    # SEO: ë©”íƒ€ ë°ì´í„° ë° Structured Data ìƒì„±
    meta = MetaDataGenerator.generate_about_meta()
    structured_data = [
        StructuredDataGenerator.generate_organization_schema(),
        StructuredDataGenerator.generate_breadcrumb_schema([
            {'name': 'í™ˆ', 'url': '/'},
            {'name': 'eCarbon ì†Œê°œ', 'url': '/about'}
        ])
    ]

    return render_template(
        'pages/main/about.html',
        meta=meta,
        structured_data=structured_data
    )

# ==========================================================================
# ğŸŸï¸ íšŒì›ê¶Œ í˜ì´ì§€
# ==========================================================================
@main_bp.route('/membership/plans')
def membership_plans():
    # SEO: ë©”íƒ€ ë°ì´í„° ë° Structured Data ìƒì„±
    meta = MetaDataGenerator.generate_page_meta(
        title="eCarbon íšŒì›ê¶Œ - í”„ë¦¬ë¯¸ì—„ í”Œëœ",
        description="eCarbon í”„ë¦¬ë¯¸ì—„ íšŒì›ê¶Œìœ¼ë¡œ ë¬´ì œí•œ ì›¹ì‚¬ì´íŠ¸ ë¶„ì„, ìš°ì„  ì§€ì›, ê³ ê¸‰ ë¦¬í¬íŠ¸ ê¸°ëŠ¥ì„ ì´ìš©í•˜ì„¸ìš”.",
        canonical_path="/membership/plans",
        og_type='website',
        keywords=['íšŒì›ê¶Œ', 'í”„ë¦¬ë¯¸ì—„', 'ë¬´ì œí•œ ë¶„ì„', 'eCarbon í”Œëœ']
    )
    structured_data = [
        StructuredDataGenerator.generate_organization_schema(),
        StructuredDataGenerator.generate_breadcrumb_schema([
            {'name': 'í™ˆ', 'url': '/'},
            {'name': 'íšŒì›ê¶Œ', 'url': '/membership/plans'}
        ])
    ]

    return render_template(
        'pages/membership/membership-plans.html',
        meta=meta,
        structured_data=structured_data
    )

# ==========================================================================
# ğŸ–ï¸ ë±ƒì§€ í˜ì´ì§€
# ==========================================================================
@main_bp.route('/badge')
def badge():
    # SEO: ë©”íƒ€ ë°ì´í„° ë° Structured Data ìƒì„±
    meta = MetaDataGenerator.generate_page_meta(
        title="eCarbon ë±ƒì§€ - ì¹œí™˜ê²½ ì›¹ì‚¬ì´íŠ¸ ì¸ì¦",
        description="eCarbon ë±ƒì§€ë¡œ ì¹œí™˜ê²½ ì›¹ì‚¬ì´íŠ¸ë¥¼ ì¸ì¦ë°›ê³ , ë°©ë¬¸ìì—ê²Œ í™˜ê²½ ë³´í˜¸ ë…¸ë ¥ì„ ì•Œë¦¬ì„¸ìš”.",
        canonical_path="/badge",
        og_type='website',
        keywords=['eCarbon ë±ƒì§€', 'ì¹œí™˜ê²½ ì¸ì¦', 'ì›¹ì‚¬ì´íŠ¸ ì¸ì¦', 'íƒ„ì†Œì¤‘ë¦½']
    )
    structured_data = [
        StructuredDataGenerator.generate_organization_schema(),
        StructuredDataGenerator.generate_breadcrumb_schema([
            {'name': 'í™ˆ', 'url': '/'},
            {'name': 'ë±ƒì§€', 'url': '/badge'}
        ])
    ]

    return render_template(
        'badge.html',  # ì‹¤ì œ í…œí”Œë¦¿ ê²½ë¡œ
        meta=meta,
        structured_data=structured_data
    )

# ==========================================================================
# ğŸš« ì—ëŸ¬ í˜ì´ì§€ 
# ==========================================================================
@main_bp.route('/error')
def error():
    return render_template('pages/error/error.html')

# ==================================================================================================
# @main_bp.route('/gov-analysis')
# def gov_analysis():
#     global_avg_carbon = session.get('global_avg_carbon')    
#     korea_avg_carbon = session.get('korea_avg_carbon')
#     carbon_emission = session.get('carbon_emission')
#     global_diff = session.get('global_diff')
#     korea_diff = session.get('korea_diff')
#     global_diff_abs = session.get('global_diff_abs')
#     korea_diff_abs = session.get('korea_diff_abs')
    
#     # MongoDBì—ì„œ monthly_stats ì½œë ‰ì…˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
#     from datetime import datetime, timedelta
#     import random
    
#     # í˜„ì¬ ë‚ ì§œì—ì„œ ì´ì „ ë‹¬ ì²«ë‚  êµ¬í•˜ê¸°
#     today = datetime.now()
#     last_month_start = datetime(today.year, today.month, 1) - timedelta(days=1)
#     last_month_start = datetime(last_month_start.year, last_month_start.month, 1)
    
#     # 12ê°œì›” ì „ ë‚ ì§œ ê³„ì‚°
#     twelve_months_ago = last_month_start - timedelta(days=365)
    
#     # ì´ì „ 12ê°œì›” ê° ì›”ì˜ ë‚ ì§œ ë° ì›”ë¬¸ìì—´ ì¤€ë¹„
#     month_dates = []
#     month_strings = []
    
#     for i in range(12):
#         # í˜„ì¬ë¡œë¶€í„° iê°œì›” ì´ì „
#         month_date = datetime(today.year, today.month, 1) - timedelta(days=30 * (i+1))
#         month_dates.append(month_date)
#         month_strings.append(month_date.strftime('%Y-%m'))
    
#     # DBì—ì„œ ë°ì´í„° ì¡°íšŒ ì‹œë„
#     monthly_emissions_data = []
#     db_data_months = set()  # DBì—ì„œ ê°€ì ¸ì˜¨ ë°ì´í„°ì˜ ì›” ê¸°ë¡
    
#     try:
#         # MongoDB ì—°ê²°
#         mongo_db = db.get_db()
        
#         # ì›”ë³„ í†µê³„ ì½œë ‰ì…˜ ì¡´ì¬ í™•ì¸
#         collections = mongo_db.list_collection_names()
#         print(f"MongoDB ì½œë ‰ì…˜ ëª©ë¡: {collections}")
        
#         if 'monthly_stats' in collections:
#             # ì¡°íšŒ ì¡°ê±´ ì¶œë ¥
#             print(f"twelve_months_ago: {twelve_months_ago}, last_month_start: {last_month_start}")
            
#             # monthly_stats ì½œë ‰ì…˜ì—ì„œ ìµœê·¼ 12ê°œì›” ë°ì´í„° ì¡°íšŒ
#             query = {'month': {'$gte': twelve_months_ago, '$lte': last_month_start}}
#             print(f"MongoDB ì¿¼ë¦¬: {query}")
            
#             try:
#                 monthly_stats_data = list(mongo_db.monthly_stats.find(query).sort('month', 1))
#                 print(f"monthly_stats ì½œë ‰ì…˜ ì¡°íšŒ ê²°ê³¼: {len(monthly_stats_data)}ê°œ ë°ì´í„°")
                
#                 # DBì—ì„œ ê°€ì ¸ì˜¨ ë°ì´í„° ì²˜ë¦¬
#                 if len(monthly_stats_data) > 0:
#                     for stat in monthly_stats_data:
#                         if 'month' in stat and 'avgEmission' in stat:
#                             month_str = stat['month'].strftime('%Y-%m')
#                             db_data_months.add(month_str)  # ì´ë¯¸ ì²˜ë¦¬í•œ ì›” ê¸°ë¡
                            
#                             monthly_emissions_data.append({
#                                 'month': month_str,
#                                 'avgEmission': round(stat['avgEmission'], 2)
#                             })
#                         else:
#                             print(f"monthly_stats ë°ì´í„° í˜•ì‹ ì˜¤ë¥˜: {stat.keys()}")
#             except Exception as inner_e:
#                 print(f"monthly_stats ì½œë ‰ì…˜ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(inner_e)}")
#     except Exception as e:
#         print(f"MongoDB ì—°ê²° ì˜¤ë¥˜: {str(e)}")
    
#     # DBì—ì„œ ê°€ì ¸ì˜¨ ë°ì´í„°ê°€ 12ê°œì›” ë³´ë‹¤ ì ìœ¼ë©´ ë‚˜ë¨¸ì§€ ì›”ì€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì±„ìš°ê¸°
#     if len(monthly_emissions_data) < 12:
#         print(f"DB ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ê°€: {len(monthly_emissions_data)}/12")
        
#         # ë¶€ì¡±í•œ ì›”ì— ëŒ€í•´ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
#         for month_str in month_strings:
#             if month_str not in db_data_months:
#                 # 1.5~2.0 ì‚¬ì´ì˜ ëœë¤ê°’ ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)
#                 random_emission = 1.5 + (random.random() * 0.5)
                
#                 monthly_emissions_data.append({
#                     'month': month_str,
#                     'avgEmission': round(random_emission, 2)
#                 })
    
#     # ì›” ìˆœì„œëŒ€ë¡œ ì •ë ¬
#     monthly_emissions_data.sort(key=lambda x: x['month'])
    
#     print(f"ì›”ë³„ ë°ì´í„° ìµœì¢… ê°œìˆ˜: {len(monthly_emissions_data)}ê°œ ë°ì´í„°")
    
#     return render_template('gov_analysis.html', 
#                         global_avg_carbon=global_avg_carbon,
#                         korea_avg_carbon=korea_avg_carbon,
#                         carbon_emission=carbon_emission,
#                         global_diff=global_diff,
#                         korea_diff=korea_diff,
#                         global_diff_abs=global_diff_abs,
#                         korea_diff_abs=korea_diff_abs,
#                         monthly_emissions_data=json.dumps(monthly_emissions_data))

# URL ë¶„ì„ ë¼ìš°íŠ¸ =========================================================================================
# @main_bp.route('/carbon_analysis', methods=['POST'])
# def carbon_analysis():

#     url = request.form.get('url', '').strip()
#     if url and not url.startswith('http://') and not url.startswith('https://'):
#         url = 'https://' + url

#     is_mobile = request.form.get('is_mobile') == 'true'

#     if not url:
#         return jsonify({'error': 'URL is required'}), 400

#     CELERY_QUEUE_THRESHOLD = 5  # ë™ì‹œì— ì‹¤í–‰í•  ìµœëŒ€ ì‘ì—… ìˆ˜
#     active_tasks = get_active_celery_tasks()
#     db = get_db()

#     if active_tasks >= CELERY_QUEUE_THRESHOLD:
#         task_id = str(uuid.uuid4())
#         db.task_results.insert_one({
#             '_id': task_id,
#             'status': 'QUEUED',
#             'url': url,
#             'user_id': 'anonymous',
#             'is_mobile': is_mobile,
#             'result': None,
#             'created_at': datetime.now(timezone.utc)
#         })
#         current_app.logger.info(f'Task {task_id} for {url} is queued. Active tasks: {active_tasks}')
#         return jsonify({'task_id': task_id})
#     else:
#         original_task_id = str(uuid.uuid4())
#         task = analyze_url_task.delay(url, 'anonymous', is_mobile, original_task_id, perform_subpage_crawling=False)
#         celery_task_id = task.id

#         # Store initial task info using the original_task_id
#         db.task_results.insert_one({
#             '_id': original_task_id, # Use our generated UUID as the primary key
#             'celery_task_id': celery_task_id,
#             'status': 'PENDING',
#             'url': url,
#             'user_id': 'anonymous',
#             'is_mobile': is_mobile,
#             'created_at': datetime.now(timezone.utc)
#         })

#         current_app.logger.info(f"Task for {url} started immediately. Original Task ID: {original_task_id}, Celery ID: {celery_task_id}")
#         return jsonify({'task_id': original_task_id})
#         return jsonify({'task_id': task.id})


# ==========================================================================
# ğŸ› ï¸ ê°œë°œìš© PDF ë³´ê³ ì„œ í”„ë¦¬ë·° (CSS í…ŒìŠ¤íŠ¸ìš©)
# ==========================================================================
@main_bp.route('/dev/pdf-preview')
@main_bp.route('/dev/pdf-preview/<int:page_num>')
def dev_pdf_preview(page_num=1):
    """ê°œë°œìš© PDF ë³´ê³ ì„œ í”„ë¦¬ë·° - ë¸Œë¼ìš°ì €ì—ì„œ CSS í™•ì¸

    í˜ì´ì§€ ë²ˆí˜¸:
    0: ì•í‘œì§€
    1-13: ë³¸ë¬¸
    14: ìš”ì•½
    15: ë’·í‘œì§€
    16: ëª©ì°¨
    """

    from ecoweb.app.services.report import PlaywrightPDFGenerator

    # íŠ¹ìˆ˜ í˜ì´ì§€ ë§¤í•‘
    special_pages = {
        0: 'front-cover',
        14: 'final-summary',
        15: 'back-cover',
        16: 'index'
    }

    # íŠ¹ì • í˜ì´ì§€ ë Œë”ë§
    if page_num in special_pages or (1 <= page_num <= 13):
        try:
            pdf_generator = PlaywrightPDFGenerator()
            svg_contents = pdf_generator._load_svg_files()

            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
            test_data = {
                'website_url': 'preview.example.com',
                'url': 'https://preview.example.com',
                'session_data': {},
                'svg': svg_contents
            }

            # íŠ¹ìˆ˜ í˜ì´ì§€
            if page_num in special_pages:
                page_type = special_pages[page_num]
                page_html = pdf_generator._load_special_page_template(page_type, test_data)
            # ì¼ë°˜ í˜ì´ì§€ (1-13)
            else:
                page_html = pdf_generator._load_page_template(page_num, test_data)

            # HTMLì„ ì§ì ‘ ë°˜í™˜
            response = make_response(page_html)
            response.headers['Content-Type'] = 'text/html; charset=utf-8'

        except Exception as e:
            current_app.logger.error(f"í…œí”Œë¦¿ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            response = make_response(render_template('pages/error/error.html', error_message='í…œí”Œë¦¿ ë¡œë”© ì‹¤íŒ¨'))
    else:
        # ì „ì²´ í˜ì´ì§€ ë¯¸ë¦¬ë³´ê¸°
        html_pages = []
        pdf_generator = PlaywrightPDFGenerator()
        svg_contents = pdf_generator._load_svg_files()

        for i in range(1, 14):
            try:
                test_data = {
                    'website_url': 'preview.example.com',
                    'url': 'https://preview.example.com',
                    'session_data': {},
                    'svg': svg_contents
                }
                page_html = pdf_generator._load_page_template(i, test_data)

                # í˜ì´ì§€ êµ¬ë¶„ì„ ìœ„í•œ ìŠ¤íƒ€ì¼ ì¶”ê°€
                page_html = f'<div style="page-break-after: always; border: 2px solid #ccc; margin: 20px; padding: 20px;"><h3>Page {i}</h3>{page_html}</div>'
                html_pages.append(page_html)
            except Exception as e:
                html_pages.append(f'<div style="color: red;">í˜ì´ì§€ {i} ë Œë”ë§ ì˜¤ë¥˜: {e}</div>')

        combined_html = f'''
        <!DOCTYPE html>
        <html>
        <head>
            <meta charset="utf-8">
            <title>PDF ë³´ê³ ì„œ ì „ì²´ ë¯¸ë¦¬ë³´ê¸°</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                .navigation {{ position: fixed; top: 10px; right: 10px; background: white; padding: 10px; border: 1px solid #ccc; max-height: 80vh; overflow-y: auto; }}
                .navigation a {{ display: block; margin: 5px 0; text-decoration: none; color: blue; }}
                .navigation strong {{ display: block; margin-bottom: 10px; }}
            </style>
        </head>
        <body>
            <div class="navigation">
                <strong>í˜ì´ì§€ë³„ ë³´ê¸°:</strong>
                <a href="/dev/pdf-preview/0">ì•í‘œì§€</a>
                <a href="/dev/pdf-preview/16">ëª©ì°¨</a>
                {''.join([f'<a href="/dev/pdf-preview/{i}">í˜ì´ì§€ {i}</a>' for i in range(1, 14)])}
                <a href="/dev/pdf-preview/14">ìš”ì•½</a>
                <a href="/dev/pdf-preview/15">ë’·í‘œì§€</a>
                <hr>
                <a href="/dev/pdf-preview">ì „ì²´ ë³´ê¸°</a>
            </div>
            <h1>PDF ë³´ê³ ì„œ ì „ì²´ ë¯¸ë¦¬ë³´ê¸°</h1>
            {''.join(html_pages)}
        </body>
        </html>
        '''
        response = make_response(combined_html)
        response.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'
        response.headers['Pragma'] = 'no-cache'
        response.headers['Expires'] = '0'
    
    # ê³µí†µ í—¤ë” ì„¤ì • (if ë¸”ë¡ ë°–ì—ì„œ)
    if 'response' not in locals():
        response = make_response(render_template('pages/error/error.html', error_message='ì²˜ë¦¬í•  ìˆ˜ ì—†ëŠ” ìš”ì²­'))
        response.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'
        response.headers['Pragma'] = 'no-cache'
        response.headers['Expires'] = '0'

    return response


# í™ˆí˜ì´ì§€ ë¼ìš°íŠ¸ ===========================================================================================
# @main_bp.route('/homepage' , methods=['GET', 'POST'])
# def homepage():
#     return render_template('homepage.html')