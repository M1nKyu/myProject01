from ecoweb.app.extensions import celery
from flask import current_app, session
from datetime import datetime
import threading
import os
import re
import json
import time
import asyncio
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import urllib3
from pathlib import Path
from . import db
from .services.lighthouse import run_lighthouse, process_report
from .services.subpage_crawling import subpage_crawling
from .services.analysis.analysis_service import perform_detailed_analysis
from .services.resource_size_scanner import total_bytes_for_pages
from .services.analysis.emissions import estimate_emission_per_page, estimate_emission_from_kb
from ecoweb.app.Image_Classification import png2webp
from .services.capture.website import WebsiteCapture
# from .services.capture.async_website import async_website_capture
from .utils.task_cancellation import check_task_cancelled_legacy
from .utils.emission_calculator import EmissionCalculator
from .utils.grade import grade_point, grade_point_by_emission

# SSL ê²½ê³  ë©”ì‹œì§€ ë¹„í™œì„±í™” (ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹œ verify=False ì‚¬ìš©ìœ¼ë¡œ ì¸í•œ ê²½ê³  ì–µì œ)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ==========================================================================
# ğŸ“Š ë°ì´í„° ê°•í™” í—¬í¼ í•¨ìˆ˜ (Phase 1: Session-to-DB Refactoring)
# ==========================================================================

def _predict_percentile(emission: float) -> int:
    """
    ë°±ë¶„ìœ„ ì˜ˆì¸¡ í•¨ìˆ˜ - EmissionCalculator.predict_percentile()ì˜ ë˜í¼

    ì„ í˜• ë³´ê°„ ë°©ì‹ìœ¼ë¡œ ë°°ì¶œëŸ‰ì— ë”°ë¥¸ ë°±ë¶„ìœ„ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.

    Args:
        emission: íƒ„ì†Œ ë°°ì¶œëŸ‰ (gCO2e)

    Returns:
        int: ë°±ë¶„ìœ„ (1~99)
    """
    return EmissionCalculator.predict_percentile(emission)


def _enrich_view_data(view_data: dict, url: str, mongo_db, resource_doc=None, traffic_doc=None) -> dict:
    """
    ëª¨ë“  íŒŒìƒ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ê³„ì‚°í•˜ì—¬ view_dataë¥¼ ê°•í™”í•©ë‹ˆë‹¤.

    ì´ í•¨ìˆ˜ëŠ” tasks.pyì˜ analyze_url_task()ì—ì„œ í˜¸ì¶œë˜ì–´,
    Lighthouse ë¶„ì„ì´ ì™„ë£Œëœ í›„ ëª¨ë“  ê³„ì‚°ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ MongoDBì— ì €ì¥í•©ë‹ˆë‹¤.

    ê³„ì‚°ë˜ëŠ” ë°ì´í„°:
    - íƒ„ì†Œ ë°°ì¶œëŸ‰ (carbon_emission)
    - ë°°ì¶œëŸ‰ ë“±ê¸‰ (emission_grade, emission_grade_num)
    - ë°±ë¶„ìœ„ (emission_percentile)
    - í‰ê·  ëŒ€ë¹„ ì°¨ì´ (korea_diff, global_diff ë“±)
    - ì½˜í…ì¸  ìœ í˜•ë³„ ë°°ì¶œëŸ‰ (content_emission_data)
    - ë°°ì¶œëŸ‰ ë¶„í•´ ë°ì´í„° (emissions_breakdown)

    Args:
        view_data: Lighthouse ë¶„ì„ ê²°ê³¼
        url: ë¶„ì„ ëŒ€ìƒ URL
        mongo_db: MongoDB ë°ì´í„°ë² ì´ìŠ¤ í•¸ë“¤
        resource_doc: lighthouse_resources_02 ë¬¸ì„œ (ì„ íƒì , ì—†ìœ¼ë©´ ì¡°íšŒ)
        traffic_doc: lighthouse_traffic_02 ë¬¸ì„œ (ì„ íƒì , ì—†ìœ¼ë©´ ì¡°íšŒ)

    Returns:
        enriched_result: calculated ì„¹ì…˜ì´ ì¶”ê°€ëœ view_data
    """
    # view_data ë³µì‚¬ë³¸ ìƒì„± (ì›ë³¸ ë³´ì¡´)
    enriched = dict(view_data)

    # [1] ê¸°ë³¸ ë°ì´í„° ì¶”ì¶œ
    total_byte_weight = view_data.get('total_byte_weight', 0)
    kb_weight = total_byte_weight / 1024

    # [2] íƒ„ì†Œ ë°°ì¶œëŸ‰ ê³„ì‚° (KB ê¸°ì¤€)
    carbon_emission = round(estimate_emission_from_kb(kb_weight), 2)

    # [3] ë“±ê¸‰ ê³„ì‚° (wholegraindigital.com ê¸°ì¤€)
    emission_grade = EmissionCalculator.get_emission_grade(carbon_emission)
    emission_grade_num = EmissionCalculator.get_emission_grade_number(carbon_emission)

    # [4] ë°±ë¶„ìœ„ ê³„ì‚°
    emission_percentile = _predict_percentile(carbon_emission)

    # [5] í‰ê·  ë°°ì¶œëŸ‰ ê³„ì‚°
    korea_avg_carbon = round(estimate_emission_per_page(0.00456), 2)  # í•œêµ­ í‰ê· : 4.56MB
    global_avg_carbon = round(estimate_emission_per_page(0.002344), 2)  # ì„¸ê³„ í‰ê· : 2.34MB

    # [6] í‰ê·  ëŒ€ë¹„ ì°¨ì´ ê³„ì‚°
    korea_diff = round(korea_avg_carbon - carbon_emission, 2)
    global_diff = round(global_avg_carbon - carbon_emission, 2)
    korea_diff_abs = round(abs(korea_diff), 2)
    global_diff_abs = round(abs(global_diff), 2)

    # [7] í‰ê·  ëŒ€ë¹„ ë°±ë¶„ìœ¨ ì°¨ì´
    if korea_avg_carbon > 0:
        korea_carbon_percentage_diff = round(abs((carbon_emission - korea_avg_carbon) / korea_avg_carbon) * 100)
    else:
        korea_carbon_percentage_diff = 0

    # [8] ë¹„êµ ìƒíƒœ (i18n ì§€ì›)
    korea_comparison_status = "ë‚®ìŠµë‹ˆë‹¤" if korea_diff > 0 else "ë†’ìŠµë‹ˆë‹¤"  # DEPRECATED: í•œê¸€ í•˜ë“œì½”ë”©
    korea_emission_status = "below_avg" if korea_diff > 0 else "above_avg"  # ê¶Œì¥: i18n í‚¤ë¡œ ì‚¬ìš©

    # [9] ì½˜í…ì¸  ìœ í˜•ë³„ ë°°ì¶œëŸ‰ ë°ì´í„° ì²˜ë¦¬
    content_emission_data = []
    try:
        # ì „ë‹¬ë°›ì€ traffic_doc ì‚¬ìš©, ì—†ìœ¼ë©´ ì¡°íšŒ
        if not traffic_doc:
            traffic_doc = mongo_db.lighthouse_traffic_02.find_one({'url': url})
        
        if traffic_doc:
            resource_summary = traffic_doc.get('resourceSummary')
            if resource_summary:
                from .services.analysis.analysis_service import process_content_emission_data
                content_emission_data = process_content_emission_data(resource_summary)
    except Exception as e:
        current_app.logger.warning(f"ì½˜í…ì¸  ë°°ì¶œëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        content_emission_data = []

    # [10] ë°°ì¶œëŸ‰ ë¶„í•´ ë°ì´í„° (ì„œë²„/ë„¤íŠ¸ì›Œí¬/ë””ë°”ì´ìŠ¤ë³„)
    emissions_breakdown = {}
    try:
        from .services.analysis.emissions import emissions_breakdown_from_bytes
        emissions_breakdown = emissions_breakdown_from_bytes(total_byte_weight, region='korea')
    except Exception as e:
        current_app.logger.warning(f"ë°°ì¶œëŸ‰ ë¶„í•´ ë°ì´í„° ê³„ì‚° ì‹¤íŒ¨: {e}")

    # [10-1] ì½˜í…ì¸  íƒ€ì…ë³„ ì¹´ìš´íŠ¸ ë°ì´í„° ì²˜ë¦¬ (íŒŒì´ì°¨íŠ¸ìš©)
    content_count_data = []
    try:
        from urllib.parse import urlparse
        from collections import Counter
        import os
        
        # ì „ë‹¬ë°›ì€ resource_doc ì‚¬ìš©, ì—†ìœ¼ë©´ ì¡°íšŒ
        doc = resource_doc
        if not doc:
            collection_resource = mongo_db.lighthouse_resources_02
            query_candidates = []
            if url:
                query_candidates.append({'url': url})
                stripped = url.replace('https://', '').replace('http://', '')
                query_candidates.append({'url': stripped})
            
            for q in query_candidates:
                try:
                    doc = collection_resource.find_one(
                        q,
                        {'_id': 0, 'networkRequests': 1, 'network_requests': 1, 'timestamp': 1},
                        sort=[('timestamp', -1)]  # ìµœì‹  timestamp ìš°ì„ 
                    )
                    if doc:
                        break
                except Exception:
                    continue
        
        if doc:
            requests_list = doc.get('networkRequests') or doc.get('network_requests') or []
            ext_counter = Counter()
            
            # í™•ì¥ì ë§¤í•‘ (resourceType ê¸°ë°˜)
            resource_type_to_ext = {
                'document': 'html',
                'script': 'js',
                'stylesheet': 'css',
                'image': None,  # URLì—ì„œ ì¶”ì¶œ ì‹œë„
                'font': None,   # URLì—ì„œ ì¶”ì¶œ ì‹œë„
                'media': None,  # URLì—ì„œ ì¶”ì¶œ ì‹œë„
            }
            
            # ì¼ë°˜ì ì¸ íŒŒì¼ í™•ì¥ì ëª©ë¡
            valid_extensions = {
                'png', 'jpg', 'jpeg', 'gif', 'webp', 'svg', 'ico',
                'js', 'css', 'html', 'htm',
                'woff', 'woff2', 'ttf', 'otf', 'eot',
                'pdf', 'json', 'xml', 'txt',
                'mp4', 'mp3', 'webm', 'avi', 'mov',
                'zip', 'rar', 'gz'
            }
            
            for req in requests_list:
                url_val = req.get('url', '')
                if not url_val:
                    continue
                
                try:
                    parsed = urlparse(url_val)
                    path = parsed.path
                    
                    # í™•ì¥ì ì¶”ì¶œ
                    ext = None
                    if '.' in path:
                        _, ext_with_dot = os.path.splitext(path)
                        if ext_with_dot:
                            ext = ext_with_dot.lower().lstrip('.')
                    
                    # í™•ì¥ìê°€ ì—†ìœ¼ë©´ resourceType ê¸°ë°˜ ë§¤í•‘ ì‹œë„
                    if not ext:
                        resource_type = (req.get('resourceType') or req.get('resource_type') or '').lower()
                        ext = resource_type_to_ext.get(resource_type)
                    
                    # ìœ íš¨í•œ í™•ì¥ìë§Œ ì¹´ìš´íŠ¸
                    if ext and ext in valid_extensions:
                        ext_counter[ext] += 1
                except Exception:
                    continue
            
            # í™•ì¥ì ë¼ë²¨ ë§¤í•‘ (í‘œì‹œìš©)
            ext_label_map = {
                'png': 'PNG', 'jpg': 'JPG', 'jpeg': 'JPG', 'gif': 'GIF', 'webp': 'WEBP',
                'svg': 'SVG', 'ico': 'ICO',
                'js': 'JS', 'css': 'CSS', 'html': 'HTML', 'htm': 'HTML',
                'woff': 'WOFF', 'woff2': 'WOFF2', 'ttf': 'TTF', 'otf': 'OTF', 'eot': 'EOT',
                'pdf': 'PDF', 'json': 'JSON', 'xml': 'XML', 'txt': 'TXT',
                'mp4': 'MP4', 'mp3': 'MP3', 'webm': 'WEBM', 'avi': 'AVI', 'mov': 'MOV',
                'zip': 'ZIP', 'rar': 'RAR', 'gz': 'GZ'
            }
            
            # ì¹´ìš´íŠ¸ê°€ ë§ì€ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ í•­ëª©ë§Œ ì„ íƒ
            sorted_exts = sorted(ext_counter.items(), key=lambda x: x[1], reverse=True)
            content_count_data = [
                {'ext': ext, 'label': ext_label_map.get(ext, ext.upper()), 'count': count}
                for ext, count in sorted_exts[:11]  # ìµœëŒ€ 11ê°œ
            ]
    except Exception as e:
        current_app.logger.warning(f"ì½˜í…ì¸  ì¹´ìš´íŠ¸ ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        content_count_data = []

    # [11] calculated ì„¹ì…˜ êµ¬ì„±
    # ëª¨ë“  ê³„ì‚°ëœ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ ì„¹ì…˜ì— ì •ë¦¬í•˜ì—¬ ì €ì¥
    enriched['calculated'] = {
        # === ê¸°ë³¸ ì¸¡ì •ê°’ ===
        'carbon_emission': carbon_emission,
        'kb_weight': kb_weight,
        'total_byte_weight': total_byte_weight,

        # === ë“±ê¸‰ ===
        'emission_grade': emission_grade,
        'emission_grade_num': emission_grade_num,

        # === ë°±ë¶„ìœ„ ===
        'emission_percentile': emission_percentile,

        # === í‰ê·  ê¸°ì¤€ ===
        'korea_avg_carbon': korea_avg_carbon,
        'global_avg_carbon': global_avg_carbon,

        # === í‰ê·  ëŒ€ë¹„ ì°¨ì´ ===
        'korea_diff': korea_diff,
        'global_diff': global_diff,
        'korea_diff_abs': korea_diff_abs,
        'global_diff_abs': global_diff_abs,

        # === í‰ê·  ëŒ€ë¹„ ë¹„ìœ¨ ===
        'korea_carbon_percentage_diff': korea_carbon_percentage_diff,
        'korea_comparison_status': korea_comparison_status,  # DEPRECATED: í•˜ìœ„ í˜¸í™˜ì„±ìš©
        'korea_emission_status': korea_emission_status,  # ê¶Œì¥: i18n ì§€ì›

        # === ì½˜í…ì¸  ìœ í˜•ë³„ ë°°ì¶œëŸ‰ ===
        'content_emission_data': content_emission_data,

        # === ì½˜í…ì¸  íƒ€ì…ë³„ ì¹´ìš´íŠ¸ ë°ì´í„° (íŒŒì´ì°¨íŠ¸ìš©) ===
        'content_count_data': content_count_data,

        # === ë°°ì¶œëŸ‰ ë¶„í•´ ë°ì´í„° ===
        'emissions_breakdown': emissions_breakdown,

        # === í•˜ìœ„ í˜¸í™˜ì„± (DEPRECATED) ===
        'korea_carbon_emission_grade': emission_grade,  # [Deprecated] use emission_grade
        'world_carbon_emission_grade': emission_grade,  # [Deprecated] use emission_grade
    }

    return enriched

@celery.task(bind=True, ignore_result=True)
def analyze_url_task(self, url, user_id, is_mobile, original_task_id, perform_subpage_crawling=False, existing_view_data=None):
    """
    Celery task to run lighthouse analysis in the background.
    """
    def check_task_cancelled():
        """ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê³ , ì·¨ì†Œëœ ê²½ìš° ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤."""
        check_task_cancelled_legacy(original_task_id, current_app.logger)

    try:
        # [2] MongoDB ì»¬ë ‰ì…˜ í•¸ë“¤ ì¤€ë¹„ ë° ì—°ê²° ìƒíƒœ í™•ì¸
        try:
            mongo_db = db.get_db()
            # MongoDB ì—°ê²° í…ŒìŠ¤íŠ¸
            mongo_db.list_collection_names()
        except Exception as mongo_error:
            current_app.logger.error(f"ë¶„ì„ ì‹¤íŒ¨: MongoDB ì—°ê²° ì‹¤íŒ¨ - {str(mongo_error)}")
            raise Exception(f"MongoDB ì—°ê²° ì‹¤íŒ¨: {str(mongo_error)}")
        
        collection_traffic = mongo_db.lighthouse_traffic_02
        collection_resource = mongo_db.lighthouse_resources_02
        collection_measured_urls = mongo_db.measured_urls
        collection_subpage = mongo_db.lighthouse_subpage
        task_results_collection = mongo_db.task_results
        
        # ì»¬ë ‰ì…˜ ì ‘ê·¼ í…ŒìŠ¤íŠ¸
        try:
            collection_traffic.find_one({}, limit=1)
            collection_resource.find_one({}, limit=1)
        except Exception as coll_error:
            current_app.logger.error(f"ë¶„ì„ ì‹¤íŒ¨: MongoDB ì»¬ë ‰ì…˜ ì ‘ê·¼ ì‹¤íŒ¨ - {str(coll_error)}")
            raise Exception(f"MongoDB ì»¬ë ‰ì…˜ ì ‘ê·¼ ì‹¤íŒ¨: {str(coll_error)}")

        # ì´ˆê¸° ì·¨ì†Œ í™•ì¸
        check_task_cancelled()

        # [2-1] MongoDB ë°ì´í„° ì¼ê´„ ì¡°íšŒ (ìµœì í™”: ì¤‘ë³µ ì¡°íšŒ ë°©ì§€)
        # ê°™ì€ URLì— ëŒ€í•œ ì¡°íšŒë¥¼ í•œ ë²ˆì— ìˆ˜í–‰í•˜ì—¬ ì¬ì‚¬ìš©
        resource_doc = None
        traffic_doc = None
        lighthouse_timestamp = None
        
        try:
            # URL ì •ê·œí™” (query_candidates íŒ¨í„´)
            query_candidates = []
            if url:
                query_candidates.append({'url': url})
                stripped = url.replace('https://', '').replace('http://', '')
                query_candidates.append({'url': stripped})
            
            # lighthouse_resources_02 ì¡°íšŒ (network_requests, timestamp)
            for q in query_candidates:
                try:
                    resource_doc = collection_resource.find_one(
                        q,
                        {
                            '_id': 0,
                            'networkRequests': 1,
                            'network_requests': 1,
                            'timestamp': 1
                        },
                        sort=[('timestamp', -1)]  # ìµœì‹  timestamp ìš°ì„ 
                    )
                    if resource_doc:
                        # Lighthouse timestamp ì¶”ì¶œ
                        ts = resource_doc.get('timestamp')
                        if ts:
                            if isinstance(ts, datetime):
                                lighthouse_timestamp = ts.isoformat()
                            elif isinstance(ts, str):
                                lighthouse_timestamp = ts
                        break
                except Exception:
                    continue
            
            # lighthouse_traffic_02 ì¡°íšŒ (resourceSummary, audits)
            for q in query_candidates:
                try:
                    traffic_doc = collection_traffic.find_one(
                        q,
                        {
                            '_id': 0,
                            'resourceSummary': 1,
                            'audits': 1
                        },
                        sort=[('timestamp', -1)]  # ìµœì‹  timestamp ìš°ì„ 
                    )
                    if traffic_doc:
                        break
                except Exception:
                    continue
        except Exception as e:
            current_app.logger.warning(f"MongoDB ì¼ê´„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ (ê³„ì† ì§„í–‰): {e}")
            # ì¡°íšŒ ì‹¤íŒ¨í•´ë„ ì‘ì—…ì€ ê³„ì† ì§„í–‰ (ë‚˜ì¤‘ì— ê°œë³„ ì¡°íšŒ ì‹œë„)

        # [3] Lighthouse ë¶„ì„ ìˆ˜í–‰ ì—¬ë¶€ ê²°ì •
        if existing_view_data:
            # ê¸°ì¡´ Lighthouse ë°ì´í„° ì‚¬ìš©
            print(f'[ê¸°ì¡´ ë°ì´í„° í™œìš©] Lighthouse ì¸¡ì • ìƒëµ, ê¸°ì¡´ ë°ì´í„° ì‚¬ìš©')
            view_data = existing_view_data.copy()
            view_data['url'] = url

            # ì§„í–‰ìƒí™© ì´ˆê¸°í™”: ì…ë ¥ í˜ì´ì§€ ì™„ë£Œ, í•˜ìœ„ í˜ì´ì§€ ëŒ€ê¸°ì¤‘
            try:
                task_results_collection.update_one(
                    {'_id': original_task_id},
                    {'$set': {
                        'progress': {
                            'current_step': 'subpages',
                            'steps': {
                                'input': {'status': 'done', 'message': 'ê¸°ì¡´ Lighthouse ë°ì´í„° ì‚¬ìš©'},
                                'subpages': {'status': 'waiting', 'message': 'í•˜ìœ„ í˜ì´ì§€ ë¶„ì„ ëŒ€ê¸°'}
                            },
                            'updated_at': datetime.utcnow().isoformat()
                        }
                    }}
                )
            except Exception:
                pass
        else:
            # ìƒˆë¡œìš´ Lighthouse ë¶„ì„ ìˆ˜í–‰
            self.update_state(state='PROGRESS', meta={'status': 'Lighthouse ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...'})
            # ì§„í–‰ìƒí™© ì´ˆê¸°í™”: ì…ë ¥ í˜ì´ì§€ ë¶„ì„ ì§„í–‰ì¤‘, í•˜ìœ„ í˜ì´ì§€ ëŒ€ê¸°ì¤‘
            try:
                task_results_collection.update_one(
                    {'_id': original_task_id},
                    {'$set': {
                        'progress': {
                            'current_step': 'input',
                            'steps': {
                                'input': {'status': 'in_progress', 'message': 'ì…ë ¥ í˜ì´ì§€ ë¶„ì„ ì‹œì‘'},
                                'subpages': {'status': 'waiting', 'message': 'í•˜ìœ„ í˜ì´ì§€ ë¶„ì„ ëŒ€ê¸°'}
                            },
                            'updated_at': datetime.utcnow().isoformat()
                        }
                    }}
                )
            except Exception:
                pass
            timeout = 240 if is_mobile else 120 # íƒ€ì„ì•„ì›ƒ ì‹œê°„ì„ 2ë°°ë¡œ ëŠ˜ë¦¼
            # Lighthouse ëŒ€ê¸° ì¤‘ í•˜íŠ¸ë¹„íŠ¸(ì§„í–‰ ì¤‘ ì‹ í˜¸) ì“°ë ˆë“œ ì‹œì‘
            _hb_stop = threading.Event()
            def _heartbeat():
                start_ts = time.time()
                while not _hb_stop.is_set():
                    try:
                        # í•˜íŠ¸ë¹„íŠ¸ ì¤‘ì—ë„ ì·¨ì†Œ í™•ì¸
                        task_doc = task_results_collection.find_one({'_id': original_task_id})
                        if task_doc and task_doc.get('status') == 'CANCELLED':
                            current_app.logger.info(f'Task {original_task_id} cancelled during heartbeat, stopping')
                            _hb_stop.set()
                            break

                        elapsed = int(time.time() - start_ts)
                        task_results_collection.update_one(
                            {'_id': original_task_id},
                            {'$set': {
                                'progress.steps.input': {'status': 'in_progress', 'message': f'Lighthouse ì‹¤í–‰ ì¤‘... ({elapsed}s)'},
                                'progress.current_step': 'input',
                                'progress.updated_at': datetime.utcnow().isoformat()
                            }}
                        )
                    except Exception:
                        pass
                    # ë¡œê·¸ë„ ì£¼ê¸°ì ìœ¼ë¡œ ë‚¨ê¹€ (stdout)
                    # (ê°„ì†Œí™”) í‘œì¤€ ì¶œë ¥ ì œê±°
                    try:
                        _ = elapsed  # no-op
                    except Exception:
                        pass
                    _hb_stop.wait(5.0)
            _hb_thread = threading.Thread(target=_heartbeat, daemon=True)
            _hb_thread.start()
            try:
                # Lighthouse ì‹¤í–‰ ì „ ì·¨ì†Œ í™•ì¸
                check_task_cancelled()
                # Lighthouse ì¬ì‹œë„ ë¡œì§ (í¬ë˜ì‹œ ë°©ì§€)
                max_retries = 2
                exit_code = None
                last_error = None
                
                for attempt in range(max_retries):
                    try:
                        exit_code = run_lighthouse(url, timeout)
                        
                        # exit_codeê°€ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° (ë¹„ë™ê¸° ëª¨ë“œì—ì„œ ì˜¤ë¥˜ ë°œìƒ ì‹œ)
                        if isinstance(exit_code, dict):
                            if exit_code.get("success", False):
                                exit_code = 0
                                break
                            else:
                                last_error = exit_code.get("error", "Unknown error")
                                if "crashed" not in last_error.lower() and "timeout" not in last_error.lower():
                                    # í¬ë˜ì‹œë‚˜ íƒ€ì„ì•„ì›ƒì´ ì•„ë‹Œ ê²½ìš° ì¬ì‹œë„ ë¶ˆí•„ìš”
                                    raise Exception(f"Lighthouse ë¶„ì„ ì‹¤íŒ¨: {last_error}")
                                if attempt < max_retries - 1:
                                    time.sleep(2)  # ì¬ì‹œë„ ì „ ëŒ€ê¸°
                                    continue
                                else:
                                    raise Exception(f"Lighthouse ë¶„ì„ ì‹¤íŒ¨: {last_error}")
                        elif exit_code == 0:
                            break
                        else:
                            if attempt < max_retries - 1:
                                time.sleep(2)
                                continue
                    except Exception as e:
                        last_error = str(e)
                        if attempt < max_retries - 1:
                            time.sleep(2)
                            continue
                        else:
                            raise
                
                if exit_code is None:
                    raise Exception(f"Lighthouse ë¶„ì„ ì‹¤íŒ¨: {last_error or 'Unknown error'}")
            finally:
                _hb_stop.set()
                try:
                    _hb_thread.join(timeout=2)
                except Exception:
                    pass
            
            # exit_code ìµœì¢… ê²€ì¦ (ì¬ì‹œë„ ë¡œì§ì—ì„œ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆì§€ë§Œ ì•ˆì „ì¥ì¹˜)
            if isinstance(exit_code, dict):
                if not exit_code.get("success", False):
                    error_msg = exit_code.get("error", "Unknown error")
                    raise Exception(f"Lighthouse ë¶„ì„ ì‹¤íŒ¨: {error_msg}")
                else:
                    exit_code = 0  # ì„±ê³µí•œ ê²½ìš° 0ìœ¼ë¡œ ì„¤ì •
            
            if exit_code != 0:
                raise Exception(f"Lighthouse ë¶„ì„ ì‹¤íŒ¨: ì¢…ë£Œ ì½”ë“œ {exit_code}")

            # Lighthouse ì™„ë£Œ í›„ ì·¨ì†Œ í™•ì¸
            check_task_cancelled()
            
            # report.json íŒŒì¼ ìƒì„± ì—¬ë¶€ í™•ì¸
            # osëŠ” ì´ë¯¸ íŒŒì¼ ìƒë‹¨ì—ì„œ importë¨
            report_paths = [
                os.path.join(os.getcwd(), 'report.json'),
                os.path.join('/app', 'report.json'),
                os.path.join('/app/ecoweb', 'report.json'),
                os.path.join('/app/ecoweb/app', 'report.json'),
            ]
            
            report_found = False
            for report_path in report_paths:
                if os.path.exists(report_path):
                    report_found = True
                    break
                
                if not report_found:
                    current_app.logger.error(f"ë¶„ì„ ì‹¤íŒ¨: Lighthouse report.json íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                    raise FileNotFoundError("Lighthouse ì‹¤í–‰ í›„ report.json íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            # [4] Lighthouse ë³´ê³ ì„œ ì²˜ë¦¬ ë‹¨ê³„ë¡œ ìƒíƒœ ì—…ë°ì´íŠ¸
            self.update_state(state='PROGRESS', meta={'status': 'Lighthouse ë³´ê³ ì„œë¥¼ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤...'})
            try:
                task_results_collection.update_one(
                    {'_id': original_task_id},
                    {'$set': {
                        'progress.steps.input': {'status': 'in_progress', 'message': 'Lighthouse ë³´ê³ ì„œ ì²˜ë¦¬ ì¤‘'},
                        'progress.current_step': 'input',
                        'progress.updated_at': datetime.utcnow().isoformat()
                    }}
                )
            except Exception:
                pass

            # [5] ë³´ê³ ì„œ ì²˜ë¦¬ ë° view_data ìƒì„±
            view_data = process_report(
                url,
                collection_resource,
                collection_traffic,
                collection_measured_urls,
                measured_type="manual",
                measured_cycle="None",
                measured_source="user",
                user_id=user_id,
                is_mobile=is_mobile
            )
            # (ê°„ì†Œí™”) í‘œì¤€ ì¶œë ¥ ì œê±°

            # [6] ê¸°ë³¸ í•„ë“œ ë³´ì •: URLì„ view_dataì— ë¨¼ì € í• ë‹¹
            view_data['url'] = url

            # [7] ë¶„ì„ ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬
            if not view_data or view_data.get('total_byte_weight', 0) == 0:
                raise ValueError("Lighthouse ë¶„ì„ ê²°ê³¼ ë°ì´í„°ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # [8] ì˜µì…˜: í•˜ìœ„ í˜ì´ì§€ í¬ë¡¤ë§ ë° ë¶„ì„ ìˆ˜í–‰
        # í•˜ìœ„ í˜ì´ì§€ í¬ë¡¤ë§ ì „ ì·¨ì†Œ í™•ì¸
        check_task_cancelled()
        self.update_state(state='PROGRESS', meta={'status': 'í•˜ìœ„ í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤...'})
        try:
            # ì…ë ¥ í˜ì´ì§€ ë‹¨ê³„ ì™„ë£Œ, í•˜ìœ„ í˜ì´ì§€ ë‹¨ê³„ ì‹œì‘
            task_results_collection.update_one(
                {'_id': original_task_id},
                {'$set': {
                    'progress.steps.input': {'status': 'done', 'message': 'ì…ë ¥ í˜ì´ì§€ ë¶„ì„ ì™„ë£Œ'},
                    'progress.steps.subpages': {'status': 'in_progress', 'message': 'í•˜ìœ„ í˜ì´ì§€ ë¶„ì„ ì¤‘'},
                    'progress.current_step': 'subpages',
                    'progress.updated_at': datetime.utcnow().isoformat()
                }}
            )
        except Exception:
            pass

        # í•˜ìœ„ í˜ì´ì§€ í¬ë¡¤ë§ ì‹œê°„ ì œí•œ (60ì´ˆ)
        import signal
        def timeout_handler(signum, frame):
            raise TimeoutError("í•˜ìœ„ í˜ì´ì§€ í¬ë¡¤ë§ ì‹œê°„ ì´ˆê³¼")

        subpages = []
        try:
            # Windowsì—ì„œëŠ” signal.alarmì´ ì§€ì›ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë‹¤ë¥¸ ë°©ë²• ì‚¬ìš©
            if os.name != 'nt':  # Unix/Linux
                signal.signal(signal.SIGALRM, timeout_handler)
                signal.alarm(90)  # 90ì´ˆ ì œí•œ

            print(f'[í•˜ìœ„í˜ì´ì§€ í¬ë¡¤ë§] ì‹œì‘: {url} (ìµœëŒ€ 10í˜ì´ì§€)')
            subpages = subpage_crawling(url, collection_subpage, max_pages=10)
            print(f'[í•˜ìœ„í˜ì´ì§€ í¬ë¡¤ë§] ì™„ë£Œ: {len(subpages) if subpages else 0}ê°œ í˜ì´ì§€ ë°œê²¬')

            if os.name != 'nt':
                signal.alarm(0)  # íƒ€ì´ë¨¸ í•´ì œ
        except TimeoutError:
            current_app.logger.warning(f"í•˜ìœ„ í˜ì´ì§€ í¬ë¡¤ë§ ì‹œê°„ ì´ˆê³¼: {url}")
            subpages = []
        except Exception as e:
            current_app.logger.warning(f"í•˜ìœ„ í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            subpages = []
        finally:
            if os.name != 'nt':
                signal.alarm(0)  # íƒ€ì´ë¨¸ í•´ì œ

        # í•˜ìœ„ í˜ì´ì§€ ë°ì´í„° ì²˜ë¦¬ ë° ìƒíƒœ ì—…ë°ì´íŠ¸ (ì„±ê³µ/ì‹¤íŒ¨ ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´)
        if subpages:
            # ë¦¬ì†ŒìŠ¤ ì‚¬ì´ì¦ˆ ê³„ì‚° ë° ë³‘í•©
            try:
                per_page_sizes = total_bytes_for_pages([s.get('url') for s in subpages if s.get('url')])
                by_url = {p.get('url'): int(p.get('total_bytes') or 0) for p in per_page_sizes.get('per_page', [])}
                for s in subpages:
                    u = s.get('url')
                    if u in by_url:
                        s['total_bytes'] = by_url[u]
                        s['total_kb'] = round(by_url[u] / 1024.0, 2)
            except Exception as e:
                current_app.logger.warning(f"í•˜ìœ„ í˜ì´ì§€ ë¦¬ì†ŒìŠ¤ í¬ê¸° ê³„ì‚° ì‹¤íŒ¨: {e}")
            view_data['subpages'] = subpages
            current_app.logger.info(f"í•˜ìœ„ í˜ì´ì§€ ìˆ˜ì§‘ ì™„ë£Œ: URL={url}, ìˆ˜ì§‘ ìˆ˜={len(subpages)}")
        else:
            view_data['subpages'] = []
            current_app.logger.info(f"í•˜ìœ„ í˜ì´ì§€ ìˆ˜ì§‘ ì‹¤íŒ¨/ì‹œê°„ì´ˆê³¼: URL={url}, ìˆ˜ì§‘ ìˆ˜=0")

        # í•˜ìœ„ í˜ì´ì§€ ë‹¨ê³„ ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸ (í•­ìƒ ì‹¤í–‰)
        try:
            status_message = 'í•˜ìœ„ í˜ì´ì§€ ë¶„ì„ ì™„ë£Œ' if subpages else 'í•˜ìœ„ í˜ì´ì§€ ë¶„ì„ ì™„ë£Œ (ì‹œê°„ì´ˆê³¼ ë˜ëŠ” ì‹¤íŒ¨)'
            task_results_collection.update_one(
                {'_id': original_task_id},
                {'$set': {
                    'progress.steps.subpages': {'status': 'done', 'message': status_message},
                    'progress.current_step': 'subpages',
                    'progress.updated_at': datetime.utcnow().isoformat()
                }}
            )
        except Exception as e:
            current_app.logger.warning(f"í•˜ìœ„ í˜ì´ì§€ ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        # session['subpages'] = subpages  # Celery ì‘ì—… ë‚´ì—ì„œ ì§ì ‘ sessionì— ì ‘ê·¼í•˜ëŠ” ê²ƒì€ ê¶Œì¥ë˜ì§€ ì•ŠìŒ
            # perform_detailed_analysis(url)

        # [9] ì´ë¯¸ì§€ ìµœì í™” ë° ìº¡ì²˜ ì‘ì—… ì‹¤í–‰ (ê¸°ì¡´ img_optimization ë¡œì§ ì´ë™)
        # ì´ë¯¸ì§€ ìµœì í™” ì „ ì·¨ì†Œ í™•ì¸
        check_task_cancelled()
        try:
            # ì§„í–‰ ìƒíƒœ: ì´ë¯¸ì§€ ìµœì í™” ì‹œì‘
            try:
                task_results_collection.update_one(
                    {'_id': original_task_id},
                    {'$set': {
                        'progress.steps.image_opt': {'status': 'in_progress', 'message': 'ì´ë¯¸ì§€ ìµœì í™” ì¤‘'},
                        'progress.current_step': 'image_opt',
                        'progress.updated_at': datetime.utcnow().isoformat()
                    }}
                )
            except Exception:
                pass
            original_url = url  # ì„¸ì…˜ URL ëŒ€ì²´
            # íŒŒì¼ ì €ì¥ìš©: ìŠ¤í‚´ ì œê±° ë²„ì „
            url_s_stripped = (original_url or '').replace('https://', '').replace('http://', '')
            print(f"[IMAGE_OPT] URL: {original_url}, url_s_stripped: {url_s_stripped}")
            current_app.logger.info(f"[IMAGE_OPT] URL: {original_url}, url_s_stripped: {url_s_stripped}")

            # 1) ì´ë¯¸ì§€ URL ìˆ˜ì§‘: ì´ë¯¸ ì¡°íšŒëœ resource_doc ì¬ì‚¬ìš© (ìµœì í™”)
            Image_paths = []
            doc = resource_doc  # ì¼ê´„ ì¡°íšŒí•œ ë°ì´í„° ì¬ì‚¬ìš©
            lighthouse_timestamp = None
            
            try:
                # resource_docì´ ì—†ìœ¼ë©´ ì¡°íšŒ (fallback)
                if not doc:
                    collection_resource = mongo_db.lighthouse_resources_02
                    query_candidates = []
                    if original_url:
                        query_candidates.append({'url': original_url})
                        stripped = original_url.replace('https://', '').replace('http://', '')
                        query_candidates.append({'url': stripped})
                    
                    # ê°€ì¥ ìµœì‹  timestampì˜ ë°ì´í„° ì¡°íšŒ
                    for q in query_candidates:
                        try:
                            doc = collection_resource.find_one(
                                q, 
                                {'_id': 0, 'networkRequests': 1, 'network_requests': 1, 'timestamp': 1},
                                sort=[('timestamp', -1)]  # ìµœì‹  timestamp ìš°ì„ 
                            )
                            if doc:
                                break
                        except Exception as query_error:
                            current_app.logger.error(f"ë¶„ì„ ì‹¤íŒ¨: ì´ë¯¸ì§€ URL ì¡°íšŒ ì˜¤ë¥˜ - {str(query_error)}")
                
                # Lighthouse timestamp ì¶”ì¶œ (ì´ë¯¸ ì¡°íšŒëœ ê²½ìš° ë˜ëŠ” ìƒˆë¡œ ì¡°íšŒí•œ ê²½ìš°)
                if doc:
                    ts = doc.get('timestamp')
                    if ts:
                        if isinstance(ts, datetime):
                            lighthouse_timestamp = ts.isoformat()
                        elif isinstance(ts, str):
                            lighthouse_timestamp = ts
                
                image_exts = ('.jpg', '.jpeg', '.png', '.webp', '.gif')
                if doc:
                    requests_list = doc.get('networkRequests') or doc.get('network_requests') or []
                    
                    from urllib.parse import urlparse, urljoin
                    target_parsed = urlparse(original_url)
                    target_domain = target_parsed.netloc
                    
                    for item in requests_list:
                        url_val = item.get('url', '')
                        if not url_val:
                            continue
                        
                        # data: URL ìŠ¤í‚µ (base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ëŠ” ë‹¤ìš´ë¡œë“œ ë¶ˆê°€)
                        if url_val.startswith('data:'):
                            continue
                        
                        # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ URLë¡œ ë³€í™˜
                        if not url_val.startswith(('http://', 'https://')):
                            url_val = urljoin(original_url, url_val)
                        
                        rtype = (item.get('resourceType') or item.get('resource_type') or '').lower()
                        # resourceTypeì´ 'image'ì¸ ê²½ìš°ë§Œ ìˆ˜ì§‘ (í™•ì¥ì ì²´í¬ëŠ” ë³´ì¡°ì ìœ¼ë¡œë§Œ ì‚¬ìš©)
                        is_image = rtype == 'image'
                        
                        # resourceTypeì´ ì—†ê±°ë‚˜ 'image'ê°€ ì•„ë‹Œ ê²½ìš°, URL í™•ì¥ìë¡œ í™•ì¸
                        if not is_image:
                            is_image = url_val.lower().endswith(image_exts)
                        
                        if is_image:
                            Image_paths.append(url_val)
            except Exception as e:
                current_app.logger.error(f"ë¶„ì„ ì‹¤íŒ¨: ì´ë¯¸ì§€ URL ì¡°íšŒ ì˜¤ë¥˜ - {str(e)}")

            # ì²˜ë¦¬ ì´ë¯¸ì§€ ìˆ˜ ì œí•œ
            try:
                max_images = int(os.getenv('IMG_OPT_MAX', '100'))
            except Exception:
                max_images = 100
            if isinstance(Image_paths, list) and len(Image_paths) > max_images:
                Image_paths = Image_paths[:max_images]

            # ì €ì¥ ê²½ë¡œ ì¤€ë¹„ (var/optimization_images ì‚¬ìš©)
            from ecoweb.config import Config
            image_dir_path = os.path.join(Config.OPTIMIZATION_IMAGES_FOLDER, url_s_stripped)
            if not os.path.exists(image_dir_path):
                os.makedirs(image_dir_path, exist_ok=True)

            # ì´ë¯¸ì§€ ìºì‹œ ìœ í‹¸ë¦¬í‹° ì„í¬íŠ¸
            from ecoweb.app.utils.image_cache import (
                get_cached_image_info,
                is_cache_valid,
                check_image_changed,
                update_image_cache,
                calculate_file_hash,
                load_cache_metadata
            )
            
            # ìºì‹œ ì„¤ì • í™•ì¸
            cache_enabled = Config.IMG_CACHE_ENABLED
            cache_ttl_days = Config.IMG_CACHE_TTL_DAYS

            # ê³ ì„±ëŠ¥ ë‹¤ìš´ë¡œë“œ: ì„¸ì…˜ + í’€ + ë³‘ë ¬ (ë‹¤ìš´ë¡œë“œ ì „ìš©)
            downloaded = []
            from concurrent.futures import ThreadPoolExecutor, as_completed
            import ssl as _ssl

            class TLSAdapter(HTTPAdapter):
                def init_poolmanager(self, *args, **kwargs):
                    context = _ssl.SSLContext(_ssl.PROTOCOL_TLS_CLIENT)
                    try:
                        context.set_ciphers('DEFAULT@SECLEVEL=1')
                    except Exception:
                        pass
                    try:
                        context.options |= 0x4  # OP_LEGACY_SERVER_CONNECT
                    except Exception:
                        pass
                    context.check_hostname = False
                    context.verify_mode = _ssl.CERT_NONE
                    kwargs['ssl_context'] = context
                    return super().init_poolmanager(*args, **kwargs)

            retry = Retry(total=3, connect=2, read=1, backoff_factor=0.3, status_forcelist=[429, 500, 502, 503, 504], allowed_methods=["GET"]) 
            dl_session = requests.Session()
            dl_session.headers.update({'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'})
            adapter = TLSAdapter(max_retries=retry, pool_connections=16, pool_maxsize=16)
            dl_session.mount('https://', adapter)
            dl_session.mount('http://', adapter)
            # ì»¨í…Œì´ë„ˆ í™˜ê²½ ë³€ìˆ˜(í”„ë¡ì‹œ ë“±) ì‹ ë¢°
            dl_session.trust_env = True

            def download_one(imageurl: str):
                # ThreadPoolExecutor ë‚´ë¶€ì—ì„œëŠ” Flask ì• í”Œë¦¬ì¼€ì´ì…˜ ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë¯€ë¡œ ì¼ë°˜ logging ì‚¬ìš©
                import logging
                logger = logging.getLogger(__name__)
                
                # data: URL ìŠ¤í‚µ (base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ëŠ” HTTP ìš”ì²­ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ ë¶ˆê°€)
                if imageurl.startswith('data:'):
                    logger.debug(f"[TASKS] Skipping data: URL: {imageurl[:50]}...")
                    return None
                
                try:
                    # URLì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ (ë” ì •í™•í•œ ë°©ë²•)
                    from urllib.parse import urlparse, unquote
                    parsed = urlparse(imageurl)
                    path = unquote(parsed.path)  # URL ë””ì½”ë”©
                    
                    # ê²½ë¡œì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ
                    if '/' in path:
                        filename = os.path.basename(path)
                    else:
                        filename = path
                    
                    # íŒŒì¼ëª…ì´ ì—†ê±°ë‚˜ í™•ì¥ìê°€ ì—†ëŠ” ê²½ìš° URLì—ì„œ ì¶”ì¶œ ì‹œë„
                    if not filename or '.' not in filename:
                        # URLì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ ì‚¬ìš©
                        path_parts = [p for p in path.split('/') if p]
                        if path_parts:
                            filename = path_parts[-1]
                        else:
                            # URL ì „ì²´ë¥¼ í•´ì‹œí•˜ì—¬ íŒŒì¼ëª… ìƒì„±
                            import hashlib
                            url_hash = hashlib.md5(imageurl.encode()).hexdigest()[:8]
                            # Content-Typeì—ì„œ í™•ì¥ì ì¶”ì¶œ ì‹œë„
                            filename = f"image_{url_hash}.jpg"  # ê¸°ë³¸ê°’
                    
                    # íŒŒì¼ëª… ì •ë¦¬ (íŠ¹ìˆ˜ë¬¸ì ì œê±°)
                    filename = re.sub(r'[<>:"|?*]', '_', filename)
                    if not filename:
                        import hashlib
                        url_hash = hashlib.md5(imageurl.encode()).hexdigest()[:8]
                        filename = f"image_{url_hash}.jpg"
                    
                    destination = os.path.join(image_dir_path, filename)
                    
                    # ìºì‹œ í™•ì¸ (ìºì‹œ í™œì„±í™” ë° Lighthouse timestamp ì¡´ì¬ ì‹œ)
                    if cache_enabled and lighthouse_timestamp:
                        cached_info = get_cached_image_info(imageurl, url_s_stripped, Config)
                        
                        if cached_info:
                            # ë©”íƒ€ë°ì´í„°ì—ì„œ Lighthouse timestamp ì¡°íšŒ
                            metadata = load_cache_metadata(url_s_stripped, Config)
                            cached_lighthouse_ts = metadata.get('lighthouse_timestamp')
                            
                            if cached_lighthouse_ts:
                                # timestamp ì¼ì¹˜ ë° TTL ê²€ì¦
                                timestamp_match, ttl_valid = is_cache_valid(
                                    lighthouse_timestamp,
                                    cached_lighthouse_ts,
                                    cache_ttl_days
                                )
                                
                                # TTLì´ ìœ íš¨í•œ ê²½ìš° ìºì‹œ ì‚¬ìš© (timestamp ì¼ì¹˜ ì—¬ë¶€ì™€ ë¬´ê´€)
                                # timestampëŠ” ê°™ì€ ë¶„ì„ ì„¸ì…˜ì¸ì§€ í™•ì¸ìš©ì´ë©°, TTL ë‚´ì—ì„œëŠ” ìºì‹œ ì¬ì‚¬ìš© ê°€ëŠ¥
                                if ttl_valid:
                                    # ì´ë¯¸ì§€ ë³€ê²½ ê°ì§€ (HEAD ìš”ì²­)
                                    changed = check_image_changed(
                                        imageurl,
                                        cached_info,
                                        dl_session,
                                        destination if os.path.exists(destination) else None
                                    )
                                    
                                    if not changed:
                                        # ìºì‹œ ì¬ì‚¬ìš©
                                        if os.path.exists(destination):
                                            file_size = os.path.getsize(destination)
                                            return {
                                                'name': filename,
                                                'path': destination,
                                                'size': file_size,
                                                'url': imageurl,
                                                'cached': True,  # ìºì‹œ ì¬ì‚¬ìš© í”Œë˜ê·¸
                                            }
                    
                    # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” íŒŒì¼ì¸ ê²½ìš° URL í•´ì‹œë¥¼ ì¶”ê°€í•˜ì—¬ ì¤‘ë³µ ë°©ì§€
                    if os.path.exists(destination):
                        import hashlib
                        url_hash = hashlib.md5(imageurl.encode()).hexdigest()[:8]
                        name, ext = os.path.splitext(filename)
                        filename = f"{name}_{url_hash}{ext}"
                        destination = os.path.join(image_dir_path, filename)
                    
                    logger.debug(f"[TASKS] Downloading image: {imageurl} -> {filename}")
                    
                    resp = dl_session.get(imageurl, verify=False, timeout=(3, 8))
                    if resp.status_code != 200:
                        logger.warning(f"[TASKS] Failed to download image: {imageurl} (status: {resp.status_code})")
                        return None
                    
                    # Content-Type í™•ì¸
                    content_type = resp.headers.get('Content-Type', '').lower()
                    if 'image' not in content_type:
                        logger.warning(f"[TASKS] Not an image: {imageurl} (Content-Type: {content_type})")
                        return None
                    
                    # ETag ë° Last-Modified ì¶”ì¶œ
                    etag = resp.headers.get('ETag', '').strip('"')
                    last_modified = resp.headers.get('Last-Modified')
                    
                    with open(destination, 'wb') as f:
                        f.write(resp.content)
                    if not os.path.exists(destination):
                        return None
                    file_size = os.path.getsize(destination)
                    
                    logger.debug(f"[TASKS] Downloaded: {filename} ({file_size} bytes) from {imageurl}")
                    
                    # ìºì‹œ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ (ë‹¤ìš´ë¡œë“œ ì™„ë£Œ í›„)
                    if cache_enabled and lighthouse_timestamp:
                        update_image_cache(
                            imageurl,
                            url_s_stripped,
                            filename,
                            destination,
                            file_size,
                            lighthouse_timestamp,
                            Config,
                            etag=etag if etag else None,
                            last_modified=last_modified if last_modified else None
                        )
                    
                    return {
                        'name': filename,
                        'path': destination,
                        'size': file_size,
                        'url': imageurl,  # ì›ë³¸ URL ì €ì¥ (ë””ë²„ê¹…ìš©)
                        'cached': False,  # ìƒˆë¡œ ë‹¤ìš´ë¡œë“œë¨
                    }
                except requests.exceptions.RequestException as req_err:
                    logger.warning(f"[TASKS] Request error downloading {imageurl}: {req_err}")
                    return None
                except Exception as e:
                    logger.error(f"[TASKS] Error downloading {imageurl}: {e}")
                    import traceback
                    logger.error(f"[TASKS] Traceback: {traceback.format_exc()}")
                    return None

            max_workers = min(3, len(Image_paths)) if len(Image_paths) > 0 else 0
            cached_count = 0
            download_count = 0
            failed_count = 0
            if max_workers > 0:
                with ThreadPoolExecutor(max_workers=max_workers) as executor:
                    future_map = {executor.submit(download_one, u): u for u in Image_paths}
                    for fut in as_completed(future_map):
                        res = fut.result()
                        if isinstance(res, dict):
                            downloaded.append(res)
                            if res.get('cached'):
                                cached_count += 1
                            else:
                                download_count += 1
                        else:
                            failed_count += 1
            

            # ì´ë¯¸ì§€ íŒŒì¼ ì •ë³´ ìˆ˜ì§‘ (ëª¨ë¸ ë¶„ë¥˜ ì œê±°: ëª¨ë“  ì´ë¯¸ì§€ë¥¼ WebP ë³€í™˜ ëŒ€ìƒìœ¼ë¡œ ì„¤ì •)
            files = []
            try:
                sizes = {it['name']: it['size'] for it in downloaded}
                for it in downloaded:
                    # section05ì—ì„œ í‘œì‹œí•  ì›ë³¸ ì´ë¯¸ì§€ ê²½ë¡œ ì„¤ì •
                    # ì›ë³¸ ì´ë¯¸ì§€ëŠ” <url>/<name> í˜•íƒœë¡œ ì €ì¥ë¨
                    # í…œí”Œë¦¿ì—ì„œ var/optimization_images/ë¥¼ ì œê±°í•˜ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    rel_url = f"var/optimization_images/{url_s_stripped}/{it['name']}"
                    
                    files.append({
                        'name': it['name'],
                        'url': rel_url,  # ì›ë³¸ ì´ë¯¸ì§€ ê²½ë¡œ (section05 í‘œì‹œìš©)
                        'size': sizes.get(it['name'], it['size'])
                    })
            except Exception as e:
                current_app.logger.error(f"ë¶„ì„ ì‹¤íŒ¨: ì´ë¯¸ì§€ íŒŒì¼ ì •ë³´ ì²˜ë¦¬ ì˜¤ë¥˜ - {str(e)}")
                # ì˜ˆì™¸ ë°œìƒ ì‹œì—ë„ ê¸°ë³¸ íŒŒì¼ ì •ë³´ëŠ” ìœ ì§€
                for it in downloaded:
                    files.append({
                        'name': it['name'],
                        'url': f"var/optimization_images/{url_s_stripped}/{it['name']}",
                        'size': it['size']
                    })

            category = {'iconfile': [], 'logofile': [], 'others': []}
            webpfiles = []
            total_downloaded_image_bytes = 0
            eligible_original_image_bytes = 0
            # ëª¨ë“  ì´ë¯¸ì§€ë¥¼ WebP ë³€í™˜ ëŒ€ìƒìœ¼ë¡œ ì„¤ì • (ëª¨ë¸ ë¶„ë¥˜ ì œê±°)
            for fi in files:
                total_downloaded_image_bytes += fi['size']
                webpfiles.append(fi)
                eligible_original_image_bytes += fi['size']
                # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ëŠ” íŒŒì¼ëª… ê¸°ë°˜ìœ¼ë¡œë§Œ ìˆ˜í–‰
                if 'ico' in fi['name']:
                    category['iconfile'].append(fi)
                elif 'logo' in fi['name']:
                    category['logofile'].append(fi)
                else:
                    category['others'].append(fi)

            time.sleep(0.5)
            # webp/ ë””ë ‰í† ë¦¬ë¡œ ë³€ê²½
            webp_output_dir = os.path.join(image_dir_path, 'webp')
            os.makedirs(webp_output_dir, exist_ok=True)
            selected_paths = []
            for f in webpfiles:
                # ì›ë³¸ ì´ë¯¸ì§€ ê²½ë¡œ ì§ì ‘ ì‚¬ìš© (copied_path ì œê±°)
                original_path = None
                for it in downloaded:
                    if it['name'] == f['name']:
                        original_path = it.get('path')
                        break
                
                if original_path and os.path.exists(original_path):
                    # ì´ë¯¸ WebP íŒŒì¼ì¸ ê²½ìš° ì œì™¸
                    orig_path = Path(original_path)
                    if orig_path.suffix.lower() == '.webp':
                        continue
                    # ì´ë¯¸ì§€ íŒŒì¼ë§Œ í¬í•¨ (PNG, JPG, JPEG)
                    if orig_path.suffix.lower() in ['.png', '.jpg', '.jpeg']:
                        selected_paths.append(original_path)
            # WebP ë³€í™˜ (ë³€í™˜ ê³¼ì •ì—ì„œ ìë™ìœ¼ë¡œ í¬ê¸°ê°€ ë” í° ì´ë¯¸ì§€ í•„í„°ë§ë¨)
            convertedfiles, webp_total_size, success_count, failed_count = png2webp.convert_to_webp(
                image_dir_path, 
                webp_output_dir, 
                selected_files=selected_paths,
                filter_larger=True  # ë³€í™˜ í›„ í¬ê¸°ê°€ ë” í° ì´ë¯¸ì§€ ìë™ ì œì™¸
            )
            
            # webp_name í•„ë“œ ì„¤ì • ë° ìºì‹œ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ (í•„í„°ë§ì€ convert_to_webp ë‚´ë¶€ì—ì„œ ì´ë¯¸ ì²˜ë¦¬ë¨)
            for _item in convertedfiles:
                if isinstance(_item, dict):
                    # webp_nameì´ ì—†ìœ¼ë©´ nameì„ ì‚¬ìš© (í•˜ìœ„ í˜¸í™˜ì„±)
                    if 'webp_name' not in _item and 'name' in _item:
                        _item['webp_name'] = _item['name']
                    
                    # WebP íŒŒì¼ì˜ ì‹¤ì œ ê²½ë¡œ ê³„ì‚° (ë‹¨ìˆœ íŒŒì¼ëª… ì‚¬ìš©)
                    webp_name = _item.get('webp_name', _item.get('name', ''))
                    webp_file_path = os.path.join(webp_output_dir, webp_name)
                    
                    # WebP ì •ë³´ë¥¼ ìºì‹œ ë©”íƒ€ë°ì´í„°ì— ì—…ë°ì´íŠ¸
                    if cache_enabled and lighthouse_timestamp:
                        # ì›ë³¸ íŒŒì¼ëª…ìœ¼ë¡œ ì´ë¯¸ì§€ URL ì°¾ê¸°
                        original_filename = _item.get('original_name') or _item.get('name').replace('.webp', '')
                        for it in downloaded:
                            if it['name'] == original_filename or it['name'].startswith(original_filename):
                                if os.path.exists(webp_file_path):
                                    webp_size = os.path.getsize(webp_file_path)
                                    update_image_cache(
                                        it['url'],
                                        url_s_stripped,
                                        it['name'],
                                        it['path'],
                                        it['size'],
                                        lighthouse_timestamp,
                                        Config,
                                        webp_path=webp_file_path,
                                        webp_size=webp_size
                                    )
                                break
            
            # í•„í„°ë§ëœ ì´ë¯¸ì§€ë“¤ì˜ ì›ë³¸ í¬ê¸° ê³„ì‚° (convert_to_webpì—ì„œ ì´ë¯¸ í•„í„°ë§ë¨)
            filtered_eligible_original_image_bytes = sum(
                item.get('original_size', 0) 
                for item in convertedfiles 
                if isinstance(item, dict)
            )
            eligible_original_image_bytes = filtered_eligible_original_image_bytes
            
            convertedfiles.sort(key=lambda x: x['name'], reverse=False)
            webpfiles.sort(key=lambda x: x['name'], reverse=False)
            time.sleep(0.5)

            # ìº¡ì²˜ ìˆ˜í–‰ (Phase 4: Playwright asyncë¡œ ì „í™˜, Worker ë¸”ë¡œí‚¹ ì—†ìŒ)
            captured_image_path = None
            try:
                # WebsiteCapture í´ë˜ìŠ¤ ì‚¬ìš© (Playwright ê¸°ë°˜)
                from ecoweb.app.services.capture.website import WebsiteCapture
                website_capture = WebsiteCapture()
                
                # ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰
                import asyncio
                async def _capture_task():
                    return await website_capture.capture_with_highlight(
                        f"https://{url_s_stripped}",
                        user_id=str(user_id) if user_id is not None else None,
                        task_id=str(original_task_id) if original_task_id is not None else None
                    )
                capture_result = asyncio.run(_capture_task())
                if capture_result and capture_result.get('success'):
                    captured_image_filename = capture_result.get('filename')
                    # filename may already include task_id subdirectory
                    captured_image_path = f"var/captures/{captured_image_filename}"
            except Exception as e:
                pass  # ìº¡ì²˜ ì‹¤íŒ¨ëŠ” ì¡°ìš©íˆ ì²˜ë¦¬

            # CO2 ì ˆê°ëŸ‰ ê³„ì‚°
            saved_bytes = int(eligible_original_image_bytes) - int(webp_total_size)
            co2_saved = 0
            if saved_bytes > 0:
                saved_gb = saved_bytes / (1024**3)
                co2_saved = estimate_emission_per_page(saved_gb)

            # ì‹ ê·œ ì§€í‘œ ê³„ì‚° (DB ê¸°ë°˜)
            converted_stems = set()
            try:
                from pathlib import Path as _Path
                for _cf in convertedfiles:
                    if isinstance(_cf, dict) and 'name' in _cf:
                        converted_stems.add(_Path(_cf['name']).stem)
            except Exception:
                pass

            db_images = []
            try:
                requests_list = (doc.get('networkRequests') or doc.get('network_requests')) if doc else None
                if requests_list:
                    import urllib.parse as _urlparse
                    for item in requests_list:
                        rtype = (item.get('resourceType') or item.get('resource_type') or '').lower()
                        if rtype != 'image':
                            url_val = item.get('url', '')
                            if not url_val.lower().endswith(('.jpg', '.jpeg', '.png', '.gif', '.webp')):
                                continue
                        url_val = item.get('url', '')
                        try:
                            parsed = _urlparse.urlparse(url_val)
                            basename = os.path.basename(parsed.path)
                        except Exception:
                            parts = re.split(r':|\/|\.', url_val)
                            basename = parts[-2] + '.' + parts[-1] if len(parts) >= 2 else url_val
                        size = (
                            item.get('resourceSize')
                            or item.get('resource_size')
                            or item.get('transferSize')
                            or item.get('transfer_size')
                            or 0
                        )
                        try:
                            size = int(size)
                        except Exception:
                            size = 0
                        db_images.append({'basename': basename, 'stem': os.path.splitext(basename)[0], 'size': size})
            except Exception as _e:
                pass

            optimized_image_total_bytes = 0
            optimization_saved_bytes = 0
            converted_image_count = len(convertedfiles)
            
            # optimization_saved_bytes ê³„ì‚°: ì‹¤ì œ ë‹¤ìš´ë¡œë“œëœ ì´ë¯¸ì§€ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°
            # eligible_original_image_bytesëŠ” WebP ë³€í™˜ ëŒ€ìƒ ì´ë¯¸ì§€ë“¤ì˜ ì›ë³¸ í¬ê¸°
            # webp_total_sizeëŠ” ë³€í™˜ëœ WebP ì´ë¯¸ì§€ë“¤ì˜ ì´ í¬ê¸°
            optimization_saved_bytes = max(int(eligible_original_image_bytes) - int(webp_total_size), 0)
            
            # optimized_image_total_bytes ê³„ì‚°: ìµœì í™” í›„ ì „ì²´ ì´ë¯¸ì§€ í¬ê¸°
            # = WebP ë³€í™˜ëœ ì´ë¯¸ì§€ í¬ê¸° + ë³€í™˜ë˜ì§€ ì•Šì€ ì´ë¯¸ì§€ í¬ê¸°
            # ë³€í™˜ë˜ì§€ ì•Šì€ ì´ë¯¸ì§€ = ì „ì²´ ë‹¤ìš´ë¡œë“œ ì´ë¯¸ì§€ - ë³€í™˜ ëŒ€ìƒ ì´ë¯¸ì§€
            unconverted_image_bytes = max(int(total_downloaded_image_bytes) - int(eligible_original_image_bytes), 0)
            optimized_image_total_bytes = int(webp_total_size) + int(unconverted_image_bytes)
            
            # converted_image_countëŠ” ì´ë¯¸ ê³„ì‚°ë¨ (ìœ„ì—ì„œ len(convertedfiles))

            # ìµœì í™” íš¨ê³¼ íŒë‹¨: ë³€í™˜ ëŒ€ìƒ ì´ë¯¸ì§€ì˜ ì›ë³¸ í¬ê¸°ì™€ WebP ë³€í™˜ í›„ í¬ê¸°ë¥¼ ë¹„êµ
            # webp_total_sizeëŠ” ì´ë¯¸ í•„í„°ë§ë˜ì–´ ì›ë³¸ë³´ë‹¤ ì‘ì€ ì´ë¯¸ì§€ë§Œ í¬í•¨í•˜ë¯€ë¡œ,
            # webp_total_size >= eligible_original_image_bytesì¸ ê²½ìš°ëŠ” ë³€í™˜ íš¨ê³¼ê°€ ì—†ëŠ” ê²½ìš°
            is_already_optimized = False
            if eligible_original_image_bytes > 0:
                # ë³€í™˜ ëŒ€ìƒ ì´ë¯¸ì§€ê°€ ìˆê³ , WebP ë³€í™˜ í›„ í¬ê¸°ê°€ ì›ë³¸ë³´ë‹¤ í¬ê±°ë‚˜ ê°™ìœ¼ë©´ "ì´ë¯¸ ìµœì í™”ë¨"
                if webp_total_size >= eligible_original_image_bytes:
                    is_already_optimized = True
            elif total_downloaded_image_bytes > 0:
                # ë³€í™˜ ëŒ€ìƒ ì´ë¯¸ì§€ê°€ ì—†ê³ , ì „ì²´ ë‹¤ìš´ë¡œë“œ ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ìµœì í™”í•  ì´ë¯¸ì§€ê°€ ì—†ëŠ” ê²½ìš°
                # í•˜ì§€ë§Œ ì´ ê²½ìš°ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ìµœì í™”ê°€ í•„ìš” ì—†ëŠ” ê²½ìš°ì´ë¯€ë¡œ Falseë¡œ ìœ ì§€
                is_already_optimized = False

            # ê²°ê³¼ íŒ¨í‚¤ì§• ë° view_dataì— ì €ì¥ (í–¥í›„ ë¼ìš°íŠ¸ì—ì„œ ì¬ì‚¬ìš©)
            image_opt_result = {
                'captured_image_path': captured_image_path,
                'category': category,
                'files': files,  # ëª¨ë“  ë¶„ë¥˜ëœ ì´ë¯¸ì§€ í¬í•¨ (webpfilesê°€ ì•„ë‹Œ files)
                'filecount': len(files),  # ì „ì²´ íŒŒì¼ ìˆ˜
                'totalsize': total_downloaded_image_bytes,
                'total_downloaded_image_bytes': total_downloaded_image_bytes,
                'eligible_original_image_bytes': eligible_original_image_bytes,
                'optimized_image_total_bytes': optimized_image_total_bytes,
                'optimization_saved_bytes': optimization_saved_bytes,
                'converted_image_count': converted_image_count,
                'convertedfiles': convertedfiles,
                'url_s': url_s_stripped,
                'webp_total_size': webp_total_size,
                'co2_saved': co2_saved,
                'is_already_optimized': is_already_optimized,  # ìƒˆ í•„ë“œ ì¶”ê°€
            }

            # view_data ê°•í™” ë° DBì— ë³„ë„ í•„ë“œë¡œ ì €ì¥
            try:
                view_data['image_optimization'] = image_opt_result
            except Exception:
                pass

            # [9-1] ì¤‘ê°„ ì €ì¥ (ì§„í–‰ ìƒíƒœë§Œ ì—…ë°ì´íŠ¸, resultëŠ” ìµœì¢… ì €ì¥ ì‹œì—ë§Œ)
            task_results_collection.update_one(
                {'_id': original_task_id},
                {'$set': {
                    'progress.steps.image_opt': {'status': 'done', 'message': 'ì´ë¯¸ì§€ ìµœì í™” ì™„ë£Œ'},
                    'progress.updated_at': datetime.utcnow().isoformat()
                }},
                upsert=True
            )
            
            # ì´ë¯¸ì§€ ìµœì í™” ì™„ë£Œ ë¡œê¹…
            total_images = len(downloaded)
            if cache_enabled and cached_count > 0:
                current_app.logger.info(f"ì´ë¯¸ì§€ ìµœì í™” ì™„ë£Œ: ì´ {total_images}ê°œ (ìºì‹œ ì¬ì‚¬ìš©: {cached_count}ê°œ, ìƒˆë¡œ ë‹¤ìš´ë¡œë“œ: {download_count}ê°œ, ì‹¤íŒ¨: {failed_count}ê°œ)")
            else:
                if download_count > 0 or failed_count > 0:
                    current_app.logger.info(f"ì´ë¯¸ì§€ ìµœì í™” ì™„ë£Œ: ì´ {total_images}ê°œ (ë‹¤ìš´ë¡œë“œ: {download_count}ê°œ, ì‹¤íŒ¨: {failed_count}ê°œ)")
                else:
                    current_app.logger.info(f"ì´ë¯¸ì§€ ìµœì í™” ì™„ë£Œ: ì´ {total_images}ê°œ")
        except Exception as e:
            current_app.logger.error(f"ë¶„ì„ ì‹¤íŒ¨: ì´ë¯¸ì§€ ìµœì í™” ì˜¤ë¥˜ - {str(e)}")
            try:
                task_results_collection.update_one(
                    {'_id': original_task_id},
                    {'$set': {
                        'progress.steps.image_opt': {'status': 'failed', 'message': str(e)},
                        'progress.updated_at': datetime.utcnow().isoformat()
                    }}
                )
            except Exception:
                pass

        # [9-2] ì½”ë“œ ë¶„ì„ ë°ì´í„° ì‚¬ì „ ì²˜ë¦¬ (directory_maker + CO2 ê³„ì‚°)
        # í™˜ê²½ ë³€ìˆ˜ë¡œ ì½”ë“œ ë¶„ì„ ê¸°ëŠ¥ ì œì–´ (ê¸°ë³¸ê°’: í™œì„±í™”)
        enable_code_analysis = os.getenv('ENABLE_CODE_ANALYSIS', 'true').lower() == 'true'

        if enable_code_analysis:
            # ì½”ë“œ ë¶„ì„ ì „ ì·¨ì†Œ í™•ì¸
            check_task_cancelled()
            try:
                # ì§„í–‰ ìƒíƒœ: ì½”ë“œ ë¶„ì„ ì‹œì‘
                try:
                    task_results_collection.update_one(
                        {'_id': original_task_id},
                        {'$set': {
                            'progress.steps.code_analysis': {'status': 'in_progress', 'message': 'ì½”ë“œ ë¶„ì„ ë°ì´í„° ìƒì„± ì¤‘'},
                            'progress.current_step': 'code_analysis',
                            'progress.updated_at': datetime.utcnow().isoformat()
                        }}
                    )
                except Exception:
                    pass

                # DirectoryMaker ë° CO2 ê³„ì‚°
                from ecoweb.app.ProjectMaker.DirectoryMaker import (
                    directory_maker, 
                    directory_to_json,
                    build_directory_structure_from_urls,
                    get_network_requests
                )

                # directory_maker: ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ ë° ë¡œì»¬ ë””ë ‰í„°ë¦¬ êµ¬ì¡° ìƒì„±
                directory_structure = {}
                try:
                    # ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ ì˜µì…˜ í™•ì¸
                    download_resources = Config.ENABLE_RESOURCE_DOWNLOAD
                    
                    if download_resources:
                        # ê¸°ì¡´ ë°©ì‹: ì‹¤ì œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
                        root_path = directory_maker(
                            url=url,
                            collection_traffic=collection_traffic,
                            collection_resource=collection_resource,
                            download_resources=True
                        )
                        # ë””ë ‰í„°ë¦¬ êµ¬ì¡°ë¥¼ JSONìœ¼ë¡œ ë³€í™˜
                        directory_structure = directory_to_json(root_path=root_path)
                    else:
                        # ìµœì í™” ë°©ì‹: URLë§Œìœ¼ë¡œ êµ¬ì¡° ìƒì„± (ë‹¤ìš´ë¡œë“œ ì—†ìŒ)
                        # ì´ë¯¸ ì¡°íšŒëœ resource_doc ì¬ì‚¬ìš© (ìµœì í™”)
                        urls = []
                        if resource_doc:
                            # resource_docì—ì„œ network_requests ì¶”ì¶œ
                            requests_list = resource_doc.get('networkRequests') or resource_doc.get('network_requests') or []
                            urls = [
                                req.get('url', '') for req in requests_list
                                if req.get('url')
                            ]
                        else:
                            # fallback: get_network_requests í˜¸ì¶œ
                            documents = get_network_requests(collection_resource=collection_resource, url=url)
                            urls = [
                                doc["url"] for doc in documents
                                if doc.get("url")
                            ]
                        
                        if urls:
                            structure_dict = build_directory_structure_from_urls(urls)
                            directory_structure = directory_to_json(structure_dict=structure_dict)
                except Exception as e:
                    pass
                    try:
                        from ecoweb.app.ProjectMaker.DirectoryMaker import generate_sample_directory_structure
                        directory_structure = generate_sample_directory_structure()
                    except Exception:
                        directory_structure = {"project": {"__files__": ["index.html", "main.js", "styles.css"]}}

                # CO2 ë°°ì¶œëŸ‰ ê³„ì‚°
                co2_emissions = {}
                total_js_bytes = view_data.get('total_resource_bytes_script', 0)
                unused_js_bytes = view_data.get('total_unused_bytes_script', 0)
                view_data['used_js_script_size'] = max(total_js_bytes - unused_js_bytes, 0)

                metrics_to_calculate_co2 = [
                    'total_byte_weight',
                    'third_party_summary_wasted_bytes',
                    'total_resource_bytes_script',
                    'total_unused_bytes_script',
                    'used_js_script_size',
                    'can_optimize_css_bytes',
                    'modern_image_formats_bytes',
                    'efficient_animated_content_bytes',
                    'duplicated_javascript_bytes'
                ]

                for key in metrics_to_calculate_co2:
                    byte_value = view_data.get(key, 0)
                    try:
                        byte_value = int(byte_value) if byte_value else 0
                    except (ValueError, TypeError):
                        byte_value = 0

                    if byte_value > 0:
                        gb_value = byte_value / (1024 * 1024 * 1024)
                        co2_emissions[key] = estimate_emission_per_page(data_gb=gb_value)
                    else:
                        co2_emissions[key] = 0.0

                # í°íŠ¸ ìµœì í™” ë°ì´í„° ê³„ì‚°
                font_total_bytes = view_data.get('font_total_bytes', 0)
                detailed_font_info = view_data.get('detailed_font_info', [])

                current_font_co2_emission = 0.0
                if font_total_bytes > 0:
                    font_data_gb = font_total_bytes / (1024 * 1024 * 1024)
                    current_font_co2_emission = estimate_emission_per_page(data_gb=font_data_gb)

                # í°íŠ¸ ìµœì í™” ë°©ë²• ì„¤ì • (optimization.pyì—ì„œ ë³µì‚¬)
                FONT_OPTIMIZATION_METHODS_CONFIG = [
                    {'name': 'WOFFë¡œ ë³€ê²½', 'description': 'ì›¹ ìµœì í™” ì••ì¶• í˜•ì‹. ëŒ€ë¶€ë¶„ì˜ ëª¨ë˜ ë¸Œë¼ìš°ì €ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.', 'size_multiplier': 0.55},
                    {'name': 'WOFF2ë¡œ ë³€ê²½', 'description': 'WOFFë³´ë‹¤ í–¥ìƒëœ ì••ì¶•ë¥ ì„ ì œê³µí•˜ëŠ” ì°¨ì„¸ëŒ€ ì›¹ í°íŠ¸ í˜•ì‹ì…ë‹ˆë‹¤.', 'size_multiplier': 0.50},
                    {'name': 'ì„œë¸Œì…‹ í°íŠ¸ ì ìš©', 'description': 'ì›¹ì‚¬ì´íŠ¸ì— ì‹¤ì œ ì‚¬ìš©ë˜ëŠ” ê¸€ìë“¤ë§Œ í¬í•¨í•˜ì—¬ í°íŠ¸ íŒŒì¼ í¬ê¸°ë¥¼ ì¤„ì…ë‹ˆë‹¤.', 'size_multiplier': 0.25},
                    {'name': 'ê°€ë³€ í°íŠ¸ë¡œ ë³€ê²½', 'description': 'í•˜ë‚˜ì˜ í°íŠ¸ íŒŒì¼ë¡œ ë‹¤ì–‘í•œ ìŠ¤íƒ€ì¼ì„ ì§€ì›í•˜ì—¬ ì—¬ëŸ¬ ì •ì  í°íŠ¸ íŒŒì¼ ìš”ì²­ì„ ì¤„ì…ë‹ˆë‹¤.', 'size_multiplier': 0.30},
                    {'name': 'ì‹œìŠ¤í…œ í°íŠ¸ë¡œ ë³€ê²½', 'description': 'ì›¹ í°íŠ¸ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ëŠ” ëŒ€ì‹  ì‚¬ìš©ì ìš´ì˜ì²´ì œì˜ ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.', 'size_multiplier': 0.00}
                ]

                font_optimization_data = []
                if font_total_bytes > 0:
                    for method in FONT_OPTIMIZATION_METHODS_CONFIG:
                        original_size_bytes = font_total_bytes
                        reduced_size_bytes = original_size_bytes * method['size_multiplier']
                        saved_bytes = original_size_bytes - reduced_size_bytes

                        emissions_gCO2eq = 0.0
                        if reduced_size_bytes > 0:
                            reduced_size_gb = reduced_size_bytes / (1024 * 1024 * 1024)
                            emissions_gCO2eq = estimate_emission_per_page(data_gb=reduced_size_gb)

                        font_optimization_data.append({
                            'name': method['name'],
                            'description': method['description'],
                            'saved_bytes': saved_bytes,
                            'emissions_gCO2eq': emissions_gCO2eq
                        })

                # ì½”ë“œ ìµœì í™” ë°ì´í„° ì¶”ì¶œ (final_report ê¸°ë°˜)
                # ì´ë¯¸ ì¡°íšŒëœ traffic_doc ì¬ì‚¬ìš© (ìµœì í™”)
                code_optimization_data = {'total_wasted_bytes': 0, 'co2_saving': 0.0, 'unused_css_count': 0, 'unused_js_count': 0, 'unused_css_rules': [], 'unused_javascript': []}
                try:
                    # ì¼ê´„ ì¡°íšŒí•œ traffic_doc ì¬ì‚¬ìš©, ì—†ìœ¼ë©´ ì¡°íšŒ (fallback)
                    code_traffic_doc = traffic_doc
                    if not code_traffic_doc:
                        code_traffic_doc = collection_traffic.find_one({'url': url}, {'_id': 0, 'audits': 1})
                    
                    if code_traffic_doc and 'audits' in code_traffic_doc:
                        audits = code_traffic_doc.get('audits', {})
                        unused_css_rules = audits.get('unused-css-rules', {}).get('details', {}).get('items', [])
                        unused_javascript = audits.get('unused-javascript', {}).get('details', {}).get('items', [])

                        total_css_wasted_bytes = sum(item.get('wastedBytes', 0) for item in unused_css_rules)
                        total_js_wasted_bytes = sum(item.get('wastedBytes', 0) for item in unused_javascript)
                        total_wasted_bytes = total_css_wasted_bytes + total_js_wasted_bytes

                        co2_saving = estimate_emission_per_page((total_wasted_bytes or 0) / (1024 * 1024 * 1024))

                        code_optimization_data = {
                            'total_wasted_bytes': total_wasted_bytes,
                            'co2_saving': co2_saving,
                            'unused_css_count': len(unused_css_rules),
                            'unused_js_count': len(unused_javascript),
                            'unused_css_rules': unused_css_rules,
                            'unused_javascript': unused_javascript
                        }
                except Exception as e:
                    pass

                # ê²°ê³¼ íŒ¨í‚¤ì§•
                code_analysis_result = {
                    'directory_structure': directory_structure,
                    'co2_emissions': co2_emissions,
                    'font_total_bytes': font_total_bytes,
                    'current_font_co2_emission': current_font_co2_emission,
                    'detailed_font_info': detailed_font_info,
                    'font_optimization_data': font_optimization_data,
                    'code_optimization_data': code_optimization_data
                }

                # view_data ê°•í™”
                try:
                    view_data['code_analysis'] = code_analysis_result
                except Exception:
                    pass

                # ì¤‘ê°„ ì €ì¥ (ì§„í–‰ ìƒíƒœë§Œ ì—…ë°ì´íŠ¸, resultëŠ” ìµœì¢… ì €ì¥ ì‹œì—ë§Œ)
                task_results_collection.update_one(
                    {'_id': original_task_id},
                    {'$set': {
                        'progress.steps.code_analysis': {'status': 'done', 'message': 'ì½”ë“œ ë¶„ì„ ì™„ë£Œ'},
                        'progress.updated_at': datetime.utcnow().isoformat()
                    }},
                    upsert=True
                )
            except Exception as e:
                current_app.logger.error(f"ë¶„ì„ ì‹¤íŒ¨: ì½”ë“œ ë¶„ì„ ì˜¤ë¥˜ - {str(e)}")
                try:
                    task_results_collection.update_one(
                        {'_id': original_task_id},
                        {'$set': {
                            'progress.steps.code_analysis': {'status': 'failed', 'message': str(e)},
                            'progress.updated_at': datetime.utcnow().isoformat()
                        }}
                    )
                except Exception:
                    pass

        # [10] ë°ì´í„° ê°•í™”: ëª¨ë“  íŒŒìƒ ë°ì´í„° ê³„ì‚° (Phase 1: Session-to-DB Refactoring)
        current_app.logger.info("[ENRICH] view_data ê°•í™” ì‹œì‘: ëª¨ë“  íŒŒìƒ ë°ì´í„° ê³„ì‚°")
        try:
            # ì¼ê´„ ì¡°íšŒí•œ ë°ì´í„°ë¥¼ ì „ë‹¬í•˜ì—¬ ì¤‘ë³µ ì¡°íšŒ ë°©ì§€
            enriched_result = _enrich_view_data(view_data, url, mongo_db, resource_doc=resource_doc, traffic_doc=traffic_doc)
            current_app.logger.info("[ENRICH] view_data ê°•í™” ì™„ë£Œ: calculated ì„¹ì…˜ ì¶”ê°€ë¨")
        except Exception as e:
            current_app.logger.error(f"[ENRICH] view_data ê°•í™” ì‹¤íŒ¨: {e}, ì›ë³¸ ë°ì´í„° ì‚¬ìš©", exc_info=True)
            enriched_result = view_data  # ê°•í™” ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì‚¬ìš©

        # [11] ì¸¡ì • ì™„ë£Œ ê²°ê³¼ë¥¼ task_results ì»¬ë ‰ì…˜ì— ì €ì¥ (ìµœì¢… ìƒíƒœ MEASUREMENT_COMPLETE)
        update_result = task_results_collection.update_one(
            {'_id': original_task_id},
            {'$set': {
                'status': 'MEASUREMENT_COMPLETE',
                'result': enriched_result,  # â† enriched_result ì‚¬ìš© (calculated ì„¹ì…˜ í¬í•¨)
                'completed_at': datetime.utcnow()
            }},
            upsert=True
        )
        current_app.logger.info(f"MongoDB ì €ì¥ ê²°ê³¼: ì¼ì¹˜={update_result.matched_count}, ìˆ˜ì •={update_result.modified_count}, ì‚½ì…ID={update_result.upserted_id}")

        current_app.logger.info(f'Celery ì‘ì—… ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ: URL={url}')
        return {'status': 'MEASUREMENT_COMPLETE'}

    except Exception as e:
        # [11] ì˜ˆì™¸ ì²˜ë¦¬: ì‹¤íŒ¨ ìƒíƒœ ë° ì˜¤ë¥˜ ë©”ì‹œì§€ ì €ì¥
        # ì·¨ì†Œëœ ì‘ì—…ì¸ ê²½ìš° ë‹¤ë¥´ê²Œ ì²˜ë¦¬
        if 'cancelled by user' in str(e).lower() or 'task cancelled' in str(e).lower():
            current_app.logger.info(f'Celery ì‘ì—… ì·¨ì†Œë¨: URL={url}, ì‚¬ìœ ={e}')
            return {'status': 'CANCELLED', 'reason': str(e)}

        current_app.logger.error(f'Celery ì‘ì—… ì‹¤íŒ¨: URL={url}, ì˜¤ë¥˜={e}', exc_info=True)

        # ì‹¤íŒ¨í•œ ë‹¨ê³„ì— ë”°ë¼ ì ì ˆí•œ ìƒíƒœ ì—…ë°ì´íŠ¸
        update_data = {
            'status': 'FAILURE',
            'error': str(e),
            'progress.updated_at': datetime.utcnow().isoformat()
        }

        # í˜„ì¬ ì§„í–‰ ìƒíƒœë¥¼ í™•ì¸í•˜ì—¬ ì ì ˆí•œ ë‹¨ê³„ë¥¼ ì‹¤íŒ¨ë¡œ í‘œì‹œ
        try:
            current_task = task_results_collection.find_one({'_id': original_task_id})
            current_step = current_task.get('progress', {}).get('current_step', 'input') if current_task else 'input'

            if current_step == 'input':
                update_data['progress.steps.input'] = {'status': 'failed', 'message': str(e)}
            elif current_step == 'subpages':
                update_data['progress.steps.subpages'] = {'status': 'failed', 'message': f'í•˜ìœ„ í˜ì´ì§€ ë¶„ì„ ì‹¤íŒ¨: {str(e)}'}
            elif current_step == 'image_opt':
                update_data['progress.steps.image_opt'] = {'status': 'failed', 'message': f'ì´ë¯¸ì§€ ìµœì í™” ì‹¤íŒ¨: {str(e)}'}
            else:
                update_data['progress.steps.input'] = {'status': 'failed', 'message': str(e)}
        except Exception:
            # í˜„ì¬ ë‹¨ê³„ í™•ì¸ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ì ìœ¼ë¡œ input ë‹¨ê³„ë¥¼ ì‹¤íŒ¨ë¡œ ì„¤ì •
            update_data['progress.steps.input'] = {'status': 'failed', 'message': str(e)}

        task_results_collection.update_one(
            {'_id': original_task_id},
            {'$set': update_data},
            upsert=True
        )
        return {'status': 'FAILURE', 'error': str(e)}
    finally:
        # [12] í›„ì²˜ë¦¬: íì— ë‚¨ì•„ìˆëŠ” ë‹¤ìŒ ì‘ì—… ì²˜ë¦¬ ì‹œë„
        # ì·¨ì†Œëœ ì‘ì—…ì€ í›„ì† ì²˜ë¦¬ë¥¼ ê±´ë„ˆëœ€
        try:
            mongo_db = db.get_db()
            task_doc = mongo_db.task_results.find_one({'_id': original_task_id})
            if task_doc and task_doc.get('status') == 'CANCELLED':
                current_app.logger.info(f'Task {original_task_id} was cancelled, skipping queue processing')
                return
        except Exception:
            pass

        current_app.logger.info("ì‘ì—… ì¢…ë£Œ: íì˜ ë‹¤ìŒ ì‘ì—…ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        try:
            # ìˆœí™˜ ì°¸ì¡°ë¥¼ í”¼í•˜ê¸° ìœ„í•´ í•¨ìˆ˜ ë‚´ì—ì„œ import í•©ë‹ˆë‹¤.
            from ecoweb.app.blueprints.main import process_queued_tasks
            process_queued_tasks()
        except ImportError as ie:
            current_app.logger.error(f"process_queued_tasks ì„í¬íŠ¸ ì‹¤íŒ¨: {ie}")
        except Exception as final_e:
            current_app.logger.error(f"process_queued_tasks í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {final_e}", exc_info=True)

@celery.task(bind=True, ignore_result=False)
def generate_pdf_report_task(self, session_data: dict, user_id, original_task_id):
    """
    Celery task to generate PDF report in the background.

    Args:
        session_data: Session data containing URL and analysis results
        user_id: User ID for organizing PDF files
        original_task_id: MongoDB task document ID for progress tracking

    Returns:
        dict: Task result with status and file information
    """
    import io
    from ecoweb.app.utils.task_cancellation import check_task_cancelled_legacy

    def check_task_cancelled():
        """ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê³ , ì·¨ì†Œëœ ê²½ìš° ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤."""
        check_task_cancelled_legacy(original_task_id, current_app.logger)

    try:
        # MongoDB ì»¬ë ‰ì…˜ í•¸ë“¤
        mongo_db = db.get_db()
        pdf_tasks_collection = mongo_db.pdf_generation_tasks

        # ì´ˆê¸° ì·¨ì†Œ í™•ì¸
        check_task_cancelled()

        # [2] ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸: ì´ˆê¸°í™” ë‹¨ê³„
        pdf_tasks_collection.update_one(
            {'_id': original_task_id},
            {'$set': {
                'status': 'PROCESSING',
                'progress': {
                    'current_step': 'initialization',
                    'message': 'PDF ìƒì„±ê¸° ì´ˆê¸°í™” ì¤‘',
                    'updated_at': datetime.utcnow().isoformat()
                }
            }}
        )

        # PDF ìƒì„±ê¸° ì´ˆê¸°í™”
        from ecoweb.app.services.report import PlaywrightPDFGenerator
        pdf_generator = PlaywrightPDFGenerator()

        # [3] ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸: PDF ìƒì„± ë‹¨ê³„
        pdf_tasks_collection.update_one(
            {'_id': original_task_id},
            {'$set': {
                'progress': {
                    'current_step': 'generating',
                    'message': 'PDF í˜ì´ì§€ ìƒì„± ì¤‘ (1/13)',
                    'updated_at': datetime.utcnow().isoformat()
                }
            }}
        )

        # í•˜íŠ¸ë¹„íŠ¸ ì“°ë ˆë“œ: PDF ìƒì„± ì¤‘ ì§„í–‰ ìƒíƒœ ì£¼ê¸°ì  ì—…ë°ì´íŠ¸
        _hb_stop = threading.Event()
        _hb_page_counter = {'current': 1}

        def _heartbeat():
            """PDF ìƒì„± ì¤‘ ì§„í–‰ ìƒíƒœë¥¼ ì£¼ê¸°ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ëŠ” í•˜íŠ¸ë¹„íŠ¸ ì“°ë ˆë“œ"""
            start_ts = time.time()
            while not _hb_stop.is_set():
                try:
                    # ì·¨ì†Œ í™•ì¸
                    task_doc = pdf_tasks_collection.find_one({'_id': original_task_id})
                    if task_doc and task_doc.get('status') == 'CANCELLED':
                        _hb_stop.set()
                        break

                    elapsed = int(time.time() - start_ts)
                    current_page = _hb_page_counter.get('current', 1)

                    pdf_tasks_collection.update_one(
                        {'_id': original_task_id},
                        {'$set': {
                            'progress': {
                                'current_step': 'generating',
                                'message': f'PDF í˜ì´ì§€ ìƒì„± ì¤‘ ({current_page}/13) - {elapsed}s',
                                'updated_at': datetime.utcnow().isoformat()
                            }
                        }}
                    )
                except Exception:
                    pass
                _hb_stop.wait(3.0)  # 3ì´ˆë§ˆë‹¤ ì—…ë°ì´íŠ¸

        _hb_thread = threading.Thread(target=_heartbeat, daemon=True)
        _hb_thread.start()

        try:
            # PDF ìƒì„± ì „ ì·¨ì†Œ í™•ì¸
            check_task_cancelled()

            pdf_buffer = pdf_generator.generate_pdf(session_data)
        finally:
            _hb_stop.set()
            try:
                _hb_thread.join(timeout=2)
            except Exception:
                pass

        # PDF ìƒì„± í›„ ì·¨ì†Œ í™•ì¸
        check_task_cancelled()

        # [4] ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸: íŒŒì¼ ì €ì¥ ë‹¨ê³„
        pdf_tasks_collection.update_one(
            {'_id': original_task_id},
            {'$set': {
                'progress': {
                    'current_step': 'saving',
                    'message': 'PDF íŒŒì¼ ì €ì¥ ì¤‘',
                    'updated_at': datetime.utcnow().isoformat()
                }
            }}
        )

        # [5] íŒŒì¼ ì‹œìŠ¤í…œì— PDF ì €ì¥ (var/pdf_reports ì‚¬ìš©)
        from ecoweb.config import Config
        user_pdf_dir = os.path.join(Config.PDF_REPORT_FOLDER, str(user_id))
        os.makedirs(user_pdf_dir, exist_ok=True)

        # íŒŒì¼ëª… ìƒì„±
        url = session_data.get('url', 'unknown')

        # URLì„ íŒŒì¼ëª…ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì •ë¦¬
        def _sanitize_filename(url):
            if not url:
                return 'unknown'
            # í”„ë¡œí† ì½œ ì œê±°
            if url.startswith(('http://', 'https://')):
                url = url.split('://', 1)[1]
            # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ê¸¸ì´ ì œí•œ
            safe_name = re.sub(r'[^\w\-_.]', '_', url)
            safe_name = safe_name[:50]  # ê¸¸ì´ ì œí•œ
            return safe_name

        safe_url = _sanitize_filename(url)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"carbon_report_{safe_url}_{timestamp}.pdf"

        # ì ˆëŒ€ ê²½ë¡œë¡œ íŒŒì¼ ì €ì¥
        pdf_path = os.path.join(user_pdf_dir, filename)

        with open(pdf_path, 'wb') as f:
            f.write(pdf_buffer.getvalue())

        file_size = os.path.getsize(pdf_path)

        # [6] ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸: ì™„ë£Œ ë‹¨ê³„
        relative_path = f"var/pdf_reports/{user_id}/{filename}"

        pdf_tasks_collection.update_one(
            {'_id': original_task_id},
            {'$set': {
                'status': 'SUCCESS',
                'completed_at': datetime.utcnow(),
                'result': {
                    'pdf_path': relative_path,
                    'filename': filename,
                    'file_size': file_size,
                    'generated_at': datetime.utcnow().isoformat()
                },
                'progress': {
                    'current_step': 'completed',
                    'message': 'PDF ìƒì„± ì™„ë£Œ',
                    'updated_at': datetime.utcnow().isoformat()
                }
            }}
        )

        return {
            'status': 'SUCCESS',
            'pdf_path': relative_path,
            'filename': filename,
            'file_size': file_size
        }

    except Exception as e:
        # ì·¨ì†Œëœ ì‘ì—…ì¸ ê²½ìš°
        if 'cancelled by user' in str(e).lower() or 'task cancelled' in str(e).lower():
            pdf_tasks_collection.update_one(
                {'_id': original_task_id},
                {'$set': {
                    'status': 'CANCELLED',
                    'cancelled_at': datetime.utcnow(),
                    'progress': {
                        'current_step': 'cancelled',
                        'message': 'ì‚¬ìš©ìì— ì˜í•´ ì·¨ì†Œë¨',
                        'updated_at': datetime.utcnow().isoformat()
                    }
                }}
            )
            return {'status': 'CANCELLED', 'reason': str(e)}

        # ì¼ë°˜ ì˜¤ë¥˜
        current_app.logger.error(f'PDF ìƒì„± ì‹¤íŒ¨: {str(e)}')

        pdf_tasks_collection.update_one(
            {'_id': original_task_id},
            {'$set': {
                'status': 'FAILURE',
                'failed_at': datetime.utcnow(),
                'error': str(e),
                'progress': {
                    'current_step': 'failed',
                    'message': f'PDF ìƒì„± ì‹¤íŒ¨: {str(e)}',
                    'updated_at': datetime.utcnow().isoformat()
                }
            }}
        )

        return {'status': 'FAILURE', 'error': str(e)}
